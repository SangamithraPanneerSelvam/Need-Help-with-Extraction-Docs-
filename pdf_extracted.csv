0
" IEEE First International Conference on Data Science in Cyberspace IEEE First International Conference on Data Science in CyberspaceOPSDS a semantic data integration and service system based on domain ontology Liu Xin School of Computer and Communication Engineering University of Science and Technology Beijing Beijing, China liuxin_ustb"
" Hu Chungjin School of Computer and Communication Engineering University of Science and Technology Beijing Beijing, China hucj"
" Huang Jianyi School of Computer and Communication Engineering University of Science and Technology Beijing Beijing, China huangjianyi_ustb"
" Liu Feng School of Computer and Communication Engineering University of Science and Technology Beijing Beijing, China m"
" Abstract—For the distributed, heterogeneous, relational plex data sources of petroleum engineering, we present an oilproduction engineering semanticbased data integration system OPSDS"
 OPSDS establishes a semantic data integration and service system based on domain ontology on the premise of building a global semantic model and realizing the global semantic search
" The global semantic data model applied to various oil fields is set up by ontology extraction, ontology evolution, ontology bination and semantic constraints"
" The domainoriented data integration to provide the data access and is realized by ontology mapping, query shared service transformation, and data cleaning"
" Users and upper applications can have a direct access to underlying plex data sources in times of need through the global semantic data model, and the cleaned data can be returned in a unified format"
 OPSDS has been realized and got extensive use in many platforms of China National Petroleum CorporationCNPC
" It has been found that the method can not only provide the prehensive and realtime data support for oil and gas wells, but also improve the production and recovery efficiency with good application"
 Keywords petroleum information system data service technology of domain data semantic integration ontology petroleum engineering distributed data processingI
INTRODUCTION Oil is an important strategic resource of a country
 But oil and gas production of many oil fields around the world is trending downward and more fields are transiting into decline each year
 Reports indicate that a hybrid average production decline rate for oil fields worldwide is more or less
" Currently, various research methods to improve oil and gas production are booming"
" Production design, decision analysis, diagnosis and management of oil and gas wells are the keys to enhance productivity, reduce costs and increase profits"
" Optimal design of oil and gas wells involves large amounts of data, such as production data, well profiles, equipment data, geological structures, seismic data, reservoir data, etc"
"data with huge value have drawn attentions of academia, industry and government"
" For the purpose of improving production and saving energy, it is a highly advocated idea to dig out the value to utilize the data more efficiently"
 We focus on the features of oil data firstly
" Oil field is posed of a number of oilproduction plants, exploration institutes, geophysical research institutes and other units"
" Different units collect, collate, process, analysis and apply different kinds of data, and store corresponding data in their own databases which results in that different types of data are stored in different professional databases"
" Each database has its own specialized data organization and naming conventions, leading to system heterogeneity, syntax heterogeneity, structure heterogeneity and semantic heterogeneity"
 System Heterogeneity means operating environments and hardware platforms of data are various in different oil panies
 Syntax Heterogeneity indicates that oil panies take different storage methods for different types of data
" For example, some data are stored in relational databases, and some are stored in forms of text files"
 that different oil fields intends Structure Heterogeneity represent the same type of data with different data schemas
 A typical example is shown in Figure 
 Semantic Heterogeneity mainly refers to different words with the same meaning or the same words with different meanings
 Sucker rod data with structure heterogeneity
 Authorized licensed use limited to Hochschule Heilbronn
" Downloaded on June , at UTC from IEEE Xplore"
 Figure shows an instance of sucker rod structural data stored in relational database
" A well requires a set of sucker rod data, which contain rod level, length, diameter and other information, and different rod levels correspond to different rod lengths"
" For a multilevel sucker rod, D bines the threelevel rod length and saves as one field, D saves rod lengths into three rows according to different rod level, and D represents three fields in one row"
 that information And new features of data integration are concluded
" integration Bernstein and Haas say bines information from different sources into a unified format, and they specify the plicacy of integration after investigating the tools and technologies of data integration in the enterprise"
 They also indicate that every step of the integration process requires a good deal of manual intervention and more automation is surely possible
" Oil production engineering data are dynamic, updated in real time, and in critical instant need"
" Each oil field has not only production data every day, but also constantly updated basic data and regularly updated equipment data"
 So it is vital to ensure the realtime of data for upper applications
 Complex semantic associations
 It mainly refers to the plex associations between different data
" For example, we regard the well whose deviation angle is less than degrees as a vertical well, the well whose deviation angle is greater than degrees as a horizontal well, and the well whose deviation angle is between degrees and degrees as a inclined well"
 Focusing on the characteristics of the data leads us to discuss the challenge of traditional data management
" On one hand, data of oil fields are scatteredly stored, the logical organization lacks of ‘soul’, and the data schemas are various without naming rules and management methods"
 Thus it is urgent to establish a global semantic data model which is suitable for multiple oil fields to achieve the unification of data management platform
" On the other hand, data of oil panies are considerable autonomy, which increas the difficulty of data exchange and sharing"
 But data from different professional databases are increasingly need to work together to support upper applications of the domain
 So semantic data integration and building uniform interfaces directly accessing to the underlying data resources is of great significance
" In this paper, a petroleumengineering semanticbased data service OPSDS is presented to achieve a semantic data integration and service system based on domain ontology"
 The system provides a semantically richer global ontology and querybased access to the distributed and heterogeneous data
 OPSDS shields the plexity and sources of data to enable users and upper applications to take full advantage of data resources in a payasyougo approach everywhere
" Besides, a reasoning function is available for inferring the hidden information behind the semantic associations"
" RELATED WORKAs the plexity of data leads to a raising challenge for traditional data management, it is of utmost importance to generate a new way of data service"
 Data services provide access to data drawn from one or more underlying information sources
" Serviceenabling data stores, integrated data services and cloud data services inthe enterprise world are introduced in detail, but semantic relationships are not considered"
 propose a framework for scientific data services
 Data integration is a pervasive challenge faced in applications that need to query data residing at multiple autonomous and heterogeneous data sources 
" present a collaborative environment called distributed interoperable manufacturing platform, in which the STEPNC data model is built to promote data exchange among heterogeneous systems"
" propose a serviceoriented framework for integration of domainspecific data models in scientific workflows, which links the data sources and upper applications"
" However, the data model is built by domain experts, which is subjectivity and lacking in semantic relations between the data elements"
" Mapping of data with association relationships are constructed, but a certain inaccuracy exists"
 present three existing enterprise ontologies with different levels of expressivity
" Apparently, their work is not for specific domain, especially for the oil field"
 ARCHITECTURE OF OPSDS A
 OPSDS architecture OPSDS provides a rich semantic view of the underlying data and interfaces enabling users and upper applications to access data
 The architecture of OPSDS is shown in Figure 
" The bottom of the architecture is data sources storing in different databases, such as Oracle, SQL Server, etc"
 The middle layer is local ontologies extracting from the data sources below
" And then, the global ontology is formed as the local ontologies"
 Users and upper result of bining applications can access the data resources easily
" The only thing service consumers need to do is to send queries according to the global ontology, and then the desired data can be received"
 Architecture of OPSDS
 Authorized licensed use limited to Hochschule Heilbronn
" Downloaded on June , at UTC from IEEE Xplore"
 Construction of global ontology We adopt a hybrid strategy to construct the global ontology
" On the one hand, a topdown approach is used to filter the demand data"
" Entities, attributes and relationships"
 For Peer Review Only between entities can be got by classifying and organizing the data
" On the other hand, take a bottomupmethod to build local ontologies, which are results of extracting schemas of databases and items of synonym list"
" And then the global ontology is established according to ontology evolution, ontology mapping and imposed semantic constraints"
 Figure shows the construction process of global ontology
 The construction process of a global ontology
" Data of petroleum exploration and development vary in many aspects, such as exploration, production, geology, seismology, well logging, well drilling, etc, while data of petroleum engineering are just a part of them"
" Thus, firstly, we filter data to define the basic requirements, and classify, organize and aggregate the data to form entities, attributes and the relationships between entities labels"
 Then referring to the data dictionary
 identify Take production data entity and equipment data entity as examples
 The entity models are as follows
" Production data entity Production oil_output, gas_output, flow_pressure…… Equipment data entity Equipment pumping_unit, sucker_rod, defueling_pump……Since the majority of petroleum engineering data are stored in relational databases, we are here to study mapping fromRelational Database to OWL ontology"
" A relational database is posed of a set of relational schemas, including basic table structures and integrity constraints"
" An OWL ontology consists of classes, properties, individuals and axioms"
" As we aim at providing mappings between data models and ontologies, classes and properties are considered in this step"
" Because of individuals are widely exist in underlying database, individuals are not taken into consideration"
 Axioms are covered later in this paper
 The synonym list of petroleum engineering is built by domain experts and DBAs by reference to exploration and development handbooks of oil fields
 The synonymous items with different names and same meaning in the handbooks are gathered together in the synonym list to solve the phenomena of semantic heterogeneity
" Based on the schemas of tables in the specialized databases, we analyze characteristics of tables and constraints between tables, and then define an oil production engineering data source ontology OPDSOnto, which maps synonyms in the synonym list and schemas of tables to classes and properties in the ontology"
 The local ontology can be generated automatically through the program
 Getting innovations from Relational
"OWL, OWLRDBO and ProInnovator, we design OPDSOnto to describe tables, columns relations of tables and synonymy"
 Then extraction rules are defined as follows
" Convert tables and columns in databases to classes OPDSOnto Table or OPDSOnto Column owl Class, which express main concepts of the domain"
 Hierarchical relationships between tables and columns are presented by OPDSOnto hasParent and OPDSOnto hasChild owl ObjectProperty with owl inverseOf constructs
" OPDSOnto hasChild has a direction from domain Table to range Column, while OPDSOntohasParent has an opposite direction"
" Relationships between columns in one table are presented by OPDSOnto hasBrother, which is defined in owl ObjectProperty"
" If a column C in table A is the foreign key to table B, OPDSOnto hasChild represents the foreign key constraint, from domain column C to range Table B, while OPDSOntohasParent is the reverse semantic association"
" Datatype Properties of classes are defined, such as OPDSOnto isPK, OPDSOnto isFK, OPDSOnto isNullable, OPDSOnto dataType, to describe the primary key, the foreign key, nullable and data type of the individual"
" Extract the items which express the same meaning from the synonym list to convert into classes, and the relationships between classes are defined as OPDSOntohasSynonymy, which is built in owl ObjectProperty"
 Semantic Relationship is defined as shown below
 DEFINITIONSemantic Relationship
" ∀ x,x……xn,z, if xz, xz, …… , xnz, then xx"
"xn, where indicates the relation between two classes and identifies the semantic relationships between classes"
" According to the definition, the relationships that are hasParent, hasChild, hasBrother and hasSynonymy defined above among classes can be enriched"
 Tables from production database partial
 Authorized licensed use limited to Hochschule Heilbronn
" Downloaded on June , at UTC from IEEE Xplore"
 Figure shows schemas of four tables from production database
" Take Table well_info, Column well_name from Table well_info and Table output as examples to specify the process of ontology extraction, which is shown in Figure "
" information, that is, the two classes correspond to different attributes of one entity formed in the step of data filtering, the two classes evolve to a relation of hasBrother, parents of the two classes evolve to a relation of hasSynonymy"
 hasParenthasChildhasSynonymyhasBrotherhasBrotherhasBrotherhasParenthasChild 
 The steps of local ontology extraction
 The number in Figure corresponds to the rule number
 indicates that convert table names well_info and column name well_name into classes well_info
 means the relational schema of well_info and well_name is turned to a parentchild relationship in the local ontology
 is converting the two columns well_name and well_class from the same table into a hasBrother relation
 represents that the foreign key constraint of well_name
 Table well_info and table output is converted into a parentchild relationship
" Rule defines datatype properties of class well_name, while Rule extracts synonyms of well_name from synonym list and defines relationships between synonyms as hasSynonymy"
 Figure shows the local ontology after extracting schemas of production database and synonym list
 The arrows in Figure represent the relation of hasChild
 The classes with synonymy are circled in one node
 Local ontology of production database partial classes
" There are two steps from local ontologies to global local ontology and ontology, which are evolution of bination of to data storage local ontologies"
" Due characteristics of specialized databases in the petroleum engineering field, evolution of local ontology means if two classes with different parent nodes describe the same kind of When it es to bination of local ontologies, definite relationships must exist between the local ontologies"
 The global ontology can be built by mapping relevant local ontologies
" After analyzing schemas of different databases in the domain, the relationship between two local ontologies, which have the same class, can be established as a foreign key constraint"
 The parent node of the class with isPK property is mapped to the subclass of the class without isPK property
" For example, equip_info is a table of production database, and pump_parameters is a table of equipment database"
" The schemas of the two tables are as follows equip_info well_name, pumping_unit, pumping_rod…… pump_parameters pumping_unit, manufacturer, power……"
" Well_name is the primary key of table equip_info, and pumping_unit is the primary key of table pump_parameters"
" In our method, we regard pumping_unit as a foreign key of table equip_info linking to table pump_parameters"
" Thus, production ontology and equipment ontology are bined into one ontology"
 Figure shows the bined ontology
 Global ontology partial classes
" In Figure , the node output, production theevolutionary result of class output and class production"
" The relationship between output and production is evolved into hasSynonymy, and the relations of subclasses of output and production are hasBrother"
" is After local ontology evolution and local ontologies bination, local ontologies can be converted into a global ontology"
" We add some semantic constraints to strengthen the relations of terminology, and inference engine can reason and deduce the global ontology to reorganize the concepts using the constraints"
 Thus the implied semantic information can be got and valueadded services can be provided to users
 Semantic constraints are defined as follows
 Rule x OPDSOnto has Child y y OPDSOntohasSynonymy z x OPDSOnto has Child z Rule x OPDSOnto hasSynonymy y y OPDSOnto hasBrother z x OPDSOnto hasBrother z Authorized licensed use limited to Hochschule Heilbronn
" Downloaded on June , at UTC from IEEE Xplore"
 Rule x OPDSOnto hasSynonymy y y OPDSOnto hasParent z x OPDSOnto hasParent z C
" The process of semantic query Users and upper applications can submit query requests according to the global ontology, and OPSDS converts the requests to SPARQL statements to query the global ontology"
 Then the SPARQL statements can be rewrited as SQL statements to access underlying data sources
" Finally, the query results are returned in a uniform format after data cleaning and converting"
 Figure shows the process of semantic query
 Rewrite of SPARQL statements indicates that query requests based on the global ontology are converted into SQL statements to access underlying data sources
" Mapping from global ontology to data sources is divided into onetoone and onetomany, which include the following three types"
 Required data are from one table of a database
 Required data are from two or more tables of a database
 Query requests need to access more tables from multiple databases
 The process of semantic query implementation is as follows Step 
 Get the query request from service consumers to generate QueryGQG to query the global ontology
 QG is described by SPARQL
 Inference engine reasons names of classes and properties of QG to the names in the relevant ontologies
 Query resolver deposes QG into SubQueryLSQL to query each local ontology
" SQL {SQL , SQL , ……, SQLn , where n is the number of local ontologies"
 Query rewriter converts SQ L into SubQueryD SQDto query underlying database
" SQD {SQD , SQD , ……, SQDn , and SQD is described by SQL"
 Execute SQD and return SubResultD SRD 
" SRD {SRD , SRD , ……, SRDn "
" Result transformer cleans the query results SRDreferring to rules, and the normalized results can be got"
" Result biner bines the normalized results, and returns the final query result"
 QueryG SPARQL Result SubQueryL SPARQL SubQueryD SQL SubResultD SubResultD unified 
 shows the process of semantic query
 CONCLUSION AND FUTURE WORKintegration in specific area Semanticbased data is being a key research currently
" Build the global semantic data model for distributed, heterogeneous and plex semantic correlation data and provide prehensive and realtime data services using ontology technology are efficient and feasible"
" integration For data intensive industries, establishment of a global semantic data model based on domain ontology and realization to serve for upper of semanticbased data applications can get good results"
" Oil production engineering isa typical data intensive field, and OPSDS, which has realized data shared and reused, plays a key role in production practices"
" Through OPSDS, upper applications can directly access the underlying data resources and get the normalized data to beused for industrial production"
" The data of petroleum industrial applications show that our method can not only improve the production and recovery efficiency, but also save energy and reduce costs"
" Driven by application requirements, OPSDS connects production, study and research tightly, which is aneffective way to promote scientific and technological progress and prove that science and technology is the first productive force"
" In the future work, OPSDS will be used in more fields, and promoted to other oil areas like exploration and geology"
 Managing Scientific Data From Data Integration to Scientific WorkflowsJ
" GSA Today, Special Issue GSA Today, Special Issue"
" Carey M J, Onose N, Petropoulos M"
" Communications of the ACM, , "
" Dong B, Byna S, Wu K"
 SDS a framework for scientific data servicesCProceedings of the th Parallel Data Storage Workshop
" Halevy A, Rajaraman A, Ordille J"
 Data integration the teenage yearsCProceedings of the nd international conference on Very large data bases
" Smoot M E, Ono K, Ruscheinski J, et al"
 new features for data integration and network visualizationJ
" Bernstein P A, Haas L M"
 Information integration in the enterpriseJ
" Communications of the ACM, , "
 DIMP an interoperable solution for software integration and product data exchangeJ
" Enterprise Information Systems, , "
" Bender A, Poschlad A, Bozic S, et al"
 A serviceoriented framework for integration of domainspecific data models in scientific workflowsJ
" Procedia Computer Science, , "
" Das Sarma A, Fang L, Gupta N, et al"
 Finding related tablesCProceedings of the ACM SIGMOD International Conference on Management of Data
" Zdravković M, Panetto H, Trajanović M, et al"
 An approach for formalising the supply chain operationsJ
" Enterprise Information Systems, , "
 Authorized licensed use limited to Hochschule Heilbronn
" Downloaded on June , at UTC from IEEE Xplore"
 Information Sciences Contents lists available at SciVerse ScienceDirectInformation Sciencesj o u r n a l h o m e p a g e w w w 
" c o m l o c a t e i n sKernelMapping Remender system algorithmsMustansar Ali Ghazanfar a,, Adam PrügelBennett a, Sandor Szedmak ba School of Electronics and Computer Science, University of Southampton, Highﬁeld Campus, Southampton SO BJ, United Kingdomb Intelligent and Interactive Systems, University of Innsbruck, Innsbruck, Austriaa r t i c l ei n f oa b s t r a c tArticle historyReceived August Received in revised form March Accepted April Available online May KeywordsRemender systemsStructure learningLinear operationMaximum marginKernel"
 IntroductionRemender systems apply machine learning techniques for ﬁltering unseen informationand can predict whether a user would like a given item
" In this paper, we propose a newalgorithm that we call the KernelMapping Remender KMR, which uses a novelstructure learning technique"
" This paper makes the following contributions we showhow userbased and itembased versions of the KMR algorithm can be built userbased and itembased versions can be bined more information—features,genre, etc"
"—can be employed using kernels and how this affects the ﬁnal results and to make reliable remendations under sparse, coldstart, and long tail scenarios"
" Byextensive experimental results on ﬁve different datasets, we show that the proposedalgorithms outperform or give parable results to other stateoftheart algorithms"
"In this paper, we proposed a new class of kernelbased methods for solving the remendation problem that gives stateoftheart performance"
 The main idea is to ﬁnd a multilinear mapping between two vector spaces
" The ﬁrst vector spacemight, for example, have vectors encoding information about the items that we wish to rate, while the second vector spacemay contain a probability density function describing how a particular user will rate an item"
 Learning an appropriate mapping can be expressed as a quadratic optimisation problem
" As the problem involves a linear mapping, the solution to theoptimisation problem involves inner products in the two vector spaces"
 This allows us to use the kernel trick
 Directly solvingthe optimisation problem using quadratic programming would be too slow for most remendation datasets
" Instead, weﬁnd an approximate solution iteratively, following an idea ﬁrst developed by "
 This allows us to train the remenderin linear time
 The method described here is a specialisation of a general structure learning framework developed bySzedmak and used in for handling inplete data sources
The approach we have adopted is easily adapted to different sources of information
" We can, for example, use either rating information from other users or textual information about the items"
" Similarly, we are able to build either an itembasedor a userbased version of the algorithm"
" Because we have chosen to build a mapping to a space of functions approximatingthe probability density of the ratings, we have an intuitive interpretation of the remendations produced by the algorithm"
" This gives us ﬂexibility in how we make our ﬁnal remendation, which we can exploit to improve the ﬁnal prediction for different datasets"
"A main requirement of remender systems is to provide high quality predictions of the rating that a user would giveto an item, based on their previous rating history"
" Thus in testing remender systems, a dataset is used where somesets of ratings are treated as unseen while the other ratings are used for learning"
" To obtain accurate results, datasets Corresponding author"
 see front matter Elsevier Inc
 Information Sciences are usually selected with users that have made a relatively high number of ratings
" In real applications, however, the datasets are often highly skewed for example, a large number of users may have made only a small number of ratings, and alarge number of items may have received very few ratings"
 These are important scenarios in practical systems as givingreasonable remendations to new users can be crucial in attracting more users
" Similarly, giving a sensible rating to anew item may be necessary for those items to be taken up by the munity sufﬁciently to collect more ratings"
" Often,remender system algorithms that have been optimised to give good remendations on dense datasets performpoorly on these skewed datasets"
 We have generated highly skewed datasets to test our algorithm under these scenarios
"In particular we consider the newitem coldstart problem, the new user coldstart problem , and the long tail problem"
" We ﬁnd that the standard algorithm we developed performs poorly for these skewed datasets however, we showthat by using the ﬂexibility of our approach we can easily modify the algorithm so that it performs well under thesescenarios"
Remender systems have been a very active topic of research for around years
" This, in part, has been spurred onby the Netﬂix petition to improve the performance in terms of the root mean square error of a baseline algorithm by"
 One lesson to emerge from this was that a highly effective way to achieve a very high remendation performanceon a static dataset is to bine a large number of different algorithms
" Although such systems are interesting, they are notvery ﬂexible and may not be ideal algorithms for most real applications with rapidly changing users and items"
 Our algorithm relies on a single coherent method albeit with several variants that has not been designed for a speciﬁc dataset
 Wehave thus pared our approach with other general purpose remenders
" The best general purpose collaborative ﬁltering algorithms that we are aware of are by ,"
 These achieve a considerable gap in performance advantage over otheralgorithms
" The proposed algorithm achieves similar performance in terms of mean absolute error to these approaches,although it is outperformed by on a dataset with ,, ratings and by on a dataset of ,, ratings"
The proposed approach is however very different
" The other two approaches are based on matrix factorisation, although also uses kernel functions"
 There has been considerable work on developing matrix factorisation techniques whichare at the heart of many of the most petitive algorithms for this problem
" Part of the interest of the proposed algorithmis that it takes a very different viewpoint from the matrix factorisation approaches, yet still has very petitiveperformance"
The rest of the paper has been organised as follows In the next section we brieﬂy outline related work
 Section outlinesthe proposed algorithm using an itembased approach
 In Section we describe extensions to the basic algorithm
" Section presents details of the datasets we use for evaluation, the metrics we use and the procedure for tuning parameters of thealgorithm"
 This is followed in Section by a presentation of results from our experimental evaluation
 We conclude in Section 
 Some of the details and more extensive results are given in appendices
 Related workThere are two main types of remender systems collaborative ﬁltering and contentbased ﬁltering remender systems
" Collaborative ﬁltering CF remender systems ,,,,,, remend items by taking into account thetaste in terms of preferences of items of users, under the assumption that users will be interested in items that users similarto them have rated highly"
" Examples of these systems include GroupLens system , Ringo www"
 Collaborative ﬁltering systems are classiﬁed into two subcategories memorybased CF and modelbased CF
" Memorybased approaches make a prediction by taking into account the entire collection of previous items rated by a user, for example,the GroupLens remender systems "
" Modelbased approaches use rating patterns of users in the training set, groupusers into different classes, and use ratings of predeﬁned classes to generate remendations for an active user That is, the userfor whom the remendations are puted on a target item That is, the item a system wants to remend"
" Examples include itembased CF , Singular Value Deposition SVD based models ,,, matrix factorisation,,,,,,,,, nuclear norm regularisation , Bayesian networks , and clustering methods ,,"
"Contentbased ﬁltering remender systems ,,, remend items based on the content information of an item,under the assumption that users will like similar items to the ones they liked before"
" In these systems, an item of interest isdeﬁned by its associated features for instance, NewsWeeder , a newsgroup ﬁltering system uses the words of text asfeatures"
" Other wellknown types of remender systems include knowledgebased systems ,, Ontologybased systems, and hybrid systems ,"
"Hybrid remender systems have been proposed elsewhere ,,,,,,,,, which bine individual recommender systems to avoid certain limitations of individual remender systems"
" In the proposed approach we can addmore information about items in the forms of additional kernels, which can be thought of as bining collaborative ﬁltering with contentbased ﬁltering"
" A related approach has been proposed in , where the authors employed a uniﬁed approach for integrating the useritem ratings information with useritem attributes using kernels"
 They learned a predictionfunction using an online perceptron learning algorithm
" They claimed that adding more kernels increases the performance,which is in contrast with our ﬁndings"
" It might be due to the reasons that they used very simple kernels, such as correlation,and identity however, we used polynomial kernels, which are in turnare addition of correlation, identity, etc"
" Information Sciences In , the authors proposed a structured learning algorithm for learning from inplete dataset"
" The idea of the structure learning has been used in , where the authors employed it for enzyme prediction"
 We show how the structure learning approach can also be used to solve the remender system problem effectively
Remendations can be presented to an active user in the following two different ways by predicting ratings ofitems a user has not seen before and by constructing a list of items ordered by their preferences
" In this paper, we focuson both of them"
" KernelMapping RemenderA remender system consists of two basic entities users and items, where users provide their opinions ratings aboutitems"
 We denote these users by U fu u 
" uMg, where the number of people using the system is jUj M, and denotethe set of items being remended by I fi i "
" The users will have rated some, but not all, of theitems"
" We denote these ratings by riuji u D, where D I U is the set of useritem pairs that have been rated"
 We denote the total number of ratings made by jDj T
" Typically each user rates only a small number of the possible items, so thatjDj T jI Uj N M"
 It is not unusual in practical systems to have TN M u 
 The set of possible ratings madeby the users can be thought of as elements of an M N rating matrix R
" We denote the items for which there are ratings byuser u as Du, and the users who have rated an item i by Di"
" The task is to create a remendation algorithm that predicts anunseen rating riu, That is, for i u R D"
In this section we describe an itembased remender
 In the next section we show how we can adapt the approach to auserbased remender
 To perform the remendation task we consider building the additive and multiplicative modelsfor the residual ratings
" The residue in the additive model is given byriu riu ri ru rwhere ri, ru and r are respectively the mean rating for the item, of the user, and the overall meanri jDijXuDiriuru riur jDujXiDujDjXuDriuThe multiplicative model can be expressed as followsriu riurriru"
" Itembased KMRwhere r, ri and ru are the geometric means for all the ratings, the ratings for item i, and the rating of user u, respectively"
" Wefound the additive model to be marginally better than the multiplicative one, and hence this work is based on the additivemodel"
We use a technique developed by Szedmak and coworkers for learning structured data 
 In the following we outlinehow this approach is adapted for solving the collaborative ﬁltering problem
 We assume that we have some informationabout the items which we denote by qi
" This may, for example, be the set of ratings riu for u Di, or it could be text describing the item i"
 We map the information to some vector qi in some extended feature Hilbert space
" Similarly, we mapthe rating residues, riu, to ‘vectors’ in some other Hilbert space"
" In this paper, we consider these objects to lie in the function space LR"
" In particular we represent each residual riu, by a normal distribution with mean riu and variance r"
" Thatis,wriu N xjriu rThe motivation of this choice is to model possible errors in the rating either due to the discretisation of the rating scale or thevariability in assigning a rating e"
 due to the mood of the user on the day they made the rating
The method developed by Szedmak is to seek a linear mapping between these two spaces which can be used for makingpredictions
" More speciﬁcally, in our application, we look for a linear mapping Wu from the space of vectors to the space ofw vectors refer to "
 We will use the mapping Wuqj to make a prediction for the rating of a new item j by the user u
To learn the mappings Wu we will minimise the Frobenius norm of Wu subject to the constraintshwriu Wuqii P fiwhere fi P is a slack variable and where we have a constraint for each pair i u D
 This ensures that Wuqj is alignedwith wriu for the ratings in the training set
 We can write the training problem for the mappings Wu as a quadratic programming problemM
 Information Sciences minimiseXuUkWuk CXfiiIwith respect to Wu u U fi i Isubject tohwriu Wuqii P fifi P i I u DiNote that minimisation will be achieved when the vectors Wuqi are as uniformly aligned as possible with the vector wriu
Having learned the mappings Wu we can then make predictions for a new item j using Wuqj
 This outputs a functionwhich informally we can think of as an estimate for the probability density of the residue rju
" However, Wuqj does notneed to be, and typically is not, positive everywhere or normalised"
" Thus, it is not itself a probability density"
 We discuss laterdifferent methods for interpreting Wuqj
"To solve this constrained optimisation problem, we deﬁne the LagrangianL XuUkWuk CXfi Xaiuhwriu Wuqii fi iIiuDXiIkifiwhere aiu P are Lagrange multipliers introduced to ensure that hwriu Wuqii P fi and ki P are Lagrange multipliers introduced to ensure that fi P "
 The optimum mapping is found by solvingsubject to the constraints that aiu P for all i u D and ki P for all i I 
" For a general linear mapping, Wu, we have thatwhere is the tensorproduct of the two vectors"
" This is clearly the case when the Hilbert spaces are ﬁnite dimensions sothat the mapping Wu can be represented by a matrix, but this can be extended for linear mappings between more generalHilbert spaces"
" Using this result we ﬁndThat is, the Lagrangian is minimised with respect to Wu when Wu Pfi we ﬁndiDiaiuwriu qi"
 Taking derivatives with respect tominfWugffigmaxfaiugfkigLhwriu Wuqii wriu qiWuLWuLfiXuDi Wu aiuwriu qiXiDu C aiu kiXuDiaiu C ki CSetting these derivatives to we ﬁnd that the Lagrangian is maximised with respect to fi whenwhere the inequality arises because ki P 
After substituting back the expressions containing only the Lagrange multipliers into the Lagrangian we obtain the dualproblem of which is a maximisation problem with respect to the variables aiuf a aiuaiuhwriu wriuihqi qi i XXuUiiDuXaiuiuDsubject to the constraint that a Za whereZa aji IXuDiaiu C i u D aiu P Krriu riu hwriu wriuiK qqi qi hqi qi ithen we can write fa asf a aiu aiu Krriu riu K qqi qi XXuUiiDuXaiuiuDWe are now in the position where we can apply the usual kernel trick
 Deﬁning the kernel functionswhere we are free to choose any pair of positive deﬁnite kernel functions
" With our choice of mapping the rating residual, r,to wr N xjr r, we note thatM"
 Information Sciences Krr r hwr wri N r rjprwhich is inexpensive to pute
" We could build more plex kernels for Krr r, by mapping wr into another extendedfeature space, although we would then lose the interpretation of Wuqi as an approximation to the density function for riu"
" Learning the Lagrange multipliersFor largescale remender systems, solving this quadratic programming problem using a general quadratic programming solver would be impractical due to the large number of data points"
" However, we can ﬁnd an approximate solutioniteratively using the conditional gradient method"
 To understand this method it is helpful to write fa in matrix formf a aTMa bTawith a Za
 We obtain a series of approximations at for the optimal parameters starting from some initial guessa Za
 At each step we use a linear approximation for fa about the current position atf a f at a f at a atf atWe pute the next approximation using two stages
" We ﬁrst solve the linear programming problema argmaxaZaf at a argmaxaZaaTMat b constWe then ﬁnd the new approximation at to beat at sa atwhere we choose s to bes argmaxf at sa ats½smax b MatTa ata atTMa atNote that we can pute the unconstrained maximum for sf a XXaiugiu constuDiiIf ataiu XiDugiu atiu Krriu riu K qqi qi maximiseaiugiuXuDiXuDisubject toaiu C and u Di aiu P By truncating smax if necessary to ensure that it lies in the interval , we can ensure that at is the maximum value of aalong the line segment from at to a"
" Since this segment includes the current point, at, we are guaranteed that no stepdecreases the objective function"
We note that in the linear programming problem we have an objective function of the formwhich decouples for every set of Lagrange multipliers Ai fauiju Dig
 The linear constraints Za also decouple into a set ofconstraints for each set of Lagrange multipliers Ai
" Thus, the linear programming problem bees a series of linear programming problems for each i IThis linear programming problem is trivial to solve see "
 If giu has positive ponents then a maximum will occurwhen we set aiu C where giu P giu for all u Di and aiu otherwise
Since ﬁnding the largest ponent of giu can be puted in linear time we are able to perform one step of the optimisation procedure in HjDj time
 Note that at each step we have to pute a vector matrix products involving the matrix M
These products involve the sum over all u U and the sum over all i Du
" However jDuj does not grow with the number ofusers, thus these products can also be puted efﬁciently"
" Note, that solving the optimisation problem this way makes itfeasible to obtain remendation for databases with up to million ratings"
" Predicting unseen ratingsTo make a prediction for the rating riu where i u R D, we estimate the residue riu riu ri ru r using the functionM"
 Information Sciences 
 Schematic showing the linear programming problem
 The feasible region is shown as a shaded triangle
 The vector gi shows the direction of theobjective function
 The maximum occurs at the vertex corresponding to the largest ponent of gi
 Schematic showing the aim of the algorithm
" Information, qi in this case a rating vector about an item i, is ﬁrst mapped to a vector in an extendedfeature space qi"
" We then try to ﬁnd the best linear mapping, W, for user u, to the ‘vector’, wriu , describing the residual"
piur hwr Wuqii aiuKrr riuK qqi qi XiDuriu argmaxpiurrwhere wr N r r
 We have a choice in how to obtain a single prediction from this function
 Our standard predictor will beto ﬁnd the maximum argument of piurThis works well when we have a sufﬁcient number of ratings for the user and the item
 However as we will see it givespoor predictions in scenarios where we have a small amount of training data
 Recall that we argued earlier Wuqi can beregarded as an approximation for the probability density of riu
" It will not generally be positive everywhere, but by removingthe negative part of the function we can treat the remaining function as a probability density"
" In this case, we can considerthe mean, mode, or median as approximations for the most likely value of riu"
" Under conditions where we lack sufﬁcient datawe ﬁnd that using a bination of the mean, mode and median together with the standard max prediction gives a considerable improvement in accuracy"
" In particular, we consider a predictorrM wmaxrmax wmeanrmean wmodermode wmedianrmedianwhere rm with m {max, mean, mode, median are the standard predictors and the predictors using the mean, mode andmedian, while wm are a set of weights that are learnt from a validation set"
 We consider the weights to be constrained sothat wm P and they sum to 
 In the results shown later we denote those results obtained using this predictor by the superscript M
" A small scale exampleSuppose a remender system has four users That is, U fu, u, u, u and three items That is, I fi, i, i"
" The information about each item is a column vector of the useritem rating matrix, shown in Table "
" The users’, items’, and overall averages areru ri ru ri ru ri ru r M"
 Information Sciences Table Example a subset of the useritem rating matrix in a movie remender system
 We have four users rows and three moviescolumns
" The case, where a user has not rated a particular movie is shown by symbol"
" The rating scale, consisting of integervalues between and , captures the extreme like and extreme dislike behaviour of a user"
 The rating we want to predict isshown by ‘‘’’ symbol
iiiAfter applying the additive model Eq
" , the useritem rating matrix can be represented in the residual form as shown inTable "
" The input feature kernel, Kq, using the polyGaussian kernel refer to Section "
"We can pute the residual kernel, Kr, based on the inner products between Gaussian densities functions with expectedvalues r and r, and sharing the mon standard deviation r"
K q Krr r hwr wri p errrr pAssume that r 
" then we haveKruKruK residual KruKruwhereKru Kru Kru Kru The optimal values for the design variables, a, are learnt using the conditional gradient method, and are shown in Table "
" To make aAfter learning the a parameters, the mapping Wu, can be deﬁned for each user recall Wu Pprediction for the rating riu, where i u R DiDiWuqi aiuwriu qi qi aiuwriuK qqi qiXiDuXiDuIn this case, we have u u and i i, soWu qi aiu wriu K qqi qi aiu wriu K qqi qi wriu wriu wriu wriu N riu r N riu rwhich is an unnormalised probability density function of mixture of two Gaussians"
 The optimal rating then can be derivedbyTable Example the matrices of rating residues riu
" Information Sciences Table The optimal values of design variable, a, for each user and item"
"piu r hwr Wu qi i arg maxhwr wriu wriu i arg maxhKrr riu Krr riu i arg maxhN rjriu pr N rjriu prrirrTaking the optimum solution refer to , riu , the prediction for the residual is "
" Hence, user u would rate item i withrating of riu ri ru r "
 Extensions to the basic algorithm
 Userbased KMRIn this section we describe extensions to the basic algorithm which are relevant to practical remender systems
Depending on the dataset characteristics e
" number of items rated by the active user, number of users which have ratedthe target item, etc"
 different models can be trained along the rows or columns of the data matrix
" A related algorithm isproposed, which solves the problem from the user point of view, hence it is named as the userbased KMR KMRub"
" To perform a userbased remendation, we use information qu about users u and try to ﬁnd a linear mapping Wi to align someextended feature vectors qu to the residue vector wriu"
 The derivation is identical to that for the itembased remender when we interchange the subscripts i and u
 Combining user and itembased KMRUser and itembased versions provide plementary roles in generating predictions as they focus on different types ofiur be the predictions made by the user and itembased versions respectively
 Werelationships in a dataset
 Let pubhave considered three different ways of bining user and itembased predictions
"iu r and pib Using the simple linear bination In this approach, the user and itembased versions are linearly bined, where theparameter q is learned from a validation set"
piur qpubiu r qpibiurWe denote the resulting hybrid remender system by KMRLinearHybrid
" Switching on number of ratings Here, we take into account the information about user and item proﬁles"
" The rationalebehind this approach is the intuition that if we have a large number of ratings for an item pared to the number ofratings made by the active user, then the userbased version is likely to give better results than the itembased versionand vice versa"
" Rather than using the raw number of ratings, we normalise by the number of ratings given by the poweruser, up That is, the user that has rated the most number of items and by the power item iP That is, the item with the most number of ratings"
 Plotting the probability density function of mixture of two Gaussians with r f g
 The optimal solution is found to be 
 Information Sciences Table Characteristics of the datasets used in this work
" FT, SML, ML, ML, and NF represent the FilmTrust, MovieLens k, MovieLens M, MovieLens M, andNetﬂix dataset respectively"
 Average rating represents the average rating given by all users in the dataset
CharacteristicsNumber of usersNumber of moviesNumber of ratingsRating scaleSparsityMax number of ratings from a userMax number of ratings for a movieAverage ratingDatasetFT 
piur iu r pubiur pibjIup j hCntifjUijjUip j jIujotherwiseSML 
We denote the resulting hybrid remender system by KMRCntHybrid
 Switching on uncertainty in prediction Here we use a different strategy for switching between the user and itembasedpredictors
 We try to estimate the uncertainty in the prediction by examining the ‘‘variance’’ in Wuqi and Wiqu
"Since they are not real probability distributions, we must ﬁrst exclude the regions where the functions go negativeand normalise the output so that we can treat them as densities and pute their variance"
" We denote the varianceby Varub and Varib for the user and itembased versions, respectively"
 We then switch the remendation according topiur iu r pubiur pibif Varub Varib hVarotherwiseWe denote the resulting hybrid remender system by KMRVarHybrid
 Combining kernelsIn many applications there are multiple sources of information that can be used to make a remendation
 We can easily acmodate different sources of information by bining kernels
 To illustrate this we will test our algorithm on datasets consisting of ﬁlm ratings where we have three types of information available refer to Section 
 The ratings of other users from which we can construct a kernel Krat ‘‘Demographic’’ information obtained from genre about the ﬁlms from which we can construct a kernel Kdemo ‘‘Feature’’ information obtained from a textual description of the ﬁlms from which we construct a kernel Kfeat
"These kernels can be bined linearlyK bratK rat bdemoK demo bfeatK featwhere the parameters brat, bdemo and bfeat brat bdemo can be tuned by measuring the generalisation performance on avalidation set"
 This way of bining kernels can be viewed as a concatenation of the feature vectors p ratbratpbdemodemoqbfeatfeatp rat bratpbdemodemo qbfeatfeatwhere represents the direct sum
 Alternatively we can bine the kernels nonlinearlywhere the denotes the pointwise product of the kernel matrices
 This corresponds to taking a tensor product of the featurevectorsK K rat K demo K feat rat demo feat
 Experimental setupalgorithms
In this section we describe the datasets we used and the setup of the experiments for benchmarking the proposed
 Information Sciences As is mon in the ﬁeld of remender systems we used data from ﬁlm remendation sites to test the proposedalgorithm
 These provide some of the largest available datasets allowing us to test the scaling performance of the algorithm
"In addition, as these datasets are very monly used in the literature, it allows us to benchmark our algorithm against petitor algorithms"
 We used the following datasets FilmTrust denoted by FT obtained by crawling on th March the FilmTrust website httptrust
 Only users and movies having more than ﬁve ratings were used
" This has been used before in ,"
" MovieLens which we split into three groups Small MovieLens denoted by SML with million rating dataset denoted by ML million rating dataset denoted by MLThis has been widely used ,,,,"
" Random subsample of , users from the Netﬂix dataset denoted by NF"
 This dataset has been very widely used e
"see ,,, in part because of the prize offered for achieving a level of improvement over a benchmark"
 We have notattempted to pare our algorithm against the stateoftheart Netﬂix algorithms for two reasons
" Firstly they havebeen highly tuned to that particular dataset, while we have concentrated on developing a general purpose remendation algorithm"
" Secondly, the full Netﬂix dataset is so large that it is difﬁcult to process on a normal desktop machinewithout spending signiﬁcant time on optimising memory management"
The characteristics of these datasets are given in Table 
 Feature extraction and selectionTo test the remendation algorithm using textual information we also obtained information about each movie
 Thiswas used to construct two additional information vectors a ‘‘feature’’ vector and a ‘‘demographic’’ vector
 We downloadedinformation about each movie in the MovieLens SML dataset and FilmTrust dataset from IMDB
" For the ML dataset, weused the tags and genre information that is provided with this dataset"
" After stop word removal and stemming, we constructed a vector of keywords, tags, directors, actorsactresses, producers, writers, and user reviews given to a movie in IMDB"
"We used TFIDF Term FrequencyInverse Document Frequency approach for determining the weights of words in a documentThat is, movie"
" The document frequency DF thresholding feature selection technique was used to reduce the feature space byeliminating useless noise words having little or no discriminating power in a classiﬁer, or having low signaltonoise ratio"
"To construct the demographic vector, we take the genre information about movies as employed in , with the exception that we used the hierarchy of genre as shown in "
" To determine the weight of a genre in the genre vector, we used asimple weighting scheme as employed in QuickStep, an Ontologybased remender system "
" To pute an innerproduct between demographic vectors the immediate super class is assigned of a subject’s value, the next super classis assigned , and so on until the most general subject in the Ontology is reached"
" By making a hierarchy of the genreand assigning different weights to sub and superclasses, we hope to enrich an item’s proﬁle"
" MetricsIn the majority of the paper, we have used the Mean Absolute Error MAE as our measure of performance as this is themost monly used measure and de facto standard for benchmarking remender systems"
" In practice, however, remender systems are monly used for helping users in selecting high quality items"
" Thus, arguably, a more appropriatemeasure of accuracy is to study an algorithm’s ability to predict highly rated items"
 There are a number of metrics thatare more speciﬁcally designed to measure how well a remender classiﬁes good quality relevant items
 These includethe ROCsensitivity and F measure
 The details of all these metrics are given in Appendix B
" Furthermore, we also give tablesof results for these last two measures in that Appendix"
 Evaluation methodologyWe performed fold cross validation by randomly dividing the dataset into a test and training set and reported the average results
 We further subdivided the training set into a test and training set for measuring the sensitivity of the parameters
"For learning the parameters, we conducted fold cross validation on the training set"
" We matched the movie titles, provided by the SML and FT dataset, against the titles in the IMDB www"
 We used Google’s stop word list www
 We used Porter Stemming algorithm for stemming
 Information Sciences 
 Hierarchy of genres based on 
 All the super classes of a genre get a share when a genre receives some interest
" For instance if a rated movie fallsinto ‘‘crime’’ genre, then the ‘‘crime’’ subject will get a weight of q, the immediate super class, ‘‘Thriller’’ will get a weight of q and the next super class‘‘Unknown’’ will get a weight of q"
 Learning system parametersThere are a number of parameters that need to be learned
" Below, we discuss the training of these parameters"
" Number of iterationsThe algorithm we develop uses an iterative technique to learn the Lagrange multipliers, a"
 As we increase the number ofiterations the mean absolute error improves
 The speed of convergence will depend on the dataset and the type of information we are using e
 userbased or itembased
" and , show the mean absolute error and the time taken to learn theLagrange multipliers versus the number of iterations for the FT and SML datasets, respectively"
"We note that for the FT dataset, the performance of the itembased version suffers badly when the number of iterationsare very small"
" However, the performance of the userbased version is quite good even after a few iterations"
" Hence, if onehas a constraint on the time required to build the model, then it is better to switch to the userbased version rather than theitembased version for the dataset"
" In contrast, in the SML dataset, the convergence of all the methods was relatively quick"
The convergence clearly depends on the number of usersitems and the useritem proﬁle length e
" rating proﬁle, featureproﬁle length, etc"
 It is not obvious a priori how many iterations are needed to get good rating predictions
" Based on ourinitial experiments, we chose the number of iterations to be for the SML dataset, for FT, for ML, and forML and NF"
"We trained linear, polynomial, and polyGaussian kernels and chose the one giving the most accurate results"
" The optimal kernel parametersnomial kernel is of the formKx y hx yi RdFor the ratingbased version, the best polynomial kernel parameters d, R are found to be, for userbased and itembasedversions respectively , "
" for the SML dataset , "
" for the FT dataset and , "
 Information Sciences Number of Iterations FT DataSetNumber of Iterations FT DataSet 
 The number of iterations and time required to converge the proposed algorithms for the FT dataset
Number of Iterations SML DataSetUBIBUBIBUBIBFeatureDemoUBIBFeatureDemo rorrlE etuosbA naeMEAM sm emTirorrlE etuosbA naeMEAM sm emTi
Number of Iterations SML DataSet 
 The number of iterations and time required to converge the proposed algorithms for the SML dataset
" For the featurebased version, the best polynomial kernel parameters were found to be , "
" for theSML dataset and , "
"We did not tune the parameters for the ML and NF datasets, as it was putationally expensive"
" for user and itembased versions for both datasets and , "
 for the featurebased version for the ML dataset
"For the demographicbased version, we found the best kernel was the polyGaussian kernel which is a simple extensionof the Gaussian one given byM"
" Information Sciences Kx y exp kx ykqswhere the best parameters q, s were found to be "
 for the SML dataset and 
 Again we didnot tune parameters for ML dataset and they were ﬁxed to 
"The other parameter in setting up the kernel was the standard deviation, r, used in mapping wr N xjr r"
" We experimented with learning this parameter for each user, but found this putationally very expensive"
 We then tried groupingthe users according to the variance in their ratings into groups and tuned r for each group
" Although this gave improvedperformance, it was not found to be statistically signiﬁcant"
 We therefore just used a single parameter r which we tunedusing a validation set
"The parameter C that punishes the slack variables in the Lagrange formulation was ﬁxed to , after initial experimentation"
" In the extension of the basic remender there are other parameters, such as the weights for bining kernels andvarious thresholds for switching between remenders"
 The tuning of these parameters are described in Appendix A
" ResultsIn this section, we describe the results obtained from our experiments"
" In the tables we have denoted the proposed algorithm by KMRsupsub, where the subscript denotes the variant of the algorithm and the occasional superscript describes the variantin more detail where necessary"
" The main variants are itembased ib, userbased ub, featurebased F that use feature vectors rather than rating vectors, demographic D that use demographic vectors rather than rating vectors, and hybrid Hybridthat uses a mixture of userbased and itembased predictions"
 For the hybrid algorithm we use the superscript to denote thedifferent mechanisms for bining userbased and itembased predictions
" When we use binations of information, e"
"itembased ratings and features, we use KMRibF to denote the case when we add the kernels and KMRibF when we multiplythe kernels"
" Finally, for the datasets with a limited amount of ratings, instead of using the standard approach to predicting anew rating, we bined the standard approach value of r that maximises the predictor pr with the mean, mode and median of Wuqi for the itembased approach"
 We denote this version of the algorithm with a superscript M
We pare the proposed algorithms with other algorithms described in the literature
" We chose several other algorithms based on the number of citations given in the literature the algorithm classiﬁcation space That is, memorybased ormodelbased approaches and whether the algorithm claims to give stateoftheart results"
" Direct parisonWe pared the proposed algorithms with three different algorithms userbased collaborative Filtering CF with Default Voting DV proposed in which provides a useful baseline for paring algorithms, itembased CF proposed in shown by ItemBased CF, and a SVD based approach proposed in shown by SVD"
" To provide as fair a parisonas possible, we tuned all parameters of the algorithms"
Table shows that the KMRbased algorithms outperform all the aforementioned algorithms
" The percentage decrease inerror of KMRib, KMRub, and KMRVarHybrid over the baseline approach is found to be "
 for the SML dataset 
 for the MLdataset and 
 The ROCsensitivity and F measure on the same dataset are shownin Tables A
" Indirect parisonIn this section, we pare our results with other algorithms indirectly, That is, we take the result from the respective paperswithout reimplementing them, which might make the parison less than ideal"
 We conducted the weak generalisationtest procedures of using the AllButOne protocol—for each user in the training set a single rating is withheld for the testset
" We averaged the results over the three random traintest splits as used in ,,"
Table A parison of the proposed algorithm with others in terms of MAE
 The average with the respective standard deviation of results over fold is shown
 Thebest results are shown in bold font
AlgorithmUserbased CFItembased CFHybrid CFSVDKMRibKMRubKMRVarHybridBest MAESML
 Information Sciences Table A parison of different algorithms in terms of NMAE Normalised MAE for theML dataset
" The proposed algorithms outperform URP , Attitude ,MatchBox , MMMF , ImputedSVD , and Item "
 They give theparable results to EMMF and NLMF 
 Our results and the best resultsare shown in bold font
AlgorithmURPAttitudeMatchBoxImputedSVDMMMFItemEMMFNLMF LinearNLMF RBFKMRibKMRubKMRVarHybridAlgorithmNLMFMFTIBKMRibKMRubKMRVarHybridNMAE
Table A parison of different algorithms in terms of RMSE for the ML dataset
NLMF represents the nonlinear matrix factorisation technique as proposed in and M FTIB represents the Mixed Membership Matrix factorisation modelas proposed in 
 Our results and the best results are shown in bold font
A parison in terms of Normalised MAE NMAE—see Appendix B—of the algorithms is given in Table 
" In Table , URPrepresents the algorithm proposed in , Attitude represents the algorithm proposed in , MatchBox is proposed in ,MMMF represents the maximum margin matrix factorisation algorithm proposed in , ImputedSVD is proposed in , Itemis proposed in , EMMF represents the ensemble maximum margin matrix factorisation technique proposed in , andNLMF represents the nonlinear matrix factorisation technique with linear and RBF versions as proposed in "
Table shows that the NLMF and EMMF perform better than the rest
 The proposed hybrid algorithm gives slightlypoorer results to them with NMAE 
" It is worth mentioning that the EMMF is an ensemble of about predictors,which makes this algorithm unattractive"
" From this table, we may conclude that the proposed algorithm is parable to thestateoftheart algorithm for the MovieLens M dataset"
"To the best of our knowledge, the best results for the MovieLens M dataset that have been reported in the literature arethose proposed in ,"
 They claimed their proposed algorithm gives RMSE accuracy of 
 We followed their experimental setup and the results have been shown in Table 
 Table shows that the proposed algorithms outperform ’s results
 The percentage improvement is found to be 
 in the case of KMRVarHybrib
 TheMFTIB algorithm gave the best results outperforming our best algorithm KMRVarHybrid with 
" Actuallythe MFTIB integrates two plementary algorithms—discrete mixed membership modelling and continuous latent factormodelling That is, matrix factorisation—into a mon framework using the Bayesian approach, which illustrates the power ofcarefully bining different algorithms"
"Unfortunately, no NMAE or MAE was provided for MFTIB technique over Movielens M dataset, which makes itharder to pare different algorithms’ results with MFTIB"
" Considering these results, we conclude that the proposed approach appears to be petitive with the current stateoftheart approaches"
 Combining different kernelsAs discussed in Section 
", there can be different sources of information that can be used for making remendations"
"The proposed framework allows these different sources to be exploited by bining different kernels built from different The authors did not provide any numeric value, only a graph is presented showing the minimum value approximately to "
 Information Sciences Table Comparing the performance found with different binations of kernel for the SML dataset
 The average with the respective standard deviation of results overfold is shown
 The best results are shown in bold font
AlgorithmKMRibKMRDKMRFKMRibFDKMRibFKMRibDKMRFDKMRibFDKMRibFKMRibDKMRFDMAE
" In particular, we consider the rating information, feature information, and demographics information asdescribed in Section "
Table shows the performance of different binations of kernels for the SML dataset
" We have shown not only theMean Absolute Error MAE, but also a number of measures of the ability to classify ﬁlms as either highly rated or poorlyrated refer to Appendix B for details"
" We observe reasonable performance using just rating information, demographicinformation and feature information"
" Interestingly, for this dataset, bining kernels does not give signiﬁcantly betterperformance than using a kernel based on a single source of information"
" A plausible explanation of this observation is thatour error rates are close to the optimum that can be achieved there is a limit on the performance of any system due to theﬁckleness of the users making the ratings or, at least, we are close to the optimum given the way we have represented theproblem"
" On other datasets where, for example, ratings for some users are very sparse, demographic and feature informationcan be much more signiﬁcant"
 The other striking feature of Table is that multiplying kernels together seems to be moresuccessful than adding different kernels
Similar results not shown were observed in the case of FT and ML datasets
" We also attempted to linearly bine thepredictions from different kernels, but again this gave no improvement"
 Combining the user and itembased versionsThe methods of bining the user and itembased versions mentioned in Section 
 did not give any signiﬁcantimprovement over the individual results for the whole dataset
" To check the performance for imbalanced datasets, we randomly selected users and movies from the SML dataset, and users and movies from the FT dataset and randomly withheld x of their ratings"
" We checked the performance for two cases for Case , the value of x was chosenuniformly at random to lie between and That is, x , , whereas for Case , the value of x lies between and That is, x , "
 The latter case creates a relatively imbalanced subset of the dataset as pared to the former one
"Table shows the performance of userbased, itembased, and different methods used to bine the individual versions"
 We use the average of user and itembased versions as a baseline
 We observe that linearly bining the individualremender systems does not give signiﬁcant improvement over the baseline and the same is true for the second methoddiscussed in Appendix 
" However, KMRVarHybrid does signiﬁcantly improve the performance, with pvalue in the case of pairt test pared with the baseline remender found to be less than for both datasets"
 Similar results were observedfor other datasets as well
" What is evident from Table is that user and itembased versions of the algorithm are plementary and can improve the performance, if bined in a systematic way, for the imbalanced dataset"
Table Combining the userbased and itembased versions under imbalanced datasets
 The Case produces a relatively sparse subset of the dataset pared to Case
 The best results are shown in bold font
ApproachKMRibKMRubKMRib KMRubKMRLinearHybridKMRCntKMRVarHybridHybridMAECase FT
 Information Sciences 
" Sparse, skewed, and imbalanced datasetsIn practical applications remender systems often have access to limited and highly skewed information"
 Examples ofthese are
New user coldstart scenario where new users have relatively few ratings
Newitem coldstart scenario where new items have relatively few ratings
Long tail scenario where the majority of items have only a few ratings
Imbalanced sparse datasets where the majority of usersitems have only a few ratings
In the datasets that we have used so far our test set consists of randomly chosen ratings and these are overwhelmingly inthe dense region of the rating matrix
" That is, the users that we tested typically have rated many items and the items havebeen rated by many users"
" Thus, the results we have described so far are not strongly inﬂuenced by problems of limited andskewed information"
" However, these problems are often vital for a remender system to prosper"
" For example, to attractnew users it is highly beneﬁcial to be able to give them good quality remendations before they have made many ratings"
"Similarly, to introduce new items into the system it is useful to make sensible remendations even if the item has onlygained a few ratings"
"Table Comparing MAE observed in different approaches under new user coldstart scenario, for the SML dataset"
" The superﬁx M represents the correspondingversion of the KMR algorithm, where we take into account the max, mean, mode, and median of the output probability distribution"
 The best results are shownin bold font
ApproachKMRibKMRubKMRDKMRFKMRMibKMRMubKMRMFKMRMDKMRMibFBest MAEMAE
Rating WeightMean WeightMode WeightMedian Weightinoitcderp lanif eht ni sroitcderp liaudvdnii eht fo sthgeWi
Number of Ratings SML DataSet 
 Weight learning over the validation set for the new user coldstart problem SML dataset
 ‘‘Number of Ratings’’ represents the number of ratingsgiven by an active user in the training set
 Information Sciences We have tested the four scenarios outlined above by modifying the datasets we have been using to exaggerate the sparseness or skewness of the data
 We found that in all cases the standard predictor that we have been using up to now gives verypoor performance
" However, we could very substantially improve the performance by bining the standard predictorwith predictions using the mean, median and mode of Wuqi as described in Section "
 In the tables shown belowwe denote the modiﬁed predictor with a superscript M
We concentrate on the newuser coldstart scenario as the results are representative of all four scenarios
 The only majordifference is in the newitem coldstart scenario where the featurebased and demographicbased remenders also perform well as they are less inﬂuenced by a lack of ratings
" Results for the new item coldstart, long tail, and sparse data scenarios are given in Appendix C"
" New user coldstart scenarioTo test the performance of the proposed algorithms under the new user coldstart scenario, we selected randomusers, and kept their number of ratings in the training set to , , , , and "
" Keeping the number of ratings less than ensures that a user is new, and it captures well the new user coldstart problem"
" The corresponding MAE, representedby MAE, MAE, MAE, MAE, and MAE is shown in Table "
 Using the standard predictor provides very poor performance
" We can substantially improve the performance by bining the standard predictor with predictions using themean, median and mode of Wuqi as described in Section "
"Recall that we learn the weights for bining the standard predictor with the predictor using the mean, mode and median"
 The value of the weights depend on the dataset
 shows how the weights that have been learned change in the newuser coldstart scenario as we increase the number of ratings in the training set
 The new user coldstart scenario is taken asan example similar results were observed in both the new item coldstart and long tail scenarios
 The xaxis shows thenumber of ratings given by users selected as coldstart users and the yaxis shows the weights associated with differentpredictors
" We observe that the contribution of the mode, mean, and median predictors decreases with the increase inthe number of ratings, and ﬁnally bee zero when the maximum number of ratings are available, whereas, the contribution of the standard ratingsbased predictor increases with the increase in the number of ratings, and bees when themaximum number of ratings are available"
 Conclusion and future workRemender systems is a major research area in machine learning and data mining
" A number of approaches have beenproposed to solve the remender system problem including contentbased ﬁltering, Ontologybased approaches, supervised classiﬁcation techniques, unsupervised clustering techniques, memorybased collaborative ﬁltering, modelbased approaches spanning a number of algorithms including singular value deposition, matrix factorisation techniques, andprincipal ponent analysis"
" All these algorithms suffer from potential problems such as accuracy, scalability, sparsityand imbalanced dataset problems, coldstart problems, and long tail problems in one way or the other"
" Against this background, we propose a new class of kernelbased remendation algorithms that give stateoftheart performance andeliminates the recorded problems with the remender systems making the remendation generation techniquesapplicable to a wider range of practical situations and realworld scenarios"
"The proposed algorithm is petitive with what we believe to be the remender with the best performance proposedby ,"
 Interestingly both the proposed algorithm and the remender proposed in use kernelbased methodsthough in a very different way
" Although kernelbased techniques are known to give excellent performance, remendersystems are challenging because of the size of the datasets"
 By carefully choosing the constraints we have been able to createa kernelbased learning machine that can be trained in linear time in the number of data points
"The algorithm we have developed is very ﬂexible, thus we can easily adapt it so that it is either userbased or itembased"
In addition it can use other information such as textbased features and these features can be easily bined
 The best algorithm on the large datasets switches between the userbased and itembased information depending on the reliability of thepredictions as measured by the spread in the prediction of the algorithms
One interesting feature of the proposed approach is that we map the residues in the ratings onto a density function whichencodes the uncertainty in the residue
 For unseen residues we have interpreted the mapping Wuqi as an approximationto a density function for the residue
" Even though this function is not itself a density function it bees negative in someregions and is not normalised, nevertheless, it is very useful to consider the positive part of the function as a density function from which we can measure the mean, mode, median and variance"
" These measurements help in improving the performance, particularly in the case of sparse data"
One of the current drawbacks of the proposed algorithm is that the training occurs in one step
" Thus, when new data are addedit is costly to retrain the system"
 For practical remender systems this is a signiﬁcant problem as ratings are typically beingadded continuously
 We are currently investigating using a perceptronlike algorithm for updating the Lagrange multipliers
"AcknowledgmentsThe work reported in this paper has formed part of the Instant Knowledge Research Programme of Mobile VCE, the Virtual Centre of Excellence in Mobile & Personal Communications, www"
 The programme is cofunded by theM
 Information Sciences UK Technology Strategy Board’s Collaborative Research and Development programme
 Detailed technical reports on this research are available to all Industrial Members of Mobile VCE
" The third author has received funding from the European Community’s Seventh Framework Programme FP Speciﬁc Programme Cooperation, Theme , Information andCommunication Technologies under grant agreement No"
" Parameters brat, bfeat, and bdemoIn this section, we describe how we tuned the other parameters of the system"
"Parameters brat, bfeat, and bdemo brat bfeat determine the relative weights of rating, feature, and demographic kernelsin the ﬁnal prediction"
 Note that we assume the three bvalues are all positive and sum to one
" Sixtysix parameter sets weregenerated by producing all possible bination of parameters values, ranging from to "
 with differences of 
 Theparameter sets brat and bfeat gave the lowest MAE for all the datasets
Parameters q and q determine the relative weights of userbased and itembased CF in the ﬁnal prediction respectively
 We changed the value of q from to with a difference of 
 and the resulting MAE has been shown in A
" shows that for the SML dataset, the MAE is minimum at q "
", after which it starts increasing again whereas,for the FT dataset, the MAE keeps on decreasing, reaches its minimum at q "
", an then increases again"
 We choose theoptimal value of q to be 
 for SML and the FT dataset respectively
" Similarly, the value of q was trained for otherdatasets"
" It is worth noting that the itembased version got more weight except for the FT dataset in the ﬁnal prediction, forall datasets"
"In the hybrid variant, KMRCntHybrid the parameter hCnt determines the switching point between using the itembased anduserbased algorithms depending on the number of ratings of the item and the user"
 We determine the best value of hCntby varying it between and in steps of 
" shows the parameter hCnt learned for Case , as discussed in Section"
 over the validation set
" We observe that for the SML dataset, the MAE keeps on decreasing with the increase in the valueof hCnt, reaches its minimum at hCnt "
 and then either stays stable or starts increasing again
" For the FT dataset, theMAE decreases initially, when the value of hCnt changes from to "
 and then starts increasing when the value of hCnt increases beyond 
" For this reason, we choose the value hCnt to be "
 for SML and the FT datasets respectively
"Similarly, the value of hCnt was trained for other datasets"
"In the hybrid algorithm, KMRVarHybrid, the parameter hVar controls the switching from the userbased prediction to the itembased prediction depending on the uncertainty in the predictions measured by the variance in the Wuqi"
 To learn thisparameter we changed its value from to in steps of 
 and observed the corresponding MAE
 shows the parameter hVar learned for Case as discussed in Section 
" We observe that for the SML dataset, the MAEkeeps on decreasing with the increase in the value of hVar, reaches its peak at "
", and then starts increasing again"
" For the FTdataset, the decrease in the MAE is not very signiﬁcant, when hVar "
" however, afterwards, a sharp decrease in the MAE isobserved"
" The MAE keeps on decreasing, reaches its minimum at "
", and then either it stays stable or starts increasingagain"
 We choose the optimal value hVar to be 
 for SML and the FT datasets respectively
" Similarly, the valueof hVar was trained for other datasets"
 Parameter hVarAppendix B
 Mean Absolute Error MAEsigned by the user
" It is puted asMAE jpi aijnnXiMAE measures the average absolute deviation between a remender system’s predicted rating and a true rating aswhere pi and ai are the predicted and actual values of a rating, respectively, and n is the total number of rating records in thetest set"
" A rating record is a tuple consisting of a user ID, movie ID, and rating, huid,mid,ri, where r is the rating a reM"
 Information Sciences 
Value of Threshold Parameter ρ SML DataSet
EAM rorrlE etuosbA naeM
EAM rorrlE etuosbA naeM
Hybrid UBIBUBIBHybrid UBIBUBIB
Value of Threshold Parameter ρ FT DataSet
" Learning the optimal value of threshold parameter q, over the validation set, for the imbalanced datasets refer to Section "
mender system has to predict
" It has been used in ,,,,,,,,,"
 The aim of a remender system is tominimise the MAE score
"Normalised Mean Absolute Error NMAE has been used in ,, and is puted by normalising the MAE by a factor"
"The value of the factor depends on the range of the ratings for example, for the MovieLens dataset, it is "
" The motivationbehind this approach is that, the random guessing produces a score of "
" For further information, refer to "
"A closely related measure to the MAE is the Root Mean Squared Error RMSE, which is calculated as followsRMSE vuutnXpi ainiBoth MAE and RMSE are quoted in the literature"
 RMSE will be slightly more sensitive to large outliers
 The RMSE value willalways be greater than or equal to the MAE value
 Receiver Operating Characteristic ROC sensitivityROC measures the extent to which an information ﬁltering system can distinguish between good and bad items
 ROC sensitivity measures the probability with which a system accepts a good item
 The ROC sensitivity ranges from perfect to imperfect with 
" To use this metric for remender systems, we must ﬁrst determine which items are goodsignal and which are bad noise"
 We followed the procedure describe in while using this metric
" It has been used in,,"
 shows the ROCsensitive for the same set of algorithms on the same datasets as shown in Table 
" Precision, Recall, and FPrecision, recall, and F evaluate the effectiveness of a remender system by measuring the frequency with which ithelps users selectingremending a good item"
 Precision gives us the probability that a selected item is relevant
 Information Sciences EAM rorrlE etuosbA naeM
EAM rorrlE etuosbA naeM
Value of Threshold Parameter θ
 SML DataSetCntHybrid UBIBUBIBHybrid UBIBUBIB
Value of Threshold Parameter θ
" Learning the optimal value of threshold parameter, hCnt over the validation set, for the imbalanced datasets refer to Section "
gives us the probability that a relevant item is selected
" Precision and recall should be reported together, as increasing theprecision typically reduces the recall"
" The F Measure bines the precision and recall into a single metric and hasbeen used in many research projects, e"
" F is puted as followsF Precision RecallPrecision RecallThe ﬁrst step in puting the precision and recall is to divide items into two classes relevant and irrelevant, which isthe same as in ROCsensitivity"
" We calculated precision, recall, and F measures for each user, and reported the average results over all users"
 shows the F measure for the same set of algorithms on the same datasets as shown in Table 
" Sparse, skewed, and imbalanced datasetsIn this appendix we present results for the new item coldstart scenario, the long tail scenario and for sparse datasets"
 Performance evaluation under new item coldstart scenarioWe tested the new item coldstart scenario in exactly the same way we did the new user coldstart scenario
" That is, weselected random items, and kept the number of users in the training set who have rated the these item to , , , , and"
" shows again that the standard predictor fails under this scenario, whereas including the mean, mode and median predictor gives very good performance"
 We note that for new items the featurebased and demographicbased remenders work well for the coldstart scenario as these measures are not strongly inﬂuenced by a lack of ratinginformation for an item
 Information Sciences 
EAM rorrlE etuosbA naeM
EAM rorrlE etuosbA naeM
Value of Threshold Parameter θ
 SML DataSetvarHybrid UBIBUBIBHybrid UBIBUBIB
Value of Threshold Parameter θ
" Learning the optimal value of threshold parameter hvar, over the validation set, for the imbalanced datasets refer to Section "
A parison of the proposed algorithm with others in terms of ROCSensitivity metric
 The average with the respective standard deviation of results over fold is shown
 The best results are shown in bold font
AlgorithmBest ROCsensitivitySML
A parison of the proposed algorithm with others in terms of F measured over top remendations metric
 The average with the respective standarddeviation of results over fold is shown
 The best results are shown in bold font
Userbased CFItembased CFHybrid CFSVDKMRibKMRubKMRVarHybridAlgorithmUserbased CFItembased CFHybrid CFSVDKMRibKMRubKMRVarHybridM
 Information Sciences Table A
"Comparing the MAE observed in different approaches under new item coldstart scenario, for the SML dataset"
" The superﬁx M represents the correspondingversion of the KMR algorithm, where we take into account the max, mean, mode, and median of the output probability distribution"
 The average with therespective standard deviation of results over fold is shown
 The best results are shown in bold font
"Comparing MAE observed in different approaches under long tail scenario, for the SML dataset"
" The superﬁx M represents the corresponding version of theKMR algorithm, where we take into account the max, mean, mode, and median of the output probability distribution"
 The average with the respective standarddeviation of results over fold is shown
 The best results are shown in bold font
ApproachBest MAEMAEMAEMAEMAEMAEMAE
Comparing the performance of the algorithms under imbalanced and sparse datasets
" The superﬁx M represents the corresponding version of the KMRalgorithm, where we take into account the max, mean, mode, and median of the output probability distribution"
 The average with the respective standarddeviation of results over fold is shown
 The best results are shown in bold font
ApproachKMRibKMRubKMRFKMRDKMRMibKMRMubKMRMFKMRMDKMRMFibKMRibKMRubKMRFKMRDKMRMibKMRMubKMRMFKMRMDKMRMibFApproachKMRibKMRubKMRDKMRFKMRMibKMRMubKMRMDKMRMFKMRMibFC
 Performance evaluation under long tail scenarioThe long tail scenario is an important scenario for practical remender systems
" In a large Emerce system likeAmazon, there are huge numbers of items that are rated by very few users and hence the remendations generated forthese items would be poor, which could weaken the customers’ trust in the system"
To test the performance of the proposed algorithms under long tail scenario we created the artiﬁcial long tail scenario byrandomly selecting the of items in the tail
 The number of ratings given in the tail part were varied between andM
 Information Sciences —this ensures that the item is new and has very few ratings
" again shows the failure of the standard predictor inthe long tail scenario and the improvement obtained by using the mean, mode and median predictor"
" Performance evaluation under very sparse and imbalanced datasetsTo check the performance of the proposed approaches under very sparse and imbalanced datasets, we created subsets ofthe datasets by withholding x of the ratings from a rating proﬁle of useritem, where x xmin, xmax"
" We show results fortwo scenarios xmin , xmax , xmin , xmax "
" Changing the value of xmin creates different sparse subsets of the dataset, whereas keeping the value of xmax to ensures that the imbalanced dataset is created for eachscenario"
"For the SML and FT datasets, the results are shown in Table A"
 Again this follows the same pattern as the long tail andcoldstart scenarios
" Hyung Jun Ahn, A new similarity measure for collaborative ﬁltering to alleviate the new user coldstarting problem, Information Sciences Katja Astikainen, Liisa Holm, Esa Pitkanen, Sandor Szedmak, Juho Rousu, Towards structured output prediction of enzyme function, BMC Proceedings J"
" Hofmann, Unifying collaborative and contentbased ﬁltering, in Proceedings of the TwentyFirst International Conference on MachineLearning New York, NY, USA, ACM Press, , pp"
" Volinsky, The BellKor solution to the Netﬂix prize, in AT& T LabsResearch Technical report November, "
" Bell, Yehuda Koren, Lessons from the netﬂix prize challenge, SIGKDD Explorations Newsletters "
" Yolanda BlancoFernández, Martı´n LópezNores, Alberto GilSolla, Manuel RamosCabrer, José J"
" PazosArias, Exploring synergies between contentbased ﬁltering and spreading activation techniques in knowledgebased remender systems, Information Sciences "
" Breese, David Heckerman, and Carl Kadie, Empirical analysis of predictive algorithms for collaborative ﬁltering, in Proceedings of theFourteenth Conference on Uncertainty in Artiﬁcial Intelligence San Francisco, CA, USA, UAI’, Morgan Kaufmann Publishers Inc"
" Robin Burke, Integrating knowledgebased and collaborativeﬁltering remender systems, in AAAI Workshop on AI in Electronic Commerce, AAAI,, pp"
" Robin Burke, Hybrid remender systems Survey and experiments, User Modeling and UserAdapted Interaction "
" Mark Claypool, Anuja Gokhale, Tim Mir, Pavel Murnikov, Dmitry Netes, Matthew Sartin, Combining contentbased and collaborative ﬁlters in an onlinenewspaper, in Proceedings of ACM SIGIR Workshop on Remender Systems Berkeley, California, ACM, "
" Dennis DeCoste, Collaborative prediction using ensembles of maximum margin matrix factorizations, in Proceedings of the rd InternationalConference on Machine Learning New York, NY, USA, ICML ’, ACM, , pp"
" Ghazanfar, Adam PrügelBennett, An improved switching hybrid remender system using naive bayes classiﬁer and collaborativeﬁltering, in Lecture Notes in Engineering and Computer Science Proceedings of The International Multi Conference of Engineers and ComputerScientists , IMECS , March, , Hong Kong, , pp"
" Ghazanfar, Adam PrügelBennett, Building switching hybrid remender system using machine learning classiﬁers and collaborativeﬁltering, IAENG International Journal of Computer Science "
" Ghazanfar, Adam PrügelBennett, Novel signiﬁcance weighting schemes for collaborative ﬁltering generating improvedremendations in sparse environments, in DMIN, CSREA Press, , pp"
" Ghazanfar, Adam PrügelBennett, A scalable, accurate hybrid remender system, in Proceedings of the Third InternationalConference on Knowledge Discovery and Data Mining Washington, DC, USA, WKDD ’, IEEE Computer Society, , pp"
" Ghazanfar, Adam PrügelBennett, The advantage of careful imputation sources in sparse dataenvironment of remender systemsgenerating improved svdbased remendations, in IADIS European Conference on Data Mining, July "
" David Goldberg, David Nichols, Brian M"
" Oki, Douglas Terry, Using collaborative ﬁltering to weave an information tapestry, Communications of the Jonathan L"
" Riedl, Evaluating collaborative ﬁltering remender systems, ACM Transactions onACM "
" Thorsten Joachims, Training linear svms in linear time, in Proceedings of the th ACM SIGKDD International Conference on Knowledge Discovery andData Mining New York, NY, USA, KDD ’, ACM, , pp"
" HeungNam Kim, Abdulmotaleb ElSaddik, GeunSik Jo, Collaborative errorreﬂected models for coldstart remender systems, Decision Support Joseph A"
" Miller, David Maltz, Jonathan L"
" Gordon, John Riedl, Grouplens applying collaborative ﬁltering to usenetSystems "
"news, Communications of the ACM "
" Yehuda Koren, Factorization meets the neighborhood a multifaceted collaborative ﬁltering model, in Proceedings of the th ACM SIGKDDInternational Conference on Knowledge Discovery and Data Mining New York, NY, USA, KDD ’, ACM, , pp"
" Yehuda Koren, Factor in the neighbors scalable and accurate collaborative ﬁltering, ACM Transactions on Knowledge Discovery from Data TKDD "
" Csalogány, Methods for large scale SVD with missing values, in Proceedings of KDD Cup and Workshop, Citeseer, "
" Ken Lang, NewsWeeder learning to ﬁlter netnews, in Proceedings of the th International Conference on Machine Learning, Morgan Kaufmannpublishers Inc"
", San Mateo, CA, USA, , pp"
" Lawrence, Raquel Urtasun, Nonlinear matrix factorization with gaussian processes, in Proceedings of the th Annual InternationalConference on Machine Learning New York, NY, USA, ICML ’, ACM, , pp"
" DuenRen Liu, PeiYun Tsai, PoHuan Chiu, Personalized remendation of popular blog articles for mobile applications, Information Sciences Lester Mackey, David Weiss, Michael I"
" Jordan, Mixed membership matrix factorization, in Proceedings of the th International Conference on "
"Machine Learning, June "
" Benjamin Marlin, Collaborative ﬁltering a machine learning perspective, Master’s thesis, University of Toronto, "
" Benjamin Marlin, Modeling user rating proﬁles for collaborative ﬁltering, Advances in Neural Information Processing Systems "
" Rahul Mazumder, Trevor Hastie, Robert Tibshirani, Spectral regularization algorithms for learning large inplete matrices, Journal of MachineLearning Research "
" Prem Melville, Raymod J"
" Mooney, Ramadass Nagarajan, Contentboosted collaborative ﬁltering for improved remendations, in EighteenthNational Conference on Artiﬁcial Intelligence Menlo Park, CA, USA, American Association for Artiﬁcial Intelligence, , pp"
 Information Sciences Stuart E
" Middleton, Capturing knowledge of user preferences with remender systems, Ph"
" thesis, UNIVERSITY OF SOUTHAMPTON, UK, Stuart E"
" Middleton, Harith Alani, David C"
" De Roure, Exploiting synergy between ontologies and remender systems, in The Eleventh InternationalSeptember "
"World Wide Web Conference WWW, "
" Mooney, Loriene Roy, Contentbased book remending using learning for text categorization, in Proceedings of the Fifth ACMConference on Digital Libraries New York, NY, USA, DL ’, ACM, , pp"
" Pennock, Applying collaborative ﬁltering techniques to movie search for better ranking and browsing, in Proceedings of the th ACMSIGKDD International Conference on Knowledge Discovery and Data Mining, ACM, , pp"
" YoonJoo Park, Alexander Tuzhilin, The long tail of remender systems and how to leverage it, in Proceedings of the ACM Conference onRemender Systems New York, NY, USA, RecSys ’, ACM, , pp"
" Pazzani, A framework for collaborative, contentbased and demographic ﬁltering, Artiﬁcial Intelligence Review "
" Pazzani, Daniel Billsus, The Adaptive Web, SpringerVerlag, Berlin, Heidelberg, "
" Pennock, Eric Horvitz, Steve Lawrence, C"
" Lee Giles, Collaborative ﬁltering by personality diagnosis a hybrid memory and modelbasedapproach, in Proceedings of the th Conference on Uncertainty in Artiﬁcial Intelligence San Francisco, CA, USA, UAI ’, Morgan KaufmannPublishers Inc"
" HerreraViedma, A hybrid remender system for the selective dissemination of research resources in atechnology transfer ofﬁce, Information Sciences "
" Rennie Nathan Srebro, Fast maximum margin matrix factorization for collaborative prediction, in Proceedings of the nd InternationalConference on Machine Learning New York, NY, USA, ICML ’, ACM, , pp"
" Paul Resnick, Neophytos Iacovou, Mitesh Suchak, Peter Bergstrom, John Riedl, Grouplens an open architecture for collaborative ﬁltering of netnews,in Proceedings of the ACM Conference on Computer Supported Cooperative Work, CSCW ’, ACM, , pp"
" Mnih, Probabilistic matrix factorization, Advances in Neural Information Processing Systems "
" Badrul Sarwar, George Karypis, Joseph Konstan, John Reidl, Itembased collaborative ﬁltering remendation algorithms, in Proceedings of the thInternational Conference on World Wide Web New York, NY, USA, WWW ’, ACM, , pp"
" Badrul Sarwar, George Karypis, Joseph Konstan, John Riedl, Analysis of remendation algorithms for emerce, in Proceedings of the nd ACMConference on Electronic Commerce New York, NY, USA, EC ’, ACM, , pp"
" Badrul Sarwar, George Karypis, Joseph Konstan, John Riedl, Application of dimensionality reduction in remender systema case study, in ACMWEBKDD Workshop, Citeseer, "
" Badrul Sarwar, George Karypis, Joseph Konstan, John Riedl, Remender systems for largescale emerce scalable neighborhood formation usingclustering, in Proceedings of the Fifth International Conference on Computer and Information Technology, "
" Vincent SchickelZuber, Boi Faltings, Using an ontological apriori score to infer user’s preferences, in Workshop on Remender SystemsECAI, Jesus SerranoGuerrero, Enrique HerreraViedma, Jose A"
" Olivas, Andres Cerezo, Francisco P"
" Romero, A google wavebased fuzzy remender systemto disseminate information in university digital libraries "
", Information Sciences "
" Upendra Shardanand, Pattie Maes, Social information ﬁltering algorithms for automating word of mouth, in Proceedings of the SIGCHI Conference onHuman factors in Computing Systems New York, NY, USA, CHI ’, ACM PressAddisonWesley Publishing Co"
" Nathan Srebro, Jasson D"
" Jaakkola, Maximummargin matrix factorization, Advances in Neural Information Processing Systems , pp"
" Stern, Ralf Herbrich, Thore Graepel, Matchbox large scale online bayesian remendations, in Proceedings of the th InternationalConference on World Wide Web New York, NY, USA, WWW ’, ACM, , pp"
" Sandor Szedmak, Ni Yizhao, R"
" Gunn Steve, Maximum margin learning with inplete data learning networks instead of tables, Journal of MachineLearning Research Proceedings Track "
" Gábor Takács, István Pilászy, Bottyán Németh, Domonkos Tikk, Investigation of various matrix factorization methods for large remender systems,in Proceedings of the nd KDD Workshop on LargeScale Remender Systems and the Netﬂix Prize Competition New York, NY, USA, NETFLIX ’,ACM, , pp"
" Gábor Takács, István Pilászy, Bottyán Németh, Domonkos Tikk, Scalable collaborative ﬁltering approaches for large remender systems, Journal of Loren Terveen, Will Hill, Brian Amento, David McDonald, Josh Creter, Phoaks a system for sharing remendations, Communications of the ACM Machine Learning Research "
" Robin van Meteren, Maarten van Someren, Using contentbased ﬁltering for remendation, in Proceedings of the Machine Learning in the NewInformation Age MLnetECML Workshop, Citeseer, "
" Manolis Vozalis, Konstantinos G"
" Margaritis, Applying SVD on generalized itembased ﬁltering, International Journal of Computer Science andApplications "
" Manolis Vozalis, Konstantinos G"
" Margaritis, Using svd and demographic data for the enhancement of generalized collaborative ﬁltering, Information Mingru Wu, Collaborative ﬁltering via ensembles of matrix factorizations, in Proceedings of KDD Cup and Workshop, Citeseer, "
" GuiRong Xue, Chenxi Lin, Qiang Yang, WenSi Xi, HuaJun Zeng, Yong Yu, Zheng Chen, Scalable collaborative ﬁltering using clusterbased smoothing,in Proceedings of the th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval New York, NY, USA,SIGIR ’, ACM, , pp"
" Daoqiang Zhang, ZhiHua Zhou, Songcan Chen, Nonnegative matrix factorization on kernels, in Proceedings of the th Paciﬁc Rim InternationalConference on Artiﬁcial Intelligence Berlin, Heidelberg, PRICAI’, SpringerVerlag, , pp"
"See discussions, stats, and author profiles for this publication at httpswww"
"netpublicationThe Unreasonable Effectiveness of DataArticle in Intelligent Systems, IEEE · May DOI "
" · Source IEEE XploreCITATIONS authors, includingAlon HalevyGoogle Inc"
"SEE PROFILEREADS,Peter NorvigGoogle Inc"
"SEE PROFILE PUBLICATIONS , CITATIONS PUBLICATIONS , CITATIONS Some of the authors of this publication are also working on these related projectsBiomedical data integration View projectAll content following this page was uploaded by Peter Norvig on December "
The user has requested enhancement of the downloaded file
"E X P E R T O P I N I O NContact Editor Brian Brannon, bbrannonputer"
"orgThe Unreasonable Effectiveness of DataAlon Halevy, Peter Norvig, and Fernando Pereira, GoogleEugene Wigner’s article “The Unreasonable Effectiveness of Mathematics in the Natural Sciences” examines why so much of physics can be neatly explained with simple mathematical formulassuch as f ma or e mc"
" Meanwhile, sciences that involve human beings rather than elementary particles have proven more resistant to elegant mathematics"
 Economists suffer from physics envy over their inability to neatly model human behavior
" An informal, inplete grammar of the English language runs over , pages"
" Perhaps when it es to natural language processing and related fi elds, we’re doomed to plex theories that will never have the elegance of physics equations"
" But if that’s so, we should stop acting as if our goal is to author extremely elegant theories, and instead embrace plexity and make use of the best ally we have the unreasonable effectiveness of data"
"One of us, as an undergraduate at Brown University, remembers the excitement of having access to the Brown Corpus, containing one million English words"
" Since then, our fi eld has seen several notable corpora that are about times larger, and in , Google released a trillionword corpus with frequency counts for all sequences up to fi ve words long"
" In some ways this corpus is a step backwards from the Brown Corpus it’s taken from unfi ltered Web pages and thus contains inplete sentences, spelling errors, grammatical errors, and all sorts of other errors"
 It’s not annotated with carefully handcorrected partofspeech tags
 But the fact that it’s a million times larger than the Brown Corpus outweighs these drawbacks
" A trillionword corpus—along with other Webderived corpora of millions, billions, or trillions of links, videos, images, tables, and user interactions—captures even very rare aspects of human behavior"
" So, this corpus could serve as the basis of a plete model for certain tasks—if only we knew how to extract the model from the data"
Learning from Text at Web ScaleThe biggest successes in naturallanguagerelated machine learning have been statistical speech recognition and statistical machine translation
 The reason for these successes is not that these tasks are easier than other tasks they are in fact much harder than tasks such as document classifi cation that extract just a few bits of information from each document
 The reason is that translation is a natural task routinely done every day for a real human need think of the operations of the European Union or of news agencies
 The same is true of speech transcription think of closedcaption broadcasts
" In other words, a large training set of the inputoutput behavior that we seek to automate is available to us in the wild"
" In contrast, traditional natural language processing problems such as document classifi cation, partofspeech tagging, namedentity recognition, or parsing are not routine tasks, so they have no large corpus available in the wild"
" Instead, a corpus for these tasks requires skilled human annotation"
" Such annotation is not only slow and expensive to acquire but also diffi cult for experts to agree on, being bedeviled by many of the diffi culties we discuss later in relation to the Semantic Web"
 The fi rst lesson of Webscale learning is to use available largescale data rather than hoping for annotated data that isn’t available
" For instance, we fi nd that useful semantic relationships can be automatically learned from the statistics of search queries and the corresponding results or from the accumulated evidence of Webbased text patterns and formatted tables, in both cases without needing any manually annotated data"
 IEEEPublished by the IEEE Computer SocietyiEEE iNTElliGENT SYSTEMSAuthorized licensed use limited to Univ of Calif Berkeley
" Downloaded on February , at from IEEE Xplore"
 Another important lesson from statistical methods in speech recognition and machine translation is that memorization is a good policy if you have a lot of training data
 The statistical language models that are used in both tasks consist primarily of a huge database of probabilities of short sequences of consecutive words ngrams
 These models are built by counting the number of occurrences of each ngram sequence from a corpus of billions or trillions of words
" Researchers have done a lot of work in estimating the probabilities of new ngrams from the frequencies of observed ngrams using, for example, GoodTuring or KneserNey smoothing, leading to elaborate probabilistic models"
" But invariably, simple models and a lot of data trump more elaborate models based on less data"
" Similarly, early work on machine translation relied on elaborate rules for the relationships between syntactic and semantic patterns in the source and target languages"
" Currently, statistical translation models consist mostly of large memorized phrase tables that give candidate mappings between specific source and targetlanguage phrases"
"Instead of assuming that general patterns are more effective than memorizing specific phrases, today’s translation models introduce general rules only when they improve translation over just memorizing particular phrases for instance, in rules for dates and numbers"
 Similar observations have been made in every other application of machine learning to Web data simple ngram models or linear classifiers based on millions of specific features perform better than elaborate models that try to discover general rules
 In many cases there appears to be a threshold of sufficient data
" For example, James Hays and Alexei A"
" Efros addressed the task of scene pletion removing an unwanted, unsightly automobile or exspouse from a photograph and filling in the background with pixels taken from a large corpus of other photos"
" With a corpus of thousands of photos, the results were poor"
" But once they accumulated millions of photos, the same algorithm performed quite well"
" We know that the number of grammatical English sentences is theoretically infinite and the number of possible Mbyte photos is ,,"
" However, in practice we humans care to make only a finite number of distinctions"
" For many tasks, once we have a billion or so examples, we essentially have a closed set that repreFor many tasks, words and word binations provide all the representational machinery we need to learn from text"
"sents or at least approximates what we need, without generative rules"
"For those who were hoping that a small number of general rules could explain language, it is worth noting that language is inherently plex, with hundreds of thousands of vocabulary words and a vast variety of grammatical constructions"
" Every day, new words are coined and old usages are modified"
 This suggests that we can’t reduce what we want to say to the free bination of a few abstract primitives
" For those with experience in smallscale machine learning who are worried about the curse of dimensionality and overfitting of models to data, note that all the experimental evidence from the last decade suggests that throwing away rare events is almost always a bad idea, because much Web data consists of individually rare but collectively frequent events"
" For many tasks, words and word binations provide all the representational machinery we need to learn from text"
 Human language has evolved over millennia to have words for the important concepts let’s use them
 Abstract representations such as clusters from latent analysis that lack linguistic counterparts are hard to learn or validate and tend to lose information
 Relying on overt statistics of words and word cooccurrences has the further advantage that we can estimate models in an amount of time proportional to available data and can often parallelize them easily
" So, learning from the Web bees naturally scalable"
The success of ngram models has unfortunately led to a false dichotomy
" Many people now believe there are only two approaches to natural language processinga deep approach that relies on handcoded grammars and ontologies, represented as plex networks of relations and a statistical approach that relies on learning ngram statistics from large corpora"
"	In reality, three orthogonal problems arise choosing a representation language,encoding a model in that language, and performing inference on the model"
" Each problem can be addressed in several ways, resulting in dozens of approaches"
" The deep approach that was popular in the s used firstorder logic or something similar as the representation language, encoded a model with the labor of a team of graduate students, and did inference with plex inference rules appropriate to the representation language"
" In the s and s, it became fashionable to Marchapril www"
orgintelligent Authorized licensed use limited to Univ of Calif Berkeley
" Downloaded on February , at from IEEE Xplore"
" use finite state machines as the representation language, use counting and smoothing over a large corpus to encode a model, and use simple Bayesian statistics as the inference method"
"But many other binations are possible, and in the s, many are being tried"
" For example, Lise Getoor and Ben Taskar collect work on statistical relational learning—that is, representation languages that are powerful enough to represent relations between objects such as firstorder logic but that have a sound, probabilistic definition that allows models to be built by statistical learning"
 Taskar and his colleagues show how the same kind of maximummargin classifier used in support vector machines can improve traditional parsing
" Stefan Schoenmackers, Oren Etzioni, and Daniel S"
 Weld show how a relational logic and a millionpage corpus can answer questions such as “what vegetables help prevent osteoporosis” by isolating and bining the relational assertions that “kale is high in calcium” and “calcium helps prevent osteoporosis
”Semantic Web versus Semantic InterpretationThe Semantic Web is a convention for formal representation languages that lets software services interact with each other “without needing artificial intelligence
"” A software service that enables us to make a hotel reservation is transformed into a Semantic Web service by agreeing to use one of several standards for representing dates, prices, and locations"
 The service can then interoperate with other services that use either the same standard or a different one with a known translation into the chosen standard
" As Tim BernersLee, James Hendler, and Ora Lassila write, “The Semantic Web will enable machines to prehend semantic documents and data, not human speech and writings"
”The problem of understanding human speech and writing—the semantic interpretation problem—is quite different from the problem of software service interoperability
" Semantic interpretation deals with imprecise, ambiguous natural languages, whereas service interoperability deals with making data precise enough that the programs operating on the data will function effectively"
" Unfortunately, the fact that the word “semantic” appears in both “Semantic Web” and “semantic interpretation” means that the two probBecause of a huge shared cognitive and cultural context, linguistic expression can be highly ambiguous and still often be understood correctly"
"lems have often been conflated, causing needless and endless consternation and confusion"
 The “semantics” in Semantic Web services is embodied in the code that implements those services in accordance with the specifications expressed by the relevant ontologies and attached informal documentation
 The “semantics” in semantic interpretation of natural languages is instead embodied in human cognitive and cultural processes whereby linguistic expression elicits expected responses and expected changes in cognitive state
" Because of a huge shared cognitive and cultural context, linguistic expression can be highly ambiguous and still often be understood correctly"
" Given these challenges, building Semantic Web services is an engineering and sociological challenge"
" So, even though we understand the required technology, we must deal with significant hurdles Ontology writing"
 The important easy cases have been done
" For example, the Dublin Core defines dates, locations, publishers, and other concepts that are sufficient for card catalog entries"
"org defines chromosomes, species, and gene sequences"
 Other organizations provide ontologies for their specific fields
 But there’s a long tail of rarely used concepts that are too expensive to formalize with current technology
" Project Halo did an excellent job of encoding and reasoning with knowledge from a chemistry textbook, but the cost was US, per page"
 Obviously we can’t afford that cost for a trillion Web pages
 PubDifficulty of implementation
 lishing a static Web page written in natural language is easy anyone with a keyboard and Web connection can do it
" Creating a databasebacked Web service is substantially harder, requiring specialized skills"
 Making that service pliant with Semantic Web protocols is harder still
" Major sites with petent technology experts will find the extra effort worthwhile, but the vast majority of small sites and individuals will find it too difficult, at least with current tools"
" In some domains, peting factions each want to promote their own ontology"
" In other domains, the entrenched leaders of the field oppose any ontology because it would level the playing field for their petitors"
" This is a problem in diplomacy, not technology"
" As Tom Gruber says, “Every ontology is a treaty—a social agreement—among people with some mon motive in sharing"
"” When a motive for sharing is lacking, so are mon ontologies"
Inaccuracy and deception
orgintelligent iEEE iNTElliGENT SYSTEMSAuthorized licensed use limited to Univ of Calif Berkeley
" Downloaded on February , at from IEEE Xplore"
 know how to build sound inference mechanisms that take true premises and infer true conclusions
" But we don’t have an established methodology to deal with mistaken premises or with actors who lie, cheat, or otherwise deceive"
" Some work in reputation management and trust exists, but for the time being we can expect Semantic Web technology to work best where an honest, selfcorrecting group of cooperative users exists and not as well where petition and deception exist"
The challenges for achieving accurate semantic interpretation are different
 We’ve already solved the sociological problem of building a network infrastructure that has encouraged hundreds of millions of authors to share a trillion pages of content
 We’ve solved the technological problem of aggregating and indexing all this content
" But we’re left with a scientific problem of interpreting the content, which is mainly that of learning as much as possible about the context of the content to correctly disambiguate it"
 The semantic interpretation problem remains regardless of whether or not we’re using a Semantic Web framework
" The same meaning can be expressed in many different ways, and the same expression can express many different meanings"
" For example, a table of pany information might be expressed in ad hoc HTML with column headers called “Company,” “Location,” and so on"
" Or it could be expressed in a Semantic Web format, with standard identifiers for “Company Name” and “Location,” using the Dublin Core Metadata Initiative pointencoding scheme"
" But even if we have a formal Semantic Web “Company Name” attribute, we can’t expect to have an ontology for every possible value of this attribute"
" For example, we can’t know for sure what pany the string “Joe’s Pizza” refers to because hundreds of businesses have that name and new ones are being added all the time"
 We also can’t always tell which business is meant by the string “HP
” It could refer to Helmerich & Payne Corp
 when the column is populated by stock ticker symbols but probably refers to HewlettPackard when the column is populated by names of large technology panies
 The problem of semantic interpretation remains using a Semantic Web formalism just means that semantic interpretation must be done on shorter strings that fall between angle brackets
belong in the same column of a table
 We’ve never before had such a vast collection of tables and their schemata at our disposal to help us resolve semantic heterogeneity
" Using such a corpus, we hope to be able to acplish tasks such as deing when “Company” and “Company Name” are synonyms, deing when “HP” means Helmerich & Payne or HewlettPackard, and determining that an object with attributes “passengers” and “cruising altitude” is probably an aircraft"
"The same meaning can be expressed in many different ways, and the same expression can express many different meanings"
 What we need are methods to infer relationships between column headers or mentions of entities in the world
" These inferences may be incorrect at times, but if they’re done well enough we can connect disparate data collections and thereby substantially enhance our interaction with Web data"
" Interestingly, here too Webscale data might be an important part of the solution"
 The Web contains hundreds of millions of independently created tables and possibly a similar number of lists that can be transformed into tables
 These tables represent structured data in myriad domains
 They also represent how different people organize data—the choices they make for which columns to include and the names given to the columns
" The tables also provide a rich collection of column values, and values that they deed ExamplesHow can we use such a corpus of tables Suppose we want to find synonyms for attribute names—for example, when “Company Name” could be equivalent to “Company” and “price” could be equivalent to “discount”"
" Such synonyms differ from those in a thesaurus because here, they are highly context dependent both in tables and in natural language"
" Given the corpus, we can extract a set of schemata from the tables’ column labels for example, researchers reliably extracted "
" million distinct schemata from a collection of million tables, not all of which had schema"
 We can now examine the cooccurrences of attribute names in these schemata
" If we see a pair of attributes A and B that rarely occur together but always occur with the same other attribute names, this might mean that A and B are synonyms"
 We can further justify this hypothesis if we see that data elements have a significant overlap or are of the same data type
" Similarly, we can also offer a schema autoplete feature for database designers"
" For example, by analyzing such a large corpus of schemata, we can discover that schemata that have the attributes Make and Model also tend to have the attributes Year, Color, and Mileage"
" Providing such feedback to schemata creators can save them time but can also help them use more mon attribute names, thereby decreasing a possible Marchapril www"
orgintelligent Authorized licensed use limited to Univ of Calif Berkeley
" Downloaded on February , at from IEEE Xplore"
 source of heterogeneity in Webbased data
" Of course, we’ll find immense opportunities to create interesting data sets if we can automatically bine data from multiple tables in this collection"
 This is an area of active research
"Another opportunity is to bine data from multiple tables with data from other sources, such as unstructured Web pages or Web search queries"
" For example, Marius Paşca also considered the task of identifying attributes of classes"
" That is, his system first identifies classes such as “Company,” then finds examples such as “Adobe Systems,” “Macromedia,” “Apple Computer,” “Target,” and so on, and finally identifies class attributes such as “location,” “CEO,” “headquarters,” “stock price,” and “pany profile"
"” Michael Cafarella and his colleagues showed this can be gleaned from tables, but Paşca showed it can also be extracted from plain text on Web pages and from user queries in search logs"
" That is, from the user query “Apple Computer stock price” and from the other information we know about existing classes and attributes, we can confirm that “stock price” is an attribute of the “Company” class"
" Moreover, the technique works not just for a few dozen of the most popular classes but for thousands of classes and tens of thousands of attributes, including classes like “Aircraft Model,” which has attributes “weight,” “length,” “fuel consumption,” “interior photos,” “specifications,” and “seating arrangement"
"” Paşca shows that including query logs can lead to excellent performance, with percent precision over the top attributes per class"
" Choose a representation that can use unsupervised learning on unlabeled data, which is so much more plentiful than labeled data"
" Represent all the data with a nonparametric model rather than trying to summarize it with a parametric model, because with very large data sources, the data holds a lot of detail"
" For natural language applications, trust that human language has already evolved words for the important concepts"
" See how far you can go by tying together the words that are already there, rather than by inventing new concepts with clusters of words"
" Now go out and gather some data, and see what it can do"
"Choose a representation that can use unsupervised learning on unlabeled data, which is so much more plentiful than labeled data"
" Wigner, “The Unreasonable Effectiveness of Mathematics in the Natural Sciences,” Comm"
" Pure and Applied Mathematics, vol"
", A Comprehensive Grammar of the English Language, Longman, "
" Carroll, Computational Analysis of PresentDay American English, Brown Univ"
" Franz, Web T Gram Version , Linguistic Data Consortium, "
" Vasserman, “Translating Queries into Snippets for Improved Query Expansion,” Proc"
" Computational Linguistics Coling , Assoc"
" Computational Linguistics, , pp"
", “Learning to Create DataIntegrating Queries,” Proc"
" Very Large Databases VLDB , Very Large Database Endowment, , pp"
" Efros, “Scene Completion Using Millions of Photographs,” Comm"
" Taskar, Introduction to Statistical Relational Learning, MIT Press, "
", “MaxMargin Parsing,” Proc"
" Empirical Methods in Natural Language Processing EMNLP , Assoc"
" for Computational Linguistics, , pp"
" Weld, “Scaling Textual Inference to the Web,” Proc"
" Empirical Methods in Natural Language Processing EMNLP , Assoc"
" for Computational Linguistics, , pp"
" Lassila, “The Semantic Web,” Scientific Am"
", “Towards a Quantitative, PlatformIndependent Analysis of Knowledge Systems,” Proc"
" Principles of Knowledge Representation, AAAI Press, , pp"
" “Interview of Tom Gruber,” AIS SIGSEMIS Bull"
", “WebTables Exploring the Power of Tables on the Web,” Proc"
" Very Large Data Base Endowment VLDB , ACM Press, , pp"
" Paşca, “Organizing and Searching the World Wide Web of Facts"
" Step Two Harnessing the Wisdom of the Crowds,” Proc"
 th Int’l World Wide Web Conf
Alon Halevy is a research scientist at Google
 Contact him at halevygoogle
Peter Norvig is a research director at Google
 Contact him at pnorviggoogle
Fernando Pereira is a research director at Google
 Contact him at pereiragoogle
orgintelligent iEEE iNTElliGENT SYSTEMSView publication statsView publication statsAuthorized licensed use limited to Univ of Calif Berkeley
" Downloaded on February , at from IEEE Xplore"
" IEEE First International Conference on Data Science in Cyberspace IEEE First International Conference on Data Science in CyberspaceOPSDS a semantic data integration and service system based on domain ontology Liu Xin School of Computer and Communication Engineering University of Science and Technology Beijing Beijing, China liuxin_ustb"
" Hu Chungjin School of Computer and Communication Engineering University of Science and Technology Beijing Beijing, China hucj"
" Huang Jianyi School of Computer and Communication Engineering University of Science and Technology Beijing Beijing, China huangjianyi_ustb"
" Liu Feng School of Computer and Communication Engineering University of Science and Technology Beijing Beijing, China m"
" Abstract—For the distributed, heterogeneous, relational plex data sources of petroleum engineering, we present an oilproduction engineering semanticbased data integration system OPSDS"
 OPSDS establishes a semantic data integration and service system based on domain ontology on the premise of building a global semantic model and realizing the global semantic search
" The global semantic data model applied to various oil fields is set up by ontology extraction, ontology evolution, ontology bination and semantic constraints"
" The domainoriented data integration to provide the data access and is realized by ontology mapping, query shared service transformation, and data cleaning"
" Users and upper applications can have a direct access to underlying plex data sources in times of need through the global semantic data model, and the cleaned data can be returned in a unified format"
 OPSDS has been realized and got extensive use in many platforms of China National Petroleum CorporationCNPC
" It has been found that the method can not only provide the prehensive and realtime data support for oil and gas wells, but also improve the production and recovery efficiency with good application"
 Keywords petroleum information system data service technology of domain data semantic integration ontology petroleum engineering distributed data processingI
INTRODUCTION Oil is an important strategic resource of a country
 But oil and gas production of many oil fields around the world is trending downward and more fields are transiting into decline each year
 Reports indicate that a hybrid average production decline rate for oil fields worldwide is more or less
" Currently, various research methods to improve oil and gas production are booming"
" Production design, decision analysis, diagnosis and management of oil and gas wells are the keys to enhance productivity, reduce costs and increase profits"
" Optimal design of oil and gas wells involves large amounts of data, such as production data, well profiles, equipment data, geological structures, seismic data, reservoir data, etc"
"data with huge value have drawn attentions of academia, industry and government"
" For the purpose of improving production and saving energy, it is a highly advocated idea to dig out the value to utilize the data more efficiently"
 We focus on the features of oil data firstly
" Oil field is posed of a number of oilproduction plants, exploration institutes, geophysical research institutes and other units"
" Different units collect, collate, process, analysis and apply different kinds of data, and store corresponding data in their own databases which results in that different types of data are stored in different professional databases"
" Each database has its own specialized data organization and naming conventions, leading to system heterogeneity, syntax heterogeneity, structure heterogeneity and semantic heterogeneity"
 System Heterogeneity means operating environments and hardware platforms of data are various in different oil panies
 Syntax Heterogeneity indicates that oil panies take different storage methods for different types of data
" For example, some data are stored in relational databases, and some are stored in forms of text files"
 that different oil fields intends Structure Heterogeneity represent the same type of data with different data schemas
 A typical example is shown in Figure 
 Semantic Heterogeneity mainly refers to different words with the same meaning or the same words with different meanings
 Sucker rod data with structure heterogeneity
 Authorized licensed use limited to Hochschule Heilbronn
" Downloaded on June , at UTC from IEEE Xplore"
 Figure shows an instance of sucker rod structural data stored in relational database
" A well requires a set of sucker rod data, which contain rod level, length, diameter and other information, and different rod levels correspond to different rod lengths"
" For a multilevel sucker rod, D bines the threelevel rod length and saves as one field, D saves rod lengths into three rows according to different rod level, and D represents three fields in one row"
 that information And new features of data integration are concluded
" integration Bernstein and Haas say bines information from different sources into a unified format, and they specify the plicacy of integration after investigating the tools and technologies of data integration in the enterprise"
 They also indicate that every step of the integration process requires a good deal of manual intervention and more automation is surely possible
" Oil production engineering data are dynamic, updated in real time, and in critical instant need"
" Each oil field has not only production data every day, but also constantly updated basic data and regularly updated equipment data"
 So it is vital to ensure the realtime of data for upper applications
 Complex semantic associations
 It mainly refers to the plex associations between different data
" For example, we regard the well whose deviation angle is less than degrees as a vertical well, the well whose deviation angle is greater than degrees as a horizontal well, and the well whose deviation angle is between degrees and degrees as a inclined well"
 Focusing on the characteristics of the data leads us to discuss the challenge of traditional data management
" On one hand, data of oil fields are scatteredly stored, the logical organization lacks of ‘soul’, and the data schemas are various without naming rules and management methods"
 Thus it is urgent to establish a global semantic data model which is suitable for multiple oil fields to achieve the unification of data management platform
" On the other hand, data of oil panies are considerable autonomy, which increas the difficulty of data exchange and sharing"
 But data from different professional databases are increasingly need to work together to support upper applications of the domain
 So semantic data integration and building uniform interfaces directly accessing to the underlying data resources is of great significance
" In this paper, a petroleumengineering semanticbased data service OPSDS is presented to achieve a semantic data integration and service system based on domain ontology"
 The system provides a semantically richer global ontology and querybased access to the distributed and heterogeneous data
 OPSDS shields the plexity and sources of data to enable users and upper applications to take full advantage of data resources in a payasyougo approach everywhere
" Besides, a reasoning function is available for inferring the hidden information behind the semantic associations"
" RELATED WORKAs the plexity of data leads to a raising challenge for traditional data management, it is of utmost importance to generate a new way of data service"
 Data services provide access to data drawn from one or more underlying information sources
" Serviceenabling data stores, integrated data services and cloud data services inthe enterprise world are introduced in detail, but semantic relationships are not considered"
 propose a framework for scientific data services
 Data integration is a pervasive challenge faced in applications that need to query data residing at multiple autonomous and heterogeneous data sources 
" present a collaborative environment called distributed interoperable manufacturing platform, in which the STEPNC data model is built to promote data exchange among heterogeneous systems"
" propose a serviceoriented framework for integration of domainspecific data models in scientific workflows, which links the data sources and upper applications"
" However, the data model is built by domain experts, which is subjectivity and lacking in semantic relations between the data elements"
" Mapping of data with association relationships are constructed, but a certain inaccuracy exists"
 present three existing enterprise ontologies with different levels of expressivity
" Apparently, their work is not for specific domain, especially for the oil field"
 ARCHITECTURE OF OPSDS A
 OPSDS architecture OPSDS provides a rich semantic view of the underlying data and interfaces enabling users and upper applications to access data
 The architecture of OPSDS is shown in Figure 
" The bottom of the architecture is data sources storing in different databases, such as Oracle, SQL Server, etc"
 The middle layer is local ontologies extracting from the data sources below
" And then, the global ontology is formed as the local ontologies"
 Users and upper result of bining applications can access the data resources easily
" The only thing service consumers need to do is to send queries according to the global ontology, and then the desired data can be received"
 Architecture of OPSDS
 Authorized licensed use limited to Hochschule Heilbronn
" Downloaded on June , at UTC from IEEE Xplore"
 Construction of global ontology We adopt a hybrid strategy to construct the global ontology
" On the one hand, a topdown approach is used to filter the demand data"
" Entities, attributes and relationships"
 For Peer Review Only between entities can be got by classifying and organizing the data
" On the other hand, take a bottomupmethod to build local ontologies, which are results of extracting schemas of databases and items of synonym list"
" And then the global ontology is established according to ontology evolution, ontology mapping and imposed semantic constraints"
 Figure shows the construction process of global ontology
 The construction process of a global ontology
" Data of petroleum exploration and development vary in many aspects, such as exploration, production, geology, seismology, well logging, well drilling, etc, while data of petroleum engineering are just a part of them"
" Thus, firstly, we filter data to define the basic requirements, and classify, organize and aggregate the data to form entities, attributes and the relationships between entities labels"
 Then referring to the data dictionary
 identify Take production data entity and equipment data entity as examples
 The entity models are as follows
" Production data entity Production oil_output, gas_output, flow_pressure…… Equipment data entity Equipment pumping_unit, sucker_rod, defueling_pump……Since the majority of petroleum engineering data are stored in relational databases, we are here to study mapping fromRelational Database to OWL ontology"
" A relational database is posed of a set of relational schemas, including basic table structures and integrity constraints"
" An OWL ontology consists of classes, properties, individuals and axioms"
" As we aim at providing mappings between data models and ontologies, classes and properties are considered in this step"
" Because of individuals are widely exist in underlying database, individuals are not taken into consideration"
 Axioms are covered later in this paper
 The synonym list of petroleum engineering is built by domain experts and DBAs by reference to exploration and development handbooks of oil fields
 The synonymous items with different names and same meaning in the handbooks are gathered together in the synonym list to solve the phenomena of semantic heterogeneity
" Based on the schemas of tables in the specialized databases, we analyze characteristics of tables and constraints between tables, and then define an oil production engineering data source ontology OPDSOnto, which maps synonyms in the synonym list and schemas of tables to classes and properties in the ontology"
 The local ontology can be generated automatically through the program
 Getting innovations from Relational
"OWL, OWLRDBO and ProInnovator, we design OPDSOnto to describe tables, columns relations of tables and synonymy"
 Then extraction rules are defined as follows
" Convert tables and columns in databases to classes OPDSOnto Table or OPDSOnto Column owl Class, which express main concepts of the domain"
 Hierarchical relationships between tables and columns are presented by OPDSOnto hasParent and OPDSOnto hasChild owl ObjectProperty with owl inverseOf constructs
" OPDSOnto hasChild has a direction from domain Table to range Column, while OPDSOntohasParent has an opposite direction"
" Relationships between columns in one table are presented by OPDSOnto hasBrother, which is defined in owl ObjectProperty"
" If a column C in table A is the foreign key to table B, OPDSOnto hasChild represents the foreign key constraint, from domain column C to range Table B, while OPDSOntohasParent is the reverse semantic association"
" Datatype Properties of classes are defined, such as OPDSOnto isPK, OPDSOnto isFK, OPDSOnto isNullable, OPDSOnto dataType, to describe the primary key, the foreign key, nullable and data type of the individual"
" Extract the items which express the same meaning from the synonym list to convert into classes, and the relationships between classes are defined as OPDSOntohasSynonymy, which is built in owl ObjectProperty"
 Semantic Relationship is defined as shown below
 DEFINITIONSemantic Relationship
" ∀ x,x……xn,z, if xz, xz, …… , xnz, then xx"
"xn, where indicates the relation between two classes and identifies the semantic relationships between classes"
" According to the definition, the relationships that are hasParent, hasChild, hasBrother and hasSynonymy defined above among classes can be enriched"
 Tables from production database partial
 Authorized licensed use limited to Hochschule Heilbronn
" Downloaded on June , at UTC from IEEE Xplore"
 Figure shows schemas of four tables from production database
" Take Table well_info, Column well_name from Table well_info and Table output as examples to specify the process of ontology extraction, which is shown in Figure "
" information, that is, the two classes correspond to different attributes of one entity formed in the step of data filtering, the two classes evolve to a relation of hasBrother, parents of the two classes evolve to a relation of hasSynonymy"
 hasParenthasChildhasSynonymyhasBrotherhasBrotherhasBrotherhasParenthasChild 
 The steps of local ontology extraction
 The number in Figure corresponds to the rule number
 indicates that convert table names well_info and column name well_name into classes well_info
 means the relational schema of well_info and well_name is turned to a parentchild relationship in the local ontology
 is converting the two columns well_name and well_class from the same table into a hasBrother relation
 represents that the foreign key constraint of well_name
 Table well_info and table output is converted into a parentchild relationship
" Rule defines datatype properties of class well_name, while Rule extracts synonyms of well_name from synonym list and defines relationships between synonyms as hasSynonymy"
 Figure shows the local ontology after extracting schemas of production database and synonym list
 The arrows in Figure represent the relation of hasChild
 The classes with synonymy are circled in one node
 Local ontology of production database partial classes
" There are two steps from local ontologies to global local ontology and ontology, which are evolution of bination of to data storage local ontologies"
" Due characteristics of specialized databases in the petroleum engineering field, evolution of local ontology means if two classes with different parent nodes describe the same kind of When it es to bination of local ontologies, definite relationships must exist between the local ontologies"
 The global ontology can be built by mapping relevant local ontologies
" After analyzing schemas of different databases in the domain, the relationship between two local ontologies, which have the same class, can be established as a foreign key constraint"
 The parent node of the class with isPK property is mapped to the subclass of the class without isPK property
" For example, equip_info is a table of production database, and pump_parameters is a table of equipment database"
" The schemas of the two tables are as follows equip_info well_name, pumping_unit, pumping_rod…… pump_parameters pumping_unit, manufacturer, power……"
" Well_name is the primary key of table equip_info, and pumping_unit is the primary key of table pump_parameters"
" In our method, we regard pumping_unit as a foreign key of table equip_info linking to table pump_parameters"
" Thus, production ontology and equipment ontology are bined into one ontology"
 Figure shows the bined ontology
 Global ontology partial classes
" In Figure , the node output, production theevolutionary result of class output and class production"
" The relationship between output and production is evolved into hasSynonymy, and the relations of subclasses of output and production are hasBrother"
" is After local ontology evolution and local ontologies bination, local ontologies can be converted into a global ontology"
" We add some semantic constraints to strengthen the relations of terminology, and inference engine can reason and deduce the global ontology to reorganize the concepts using the constraints"
 Thus the implied semantic information can be got and valueadded services can be provided to users
 Semantic constraints are defined as follows
 Rule x OPDSOnto has Child y y OPDSOntohasSynonymy z x OPDSOnto has Child z Rule x OPDSOnto hasSynonymy y y OPDSOnto hasBrother z x OPDSOnto hasBrother z Authorized licensed use limited to Hochschule Heilbronn
" Downloaded on June , at UTC from IEEE Xplore"
 Rule x OPDSOnto hasSynonymy y y OPDSOnto hasParent z x OPDSOnto hasParent z C
" The process of semantic query Users and upper applications can submit query requests according to the global ontology, and OPSDS converts the requests to SPARQL statements to query the global ontology"
 Then the SPARQL statements can be rewrited as SQL statements to access underlying data sources
" Finally, the query results are returned in a uniform format after data cleaning and converting"
 Figure shows the process of semantic query
 Rewrite of SPARQL statements indicates that query requests based on the global ontology are converted into SQL statements to access underlying data sources
" Mapping from global ontology to data sources is divided into onetoone and onetomany, which include the following three types"
 Required data are from one table of a database
 Required data are from two or more tables of a database
 Query requests need to access more tables from multiple databases
 The process of semantic query implementation is as follows Step 
 Get the query request from service consumers to generate QueryGQG to query the global ontology
 QG is described by SPARQL
 Inference engine reasons names of classes and properties of QG to the names in the relevant ontologies
 Query resolver deposes QG into SubQueryLSQL to query each local ontology
" SQL {SQL , SQL , ……, SQLn , where n is the number of local ontologies"
 Query rewriter converts SQ L into SubQueryD SQDto query underlying database
" SQD {SQD , SQD , ……, SQDn , and SQD is described by SQL"
 Execute SQD and return SubResultD SRD 
" SRD {SRD , SRD , ……, SRDn "
" Result transformer cleans the query results SRDreferring to rules, and the normalized results can be got"
" Result biner bines the normalized results, and returns the final query result"
 QueryG SPARQL Result SubQueryL SPARQL SubQueryD SQL SubResultD SubResultD unified 
 shows the process of semantic query
 CONCLUSION AND FUTURE WORKintegration in specific area Semanticbased data is being a key research currently
" Build the global semantic data model for distributed, heterogeneous and plex semantic correlation data and provide prehensive and realtime data services using ontology technology are efficient and feasible"
" integration For data intensive industries, establishment of a global semantic data model based on domain ontology and realization to serve for upper of semanticbased data applications can get good results"
" Oil production engineering isa typical data intensive field, and OPSDS, which has realized data shared and reused, plays a key role in production practices"
" Through OPSDS, upper applications can directly access the underlying data resources and get the normalized data to beused for industrial production"
" The data of petroleum industrial applications show that our method can not only improve the production and recovery efficiency, but also save energy and reduce costs"
" Driven by application requirements, OPSDS connects production, study and research tightly, which is aneffective way to promote scientific and technological progress and prove that science and technology is the first productive force"
" In the future work, OPSDS will be used in more fields, and promoted to other oil areas like exploration and geology"
 Managing Scientific Data From Data Integration to Scientific WorkflowsJ
" GSA Today, Special Issue GSA Today, Special Issue"
" Carey M J, Onose N, Petropoulos M"
" Communications of the ACM, , "
" Dong B, Byna S, Wu K"
 SDS a framework for scientific data servicesCProceedings of the th Parallel Data Storage Workshop
" Halevy A, Rajaraman A, Ordille J"
 Data integration the teenage yearsCProceedings of the nd international conference on Very large data bases
" Smoot M E, Ono K, Ruscheinski J, et al"
 new features for data integration and network visualizationJ
" Bernstein P A, Haas L M"
 Information integration in the enterpriseJ
" Communications of the ACM, , "
 DIMP an interoperable solution for software integration and product data exchangeJ
" Enterprise Information Systems, , "
" Bender A, Poschlad A, Bozic S, et al"
 A serviceoriented framework for integration of domainspecific data models in scientific workflowsJ
" Procedia Computer Science, , "
" Das Sarma A, Fang L, Gupta N, et al"
 Finding related tablesCProceedings of the ACM SIGMOD International Conference on Management of Data
" Zdravković M, Panetto H, Trajanović M, et al"
 An approach for formalising the supply chain operationsJ
" Enterprise Information Systems, , "
 Authorized licensed use limited to Hochschule Heilbronn
" Downloaded on June , at UTC from IEEE Xplore"
 Information Sciences Contents lists available at SciVerse ScienceDirectInformation Sciencesj o u r n a l h o m e p a g e w w w 
" c o m l o c a t e i n sKernelMapping Remender system algorithmsMustansar Ali Ghazanfar a,, Adam PrügelBennett a, Sandor Szedmak ba School of Electronics and Computer Science, University of Southampton, Highﬁeld Campus, Southampton SO BJ, United Kingdomb Intelligent and Interactive Systems, University of Innsbruck, Innsbruck, Austriaa r t i c l ei n f oa b s t r a c tArticle historyReceived August Received in revised form March Accepted April Available online May KeywordsRemender systemsStructure learningLinear operationMaximum marginKernel"
 IntroductionRemender systems apply machine learning techniques for ﬁltering unseen informationand can predict whether a user would like a given item
" In this paper, we propose a newalgorithm that we call the KernelMapping Remender KMR, which uses a novelstructure learning technique"
" This paper makes the following contributions we showhow userbased and itembased versions of the KMR algorithm can be built userbased and itembased versions can be bined more information—features,genre, etc"
"—can be employed using kernels and how this affects the ﬁnal results and to make reliable remendations under sparse, coldstart, and long tail scenarios"
" Byextensive experimental results on ﬁve different datasets, we show that the proposedalgorithms outperform or give parable results to other stateoftheart algorithms"
"In this paper, we proposed a new class of kernelbased methods for solving the remendation problem that gives stateoftheart performance"
 The main idea is to ﬁnd a multilinear mapping between two vector spaces
" The ﬁrst vector spacemight, for example, have vectors encoding information about the items that we wish to rate, while the second vector spacemay contain a probability density function describing how a particular user will rate an item"
 Learning an appropriate mapping can be expressed as a quadratic optimisation problem
" As the problem involves a linear mapping, the solution to theoptimisation problem involves inner products in the two vector spaces"
 This allows us to use the kernel trick
 Directly solvingthe optimisation problem using quadratic programming would be too slow for most remendation datasets
" Instead, weﬁnd an approximate solution iteratively, following an idea ﬁrst developed by "
 This allows us to train the remenderin linear time
 The method described here is a specialisation of a general structure learning framework developed bySzedmak and used in for handling inplete data sources
The approach we have adopted is easily adapted to different sources of information
" We can, for example, use either rating information from other users or textual information about the items"
" Similarly, we are able to build either an itembasedor a userbased version of the algorithm"
" Because we have chosen to build a mapping to a space of functions approximatingthe probability density of the ratings, we have an intuitive interpretation of the remendations produced by the algorithm"
" This gives us ﬂexibility in how we make our ﬁnal remendation, which we can exploit to improve the ﬁnal prediction for different datasets"
"A main requirement of remender systems is to provide high quality predictions of the rating that a user would giveto an item, based on their previous rating history"
" Thus in testing remender systems, a dataset is used where somesets of ratings are treated as unseen while the other ratings are used for learning"
" To obtain accurate results, datasets Corresponding author"
 see front matter Elsevier Inc
 Information Sciences are usually selected with users that have made a relatively high number of ratings
" In real applications, however, the datasets are often highly skewed for example, a large number of users may have made only a small number of ratings, and alarge number of items may have received very few ratings"
 These are important scenarios in practical systems as givingreasonable remendations to new users can be crucial in attracting more users
" Similarly, giving a sensible rating to anew item may be necessary for those items to be taken up by the munity sufﬁciently to collect more ratings"
" Often,remender system algorithms that have been optimised to give good remendations on dense datasets performpoorly on these skewed datasets"
 We have generated highly skewed datasets to test our algorithm under these scenarios
"In particular we consider the newitem coldstart problem, the new user coldstart problem , and the long tail problem"
" We ﬁnd that the standard algorithm we developed performs poorly for these skewed datasets however, we showthat by using the ﬂexibility of our approach we can easily modify the algorithm so that it performs well under thesescenarios"
Remender systems have been a very active topic of research for around years
" This, in part, has been spurred onby the Netﬂix petition to improve the performance in terms of the root mean square error of a baseline algorithm by"
 One lesson to emerge from this was that a highly effective way to achieve a very high remendation performanceon a static dataset is to bine a large number of different algorithms
" Although such systems are interesting, they are notvery ﬂexible and may not be ideal algorithms for most real applications with rapidly changing users and items"
 Our algorithm relies on a single coherent method albeit with several variants that has not been designed for a speciﬁc dataset
 Wehave thus pared our approach with other general purpose remenders
" The best general purpose collaborative ﬁltering algorithms that we are aware of are by ,"
 These achieve a considerable gap in performance advantage over otheralgorithms
" The proposed algorithm achieves similar performance in terms of mean absolute error to these approaches,although it is outperformed by on a dataset with ,, ratings and by on a dataset of ,, ratings"
The proposed approach is however very different
" The other two approaches are based on matrix factorisation, although also uses kernel functions"
 There has been considerable work on developing matrix factorisation techniques whichare at the heart of many of the most petitive algorithms for this problem
" Part of the interest of the proposed algorithmis that it takes a very different viewpoint from the matrix factorisation approaches, yet still has very petitiveperformance"
The rest of the paper has been organised as follows In the next section we brieﬂy outline related work
 Section outlinesthe proposed algorithm using an itembased approach
 In Section we describe extensions to the basic algorithm
" Section presents details of the datasets we use for evaluation, the metrics we use and the procedure for tuning parameters of thealgorithm"
 This is followed in Section by a presentation of results from our experimental evaluation
 We conclude in Section 
 Some of the details and more extensive results are given in appendices
 Related workThere are two main types of remender systems collaborative ﬁltering and contentbased ﬁltering remender systems
" Collaborative ﬁltering CF remender systems ,,,,,, remend items by taking into account thetaste in terms of preferences of items of users, under the assumption that users will be interested in items that users similarto them have rated highly"
" Examples of these systems include GroupLens system , Ringo www"
 Collaborative ﬁltering systems are classiﬁed into two subcategories memorybased CF and modelbased CF
" Memorybased approaches make a prediction by taking into account the entire collection of previous items rated by a user, for example,the GroupLens remender systems "
" Modelbased approaches use rating patterns of users in the training set, groupusers into different classes, and use ratings of predeﬁned classes to generate remendations for an active user That is, the userfor whom the remendations are puted on a target item That is, the item a system wants to remend"
" Examples include itembased CF , Singular Value Deposition SVD based models ,,, matrix factorisation,,,,,,,,, nuclear norm regularisation , Bayesian networks , and clustering methods ,,"
"Contentbased ﬁltering remender systems ,,, remend items based on the content information of an item,under the assumption that users will like similar items to the ones they liked before"
" In these systems, an item of interest isdeﬁned by its associated features for instance, NewsWeeder , a newsgroup ﬁltering system uses the words of text asfeatures"
" Other wellknown types of remender systems include knowledgebased systems ,, Ontologybased systems, and hybrid systems ,"
"Hybrid remender systems have been proposed elsewhere ,,,,,,,,, which bine individual recommender systems to avoid certain limitations of individual remender systems"
" In the proposed approach we can addmore information about items in the forms of additional kernels, which can be thought of as bining collaborative ﬁltering with contentbased ﬁltering"
" A related approach has been proposed in , where the authors employed a uniﬁed approach for integrating the useritem ratings information with useritem attributes using kernels"
 They learned a predictionfunction using an online perceptron learning algorithm
" They claimed that adding more kernels increases the performance,which is in contrast with our ﬁndings"
" It might be due to the reasons that they used very simple kernels, such as correlation,and identity however, we used polynomial kernels, which are in turnare addition of correlation, identity, etc"
" Information Sciences In , the authors proposed a structured learning algorithm for learning from inplete dataset"
" The idea of the structure learning has been used in , where the authors employed it for enzyme prediction"
 We show how the structure learning approach can also be used to solve the remender system problem effectively
Remendations can be presented to an active user in the following two different ways by predicting ratings ofitems a user has not seen before and by constructing a list of items ordered by their preferences
" In this paper, we focuson both of them"
" KernelMapping RemenderA remender system consists of two basic entities users and items, where users provide their opinions ratings aboutitems"
 We denote these users by U fu u 
" uMg, where the number of people using the system is jUj M, and denotethe set of items being remended by I fi i "
" The users will have rated some, but not all, of theitems"
" We denote these ratings by riuji u D, where D I U is the set of useritem pairs that have been rated"
 We denote the total number of ratings made by jDj T
" Typically each user rates only a small number of the possible items, so thatjDj T jI Uj N M"
 It is not unusual in practical systems to have TN M u 
 The set of possible ratings madeby the users can be thought of as elements of an M N rating matrix R
" We denote the items for which there are ratings byuser u as Du, and the users who have rated an item i by Di"
" The task is to create a remendation algorithm that predicts anunseen rating riu, That is, for i u R D"
In this section we describe an itembased remender
 In the next section we show how we can adapt the approach to auserbased remender
 To perform the remendation task we consider building the additive and multiplicative modelsfor the residual ratings
" The residue in the additive model is given byriu riu ri ru rwhere ri, ru and r are respectively the mean rating for the item, of the user, and the overall meanri jDijXuDiriuru riur jDujXiDujDjXuDriuThe multiplicative model can be expressed as followsriu riurriru"
" Itembased KMRwhere r, ri and ru are the geometric means for all the ratings, the ratings for item i, and the rating of user u, respectively"
" Wefound the additive model to be marginally better than the multiplicative one, and hence this work is based on the additivemodel"
We use a technique developed by Szedmak and coworkers for learning structured data 
 In the following we outlinehow this approach is adapted for solving the collaborative ﬁltering problem
 We assume that we have some informationabout the items which we denote by qi
" This may, for example, be the set of ratings riu for u Di, or it could be text describing the item i"
 We map the information to some vector qi in some extended feature Hilbert space
" Similarly, we mapthe rating residues, riu, to ‘vectors’ in some other Hilbert space"
" In this paper, we consider these objects to lie in the function space LR"
" In particular we represent each residual riu, by a normal distribution with mean riu and variance r"
" Thatis,wriu N xjriu rThe motivation of this choice is to model possible errors in the rating either due to the discretisation of the rating scale or thevariability in assigning a rating e"
 due to the mood of the user on the day they made the rating
The method developed by Szedmak is to seek a linear mapping between these two spaces which can be used for makingpredictions
" More speciﬁcally, in our application, we look for a linear mapping Wu from the space of vectors to the space ofw vectors refer to "
 We will use the mapping Wuqj to make a prediction for the rating of a new item j by the user u
To learn the mappings Wu we will minimise the Frobenius norm of Wu subject to the constraintshwriu Wuqii P fiwhere fi P is a slack variable and where we have a constraint for each pair i u D
 This ensures that Wuqj is alignedwith wriu for the ratings in the training set
 We can write the training problem for the mappings Wu as a quadratic programming problemM
 Information Sciences minimiseXuUkWuk CXfiiIwith respect to Wu u U fi i Isubject tohwriu Wuqii P fifi P i I u DiNote that minimisation will be achieved when the vectors Wuqi are as uniformly aligned as possible with the vector wriu
Having learned the mappings Wu we can then make predictions for a new item j using Wuqj
 This outputs a functionwhich informally we can think of as an estimate for the probability density of the residue rju
" However, Wuqj does notneed to be, and typically is not, positive everywhere or normalised"
" Thus, it is not itself a probability density"
 We discuss laterdifferent methods for interpreting Wuqj
"To solve this constrained optimisation problem, we deﬁne the LagrangianL XuUkWuk CXfi Xaiuhwriu Wuqii fi iIiuDXiIkifiwhere aiu P are Lagrange multipliers introduced to ensure that hwriu Wuqii P fi and ki P are Lagrange multipliers introduced to ensure that fi P "
 The optimum mapping is found by solvingsubject to the constraints that aiu P for all i u D and ki P for all i I 
" For a general linear mapping, Wu, we have thatwhere is the tensorproduct of the two vectors"
" This is clearly the case when the Hilbert spaces are ﬁnite dimensions sothat the mapping Wu can be represented by a matrix, but this can be extended for linear mappings between more generalHilbert spaces"
" Using this result we ﬁndThat is, the Lagrangian is minimised with respect to Wu when Wu Pfi we ﬁndiDiaiuwriu qi"
 Taking derivatives with respect tominfWugffigmaxfaiugfkigLhwriu Wuqii wriu qiWuLWuLfiXuDi Wu aiuwriu qiXiDu C aiu kiXuDiaiu C ki CSetting these derivatives to we ﬁnd that the Lagrangian is maximised with respect to fi whenwhere the inequality arises because ki P 
After substituting back the expressions containing only the Lagrange multipliers into the Lagrangian we obtain the dualproblem of which is a maximisation problem with respect to the variables aiuf a aiuaiuhwriu wriuihqi qi i XXuUiiDuXaiuiuDsubject to the constraint that a Za whereZa aji IXuDiaiu C i u D aiu P Krriu riu hwriu wriuiK qqi qi hqi qi ithen we can write fa asf a aiu aiu Krriu riu K qqi qi XXuUiiDuXaiuiuDWe are now in the position where we can apply the usual kernel trick
 Deﬁning the kernel functionswhere we are free to choose any pair of positive deﬁnite kernel functions
" With our choice of mapping the rating residual, r,to wr N xjr r, we note thatM"
 Information Sciences Krr r hwr wri N r rjprwhich is inexpensive to pute
" We could build more plex kernels for Krr r, by mapping wr into another extendedfeature space, although we would then lose the interpretation of Wuqi as an approximation to the density function for riu"
" Learning the Lagrange multipliersFor largescale remender systems, solving this quadratic programming problem using a general quadratic programming solver would be impractical due to the large number of data points"
" However, we can ﬁnd an approximate solutioniteratively using the conditional gradient method"
 To understand this method it is helpful to write fa in matrix formf a aTMa bTawith a Za
 We obtain a series of approximations at for the optimal parameters starting from some initial guessa Za
 At each step we use a linear approximation for fa about the current position atf a f at a f at a atf atWe pute the next approximation using two stages
" We ﬁrst solve the linear programming problema argmaxaZaf at a argmaxaZaaTMat b constWe then ﬁnd the new approximation at to beat at sa atwhere we choose s to bes argmaxf at sa ats½smax b MatTa ata atTMa atNote that we can pute the unconstrained maximum for sf a XXaiugiu constuDiiIf ataiu XiDugiu atiu Krriu riu K qqi qi maximiseaiugiuXuDiXuDisubject toaiu C and u Di aiu P By truncating smax if necessary to ensure that it lies in the interval , we can ensure that at is the maximum value of aalong the line segment from at to a"
" Since this segment includes the current point, at, we are guaranteed that no stepdecreases the objective function"
We note that in the linear programming problem we have an objective function of the formwhich decouples for every set of Lagrange multipliers Ai fauiju Dig
 The linear constraints Za also decouple into a set ofconstraints for each set of Lagrange multipliers Ai
" Thus, the linear programming problem bees a series of linear programming problems for each i IThis linear programming problem is trivial to solve see "
 If giu has positive ponents then a maximum will occurwhen we set aiu C where giu P giu for all u Di and aiu otherwise
Since ﬁnding the largest ponent of giu can be puted in linear time we are able to perform one step of the optimisation procedure in HjDj time
 Note that at each step we have to pute a vector matrix products involving the matrix M
These products involve the sum over all u U and the sum over all i Du
" However jDuj does not grow with the number ofusers, thus these products can also be puted efﬁciently"
" Note, that solving the optimisation problem this way makes itfeasible to obtain remendation for databases with up to million ratings"
" Predicting unseen ratingsTo make a prediction for the rating riu where i u R D, we estimate the residue riu riu ri ru r using the functionM"
 Information Sciences 
 Schematic showing the linear programming problem
 The feasible region is shown as a shaded triangle
 The vector gi shows the direction of theobjective function
 The maximum occurs at the vertex corresponding to the largest ponent of gi
 Schematic showing the aim of the algorithm
" Information, qi in this case a rating vector about an item i, is ﬁrst mapped to a vector in an extendedfeature space qi"
" We then try to ﬁnd the best linear mapping, W, for user u, to the ‘vector’, wriu , describing the residual"
piur hwr Wuqii aiuKrr riuK qqi qi XiDuriu argmaxpiurrwhere wr N r r
 We have a choice in how to obtain a single prediction from this function
 Our standard predictor will beto ﬁnd the maximum argument of piurThis works well when we have a sufﬁcient number of ratings for the user and the item
 However as we will see it givespoor predictions in scenarios where we have a small amount of training data
 Recall that we argued earlier Wuqi can beregarded as an approximation for the probability density of riu
" It will not generally be positive everywhere, but by removingthe negative part of the function we can treat the remaining function as a probability density"
" In this case, we can considerthe mean, mode, or median as approximations for the most likely value of riu"
" Under conditions where we lack sufﬁcient datawe ﬁnd that using a bination of the mean, mode and median together with the standard max prediction gives a considerable improvement in accuracy"
" In particular, we consider a predictorrM wmaxrmax wmeanrmean wmodermode wmedianrmedianwhere rm with m {max, mean, mode, median are the standard predictors and the predictors using the mean, mode andmedian, while wm are a set of weights that are learnt from a validation set"
 We consider the weights to be constrained sothat wm P and they sum to 
 In the results shown later we denote those results obtained using this predictor by the superscript M
" A small scale exampleSuppose a remender system has four users That is, U fu, u, u, u and three items That is, I fi, i, i"
" The information about each item is a column vector of the useritem rating matrix, shown in Table "
" The users’, items’, and overall averages areru ri ru ri ru ri ru r M"
 Information Sciences Table Example a subset of the useritem rating matrix in a movie remender system
 We have four users rows and three moviescolumns
" The case, where a user has not rated a particular movie is shown by symbol"
" The rating scale, consisting of integervalues between and , captures the extreme like and extreme dislike behaviour of a user"
 The rating we want to predict isshown by ‘‘’’ symbol
iiiAfter applying the additive model Eq
" , the useritem rating matrix can be represented in the residual form as shown inTable "
" The input feature kernel, Kq, using the polyGaussian kernel refer to Section "
"We can pute the residual kernel, Kr, based on the inner products between Gaussian densities functions with expectedvalues r and r, and sharing the mon standard deviation r"
K q Krr r hwr wri p errrr pAssume that r 
" then we haveKruKruK residual KruKruwhereKru Kru Kru Kru The optimal values for the design variables, a, are learnt using the conditional gradient method, and are shown in Table "
" To make aAfter learning the a parameters, the mapping Wu, can be deﬁned for each user recall Wu Pprediction for the rating riu, where i u R DiDiWuqi aiuwriu qi qi aiuwriuK qqi qiXiDuXiDuIn this case, we have u u and i i, soWu qi aiu wriu K qqi qi aiu wriu K qqi qi wriu wriu wriu wriu N riu r N riu rwhich is an unnormalised probability density function of mixture of two Gaussians"
 The optimal rating then can be derivedbyTable Example the matrices of rating residues riu
" Information Sciences Table The optimal values of design variable, a, for each user and item"
"piu r hwr Wu qi i arg maxhwr wriu wriu i arg maxhKrr riu Krr riu i arg maxhN rjriu pr N rjriu prrirrTaking the optimum solution refer to , riu , the prediction for the residual is "
" Hence, user u would rate item i withrating of riu ri ru r "
 Extensions to the basic algorithm
 Userbased KMRIn this section we describe extensions to the basic algorithm which are relevant to practical remender systems
Depending on the dataset characteristics e
" number of items rated by the active user, number of users which have ratedthe target item, etc"
 different models can be trained along the rows or columns of the data matrix
" A related algorithm isproposed, which solves the problem from the user point of view, hence it is named as the userbased KMR KMRub"
" To perform a userbased remendation, we use information qu about users u and try to ﬁnd a linear mapping Wi to align someextended feature vectors qu to the residue vector wriu"
 The derivation is identical to that for the itembased remender when we interchange the subscripts i and u
 Combining user and itembased KMRUser and itembased versions provide plementary roles in generating predictions as they focus on different types ofiur be the predictions made by the user and itembased versions respectively
 Werelationships in a dataset
 Let pubhave considered three different ways of bining user and itembased predictions
"iu r and pib Using the simple linear bination In this approach, the user and itembased versions are linearly bined, where theparameter q is learned from a validation set"
piur qpubiu r qpibiurWe denote the resulting hybrid remender system by KMRLinearHybrid
" Switching on number of ratings Here, we take into account the information about user and item proﬁles"
" The rationalebehind this approach is the intuition that if we have a large number of ratings for an item pared to the number ofratings made by the active user, then the userbased version is likely to give better results than the itembased versionand vice versa"
" Rather than using the raw number of ratings, we normalise by the number of ratings given by the poweruser, up That is, the user that has rated the most number of items and by the power item iP That is, the item with the most number of ratings"
 Plotting the probability density function of mixture of two Gaussians with r f g
 The optimal solution is found to be 
 Information Sciences Table Characteristics of the datasets used in this work
" FT, SML, ML, ML, and NF represent the FilmTrust, MovieLens k, MovieLens M, MovieLens M, andNetﬂix dataset respectively"
 Average rating represents the average rating given by all users in the dataset
CharacteristicsNumber of usersNumber of moviesNumber of ratingsRating scaleSparsityMax number of ratings from a userMax number of ratings for a movieAverage ratingDatasetFT 
piur iu r pubiur pibjIup j hCntifjUijjUip j jIujotherwiseSML 
We denote the resulting hybrid remender system by KMRCntHybrid
 Switching on uncertainty in prediction Here we use a different strategy for switching between the user and itembasedpredictors
 We try to estimate the uncertainty in the prediction by examining the ‘‘variance’’ in Wuqi and Wiqu
"Since they are not real probability distributions, we must ﬁrst exclude the regions where the functions go negativeand normalise the output so that we can treat them as densities and pute their variance"
" We denote the varianceby Varub and Varib for the user and itembased versions, respectively"
 We then switch the remendation according topiur iu r pubiur pibif Varub Varib hVarotherwiseWe denote the resulting hybrid remender system by KMRVarHybrid
 Combining kernelsIn many applications there are multiple sources of information that can be used to make a remendation
 We can easily acmodate different sources of information by bining kernels
 To illustrate this we will test our algorithm on datasets consisting of ﬁlm ratings where we have three types of information available refer to Section 
 The ratings of other users from which we can construct a kernel Krat ‘‘Demographic’’ information obtained from genre about the ﬁlms from which we can construct a kernel Kdemo ‘‘Feature’’ information obtained from a textual description of the ﬁlms from which we construct a kernel Kfeat
"These kernels can be bined linearlyK bratK rat bdemoK demo bfeatK featwhere the parameters brat, bdemo and bfeat brat bdemo can be tuned by measuring the generalisation performance on avalidation set"
 This way of bining kernels can be viewed as a concatenation of the feature vectors p ratbratpbdemodemoqbfeatfeatp rat bratpbdemodemo qbfeatfeatwhere represents the direct sum
 Alternatively we can bine the kernels nonlinearlywhere the denotes the pointwise product of the kernel matrices
 This corresponds to taking a tensor product of the featurevectorsK K rat K demo K feat rat demo feat
 Experimental setupalgorithms
In this section we describe the datasets we used and the setup of the experiments for benchmarking the proposed
 Information Sciences As is mon in the ﬁeld of remender systems we used data from ﬁlm remendation sites to test the proposedalgorithm
 These provide some of the largest available datasets allowing us to test the scaling performance of the algorithm
"In addition, as these datasets are very monly used in the literature, it allows us to benchmark our algorithm against petitor algorithms"
 We used the following datasets FilmTrust denoted by FT obtained by crawling on th March the FilmTrust website httptrust
 Only users and movies having more than ﬁve ratings were used
" This has been used before in ,"
" MovieLens which we split into three groups Small MovieLens denoted by SML with million rating dataset denoted by ML million rating dataset denoted by MLThis has been widely used ,,,,"
" Random subsample of , users from the Netﬂix dataset denoted by NF"
 This dataset has been very widely used e
"see ,,, in part because of the prize offered for achieving a level of improvement over a benchmark"
 We have notattempted to pare our algorithm against the stateoftheart Netﬂix algorithms for two reasons
" Firstly they havebeen highly tuned to that particular dataset, while we have concentrated on developing a general purpose remendation algorithm"
" Secondly, the full Netﬂix dataset is so large that it is difﬁcult to process on a normal desktop machinewithout spending signiﬁcant time on optimising memory management"
The characteristics of these datasets are given in Table 
 Feature extraction and selectionTo test the remendation algorithm using textual information we also obtained information about each movie
 Thiswas used to construct two additional information vectors a ‘‘feature’’ vector and a ‘‘demographic’’ vector
 We downloadedinformation about each movie in the MovieLens SML dataset and FilmTrust dataset from IMDB
" For the ML dataset, weused the tags and genre information that is provided with this dataset"
" After stop word removal and stemming, we constructed a vector of keywords, tags, directors, actorsactresses, producers, writers, and user reviews given to a movie in IMDB"
"We used TFIDF Term FrequencyInverse Document Frequency approach for determining the weights of words in a documentThat is, movie"
" The document frequency DF thresholding feature selection technique was used to reduce the feature space byeliminating useless noise words having little or no discriminating power in a classiﬁer, or having low signaltonoise ratio"
"To construct the demographic vector, we take the genre information about movies as employed in , with the exception that we used the hierarchy of genre as shown in "
" To determine the weight of a genre in the genre vector, we used asimple weighting scheme as employed in QuickStep, an Ontologybased remender system "
" To pute an innerproduct between demographic vectors the immediate super class is assigned of a subject’s value, the next super classis assigned , and so on until the most general subject in the Ontology is reached"
" By making a hierarchy of the genreand assigning different weights to sub and superclasses, we hope to enrich an item’s proﬁle"
" MetricsIn the majority of the paper, we have used the Mean Absolute Error MAE as our measure of performance as this is themost monly used measure and de facto standard for benchmarking remender systems"
" In practice, however, remender systems are monly used for helping users in selecting high quality items"
" Thus, arguably, a more appropriatemeasure of accuracy is to study an algorithm’s ability to predict highly rated items"
 There are a number of metrics thatare more speciﬁcally designed to measure how well a remender classiﬁes good quality relevant items
 These includethe ROCsensitivity and F measure
 The details of all these metrics are given in Appendix B
" Furthermore, we also give tablesof results for these last two measures in that Appendix"
 Evaluation methodologyWe performed fold cross validation by randomly dividing the dataset into a test and training set and reported the average results
 We further subdivided the training set into a test and training set for measuring the sensitivity of the parameters
"For learning the parameters, we conducted fold cross validation on the training set"
" We matched the movie titles, provided by the SML and FT dataset, against the titles in the IMDB www"
 We used Google’s stop word list www
 We used Porter Stemming algorithm for stemming
 Information Sciences 
 Hierarchy of genres based on 
 All the super classes of a genre get a share when a genre receives some interest
" For instance if a rated movie fallsinto ‘‘crime’’ genre, then the ‘‘crime’’ subject will get a weight of q, the immediate super class, ‘‘Thriller’’ will get a weight of q and the next super class‘‘Unknown’’ will get a weight of q"
 Learning system parametersThere are a number of parameters that need to be learned
" Below, we discuss the training of these parameters"
" Number of iterationsThe algorithm we develop uses an iterative technique to learn the Lagrange multipliers, a"
 As we increase the number ofiterations the mean absolute error improves
 The speed of convergence will depend on the dataset and the type of information we are using e
 userbased or itembased
" and , show the mean absolute error and the time taken to learn theLagrange multipliers versus the number of iterations for the FT and SML datasets, respectively"
"We note that for the FT dataset, the performance of the itembased version suffers badly when the number of iterationsare very small"
" However, the performance of the userbased version is quite good even after a few iterations"
" Hence, if onehas a constraint on the time required to build the model, then it is better to switch to the userbased version rather than theitembased version for the dataset"
" In contrast, in the SML dataset, the convergence of all the methods was relatively quick"
The convergence clearly depends on the number of usersitems and the useritem proﬁle length e
" rating proﬁle, featureproﬁle length, etc"
 It is not obvious a priori how many iterations are needed to get good rating predictions
" Based on ourinitial experiments, we chose the number of iterations to be for the SML dataset, for FT, for ML, and forML and NF"
"We trained linear, polynomial, and polyGaussian kernels and chose the one giving the most accurate results"
" The optimal kernel parametersnomial kernel is of the formKx y hx yi RdFor the ratingbased version, the best polynomial kernel parameters d, R are found to be, for userbased and itembasedversions respectively , "
" for the SML dataset , "
" for the FT dataset and , "
 Information Sciences Number of Iterations FT DataSetNumber of Iterations FT DataSet 
 The number of iterations and time required to converge the proposed algorithms for the FT dataset
Number of Iterations SML DataSetUBIBUBIBUBIBFeatureDemoUBIBFeatureDemo rorrlE etuosbA naeMEAM sm emTirorrlE etuosbA naeMEAM sm emTi
Number of Iterations SML DataSet 
 The number of iterations and time required to converge the proposed algorithms for the SML dataset
" For the featurebased version, the best polynomial kernel parameters were found to be , "
" for theSML dataset and , "
"We did not tune the parameters for the ML and NF datasets, as it was putationally expensive"
" for user and itembased versions for both datasets and , "
 for the featurebased version for the ML dataset
"For the demographicbased version, we found the best kernel was the polyGaussian kernel which is a simple extensionof the Gaussian one given byM"
" Information Sciences Kx y exp kx ykqswhere the best parameters q, s were found to be "
 for the SML dataset and 
 Again we didnot tune parameters for ML dataset and they were ﬁxed to 
"The other parameter in setting up the kernel was the standard deviation, r, used in mapping wr N xjr r"
" We experimented with learning this parameter for each user, but found this putationally very expensive"
 We then tried groupingthe users according to the variance in their ratings into groups and tuned r for each group
" Although this gave improvedperformance, it was not found to be statistically signiﬁcant"
 We therefore just used a single parameter r which we tunedusing a validation set
"The parameter C that punishes the slack variables in the Lagrange formulation was ﬁxed to , after initial experimentation"
" In the extension of the basic remender there are other parameters, such as the weights for bining kernels andvarious thresholds for switching between remenders"
 The tuning of these parameters are described in Appendix A
" ResultsIn this section, we describe the results obtained from our experiments"
" In the tables we have denoted the proposed algorithm by KMRsupsub, where the subscript denotes the variant of the algorithm and the occasional superscript describes the variantin more detail where necessary"
" The main variants are itembased ib, userbased ub, featurebased F that use feature vectors rather than rating vectors, demographic D that use demographic vectors rather than rating vectors, and hybrid Hybridthat uses a mixture of userbased and itembased predictions"
 For the hybrid algorithm we use the superscript to denote thedifferent mechanisms for bining userbased and itembased predictions
" When we use binations of information, e"
"itembased ratings and features, we use KMRibF to denote the case when we add the kernels and KMRibF when we multiplythe kernels"
" Finally, for the datasets with a limited amount of ratings, instead of using the standard approach to predicting anew rating, we bined the standard approach value of r that maximises the predictor pr with the mean, mode and median of Wuqi for the itembased approach"
 We denote this version of the algorithm with a superscript M
We pare the proposed algorithms with other algorithms described in the literature
" We chose several other algorithms based on the number of citations given in the literature the algorithm classiﬁcation space That is, memorybased ormodelbased approaches and whether the algorithm claims to give stateoftheart results"
" Direct parisonWe pared the proposed algorithms with three different algorithms userbased collaborative Filtering CF with Default Voting DV proposed in which provides a useful baseline for paring algorithms, itembased CF proposed in shown by ItemBased CF, and a SVD based approach proposed in shown by SVD"
" To provide as fair a parisonas possible, we tuned all parameters of the algorithms"
Table shows that the KMRbased algorithms outperform all the aforementioned algorithms
" The percentage decrease inerror of KMRib, KMRub, and KMRVarHybrid over the baseline approach is found to be "
 for the SML dataset 
 for the MLdataset and 
 The ROCsensitivity and F measure on the same dataset are shownin Tables A
" Indirect parisonIn this section, we pare our results with other algorithms indirectly, That is, we take the result from the respective paperswithout reimplementing them, which might make the parison less than ideal"
 We conducted the weak generalisationtest procedures of using the AllButOne protocol—for each user in the training set a single rating is withheld for the testset
" We averaged the results over the three random traintest splits as used in ,,"
Table A parison of the proposed algorithm with others in terms of MAE
 The average with the respective standard deviation of results over fold is shown
 Thebest results are shown in bold font
AlgorithmUserbased CFItembased CFHybrid CFSVDKMRibKMRubKMRVarHybridBest MAESML
 Information Sciences Table A parison of different algorithms in terms of NMAE Normalised MAE for theML dataset
" The proposed algorithms outperform URP , Attitude ,MatchBox , MMMF , ImputedSVD , and Item "
 They give theparable results to EMMF and NLMF 
 Our results and the best resultsare shown in bold font
AlgorithmURPAttitudeMatchBoxImputedSVDMMMFItemEMMFNLMF LinearNLMF RBFKMRibKMRubKMRVarHybridAlgorithmNLMFMFTIBKMRibKMRubKMRVarHybridNMAE
Table A parison of different algorithms in terms of RMSE for the ML dataset
NLMF represents the nonlinear matrix factorisation technique as proposed in and M FTIB represents the Mixed Membership Matrix factorisation modelas proposed in 
 Our results and the best results are shown in bold font
A parison in terms of Normalised MAE NMAE—see Appendix B—of the algorithms is given in Table 
" In Table , URPrepresents the algorithm proposed in , Attitude represents the algorithm proposed in , MatchBox is proposed in ,MMMF represents the maximum margin matrix factorisation algorithm proposed in , ImputedSVD is proposed in , Itemis proposed in , EMMF represents the ensemble maximum margin matrix factorisation technique proposed in , andNLMF represents the nonlinear matrix factorisation technique with linear and RBF versions as proposed in "
Table shows that the NLMF and EMMF perform better than the rest
 The proposed hybrid algorithm gives slightlypoorer results to them with NMAE 
" It is worth mentioning that the EMMF is an ensemble of about predictors,which makes this algorithm unattractive"
" From this table, we may conclude that the proposed algorithm is parable to thestateoftheart algorithm for the MovieLens M dataset"
"To the best of our knowledge, the best results for the MovieLens M dataset that have been reported in the literature arethose proposed in ,"
 They claimed their proposed algorithm gives RMSE accuracy of 
 We followed their experimental setup and the results have been shown in Table 
 Table shows that the proposed algorithms outperform ’s results
 The percentage improvement is found to be 
 in the case of KMRVarHybrib
 TheMFTIB algorithm gave the best results outperforming our best algorithm KMRVarHybrid with 
" Actuallythe MFTIB integrates two plementary algorithms—discrete mixed membership modelling and continuous latent factormodelling That is, matrix factorisation—into a mon framework using the Bayesian approach, which illustrates the power ofcarefully bining different algorithms"
"Unfortunately, no NMAE or MAE was provided for MFTIB technique over Movielens M dataset, which makes itharder to pare different algorithms’ results with MFTIB"
" Considering these results, we conclude that the proposed approach appears to be petitive with the current stateoftheart approaches"
 Combining different kernelsAs discussed in Section 
", there can be different sources of information that can be used for making remendations"
"The proposed framework allows these different sources to be exploited by bining different kernels built from different The authors did not provide any numeric value, only a graph is presented showing the minimum value approximately to "
 Information Sciences Table Comparing the performance found with different binations of kernel for the SML dataset
 The average with the respective standard deviation of results overfold is shown
 The best results are shown in bold font
AlgorithmKMRibKMRDKMRFKMRibFDKMRibFKMRibDKMRFDKMRibFDKMRibFKMRibDKMRFDMAE
" In particular, we consider the rating information, feature information, and demographics information asdescribed in Section "
Table shows the performance of different binations of kernels for the SML dataset
" We have shown not only theMean Absolute Error MAE, but also a number of measures of the ability to classify ﬁlms as either highly rated or poorlyrated refer to Appendix B for details"
" We observe reasonable performance using just rating information, demographicinformation and feature information"
" Interestingly, for this dataset, bining kernels does not give signiﬁcantly betterperformance than using a kernel based on a single source of information"
" A plausible explanation of this observation is thatour error rates are close to the optimum that can be achieved there is a limit on the performance of any system due to theﬁckleness of the users making the ratings or, at least, we are close to the optimum given the way we have represented theproblem"
" On other datasets where, for example, ratings for some users are very sparse, demographic and feature informationcan be much more signiﬁcant"
 The other striking feature of Table is that multiplying kernels together seems to be moresuccessful than adding different kernels
Similar results not shown were observed in the case of FT and ML datasets
" We also attempted to linearly bine thepredictions from different kernels, but again this gave no improvement"
 Combining the user and itembased versionsThe methods of bining the user and itembased versions mentioned in Section 
 did not give any signiﬁcantimprovement over the individual results for the whole dataset
" To check the performance for imbalanced datasets, we randomly selected users and movies from the SML dataset, and users and movies from the FT dataset and randomly withheld x of their ratings"
" We checked the performance for two cases for Case , the value of x was chosenuniformly at random to lie between and That is, x , , whereas for Case , the value of x lies between and That is, x , "
 The latter case creates a relatively imbalanced subset of the dataset as pared to the former one
"Table shows the performance of userbased, itembased, and different methods used to bine the individual versions"
 We use the average of user and itembased versions as a baseline
 We observe that linearly bining the individualremender systems does not give signiﬁcant improvement over the baseline and the same is true for the second methoddiscussed in Appendix 
" However, KMRVarHybrid does signiﬁcantly improve the performance, with pvalue in the case of pairt test pared with the baseline remender found to be less than for both datasets"
 Similar results were observedfor other datasets as well
" What is evident from Table is that user and itembased versions of the algorithm are plementary and can improve the performance, if bined in a systematic way, for the imbalanced dataset"
Table Combining the userbased and itembased versions under imbalanced datasets
 The Case produces a relatively sparse subset of the dataset pared to Case
 The best results are shown in bold font
ApproachKMRibKMRubKMRib KMRubKMRLinearHybridKMRCntKMRVarHybridHybridMAECase FT
 Information Sciences 
" Sparse, skewed, and imbalanced datasetsIn practical applications remender systems often have access to limited and highly skewed information"
 Examples ofthese are
New user coldstart scenario where new users have relatively few ratings
Newitem coldstart scenario where new items have relatively few ratings
Long tail scenario where the majority of items have only a few ratings
Imbalanced sparse datasets where the majority of usersitems have only a few ratings
In the datasets that we have used so far our test set consists of randomly chosen ratings and these are overwhelmingly inthe dense region of the rating matrix
" That is, the users that we tested typically have rated many items and the items havebeen rated by many users"
" Thus, the results we have described so far are not strongly inﬂuenced by problems of limited andskewed information"
" However, these problems are often vital for a remender system to prosper"
" For example, to attractnew users it is highly beneﬁcial to be able to give them good quality remendations before they have made many ratings"
"Similarly, to introduce new items into the system it is useful to make sensible remendations even if the item has onlygained a few ratings"
"Table Comparing MAE observed in different approaches under new user coldstart scenario, for the SML dataset"
" The superﬁx M represents the correspondingversion of the KMR algorithm, where we take into account the max, mean, mode, and median of the output probability distribution"
 The best results are shownin bold font
ApproachKMRibKMRubKMRDKMRFKMRMibKMRMubKMRMFKMRMDKMRMibFBest MAEMAE
Rating WeightMean WeightMode WeightMedian Weightinoitcderp lanif eht ni sroitcderp liaudvdnii eht fo sthgeWi
Number of Ratings SML DataSet 
 Weight learning over the validation set for the new user coldstart problem SML dataset
 ‘‘Number of Ratings’’ represents the number of ratingsgiven by an active user in the training set
 Information Sciences We have tested the four scenarios outlined above by modifying the datasets we have been using to exaggerate the sparseness or skewness of the data
 We found that in all cases the standard predictor that we have been using up to now gives verypoor performance
" However, we could very substantially improve the performance by bining the standard predictorwith predictions using the mean, median and mode of Wuqi as described in Section "
 In the tables shown belowwe denote the modiﬁed predictor with a superscript M
We concentrate on the newuser coldstart scenario as the results are representative of all four scenarios
 The only majordifference is in the newitem coldstart scenario where the featurebased and demographicbased remenders also perform well as they are less inﬂuenced by a lack of ratings
" Results for the new item coldstart, long tail, and sparse data scenarios are given in Appendix C"
" New user coldstart scenarioTo test the performance of the proposed algorithms under the new user coldstart scenario, we selected randomusers, and kept their number of ratings in the training set to , , , , and "
" Keeping the number of ratings less than ensures that a user is new, and it captures well the new user coldstart problem"
" The corresponding MAE, representedby MAE, MAE, MAE, MAE, and MAE is shown in Table "
 Using the standard predictor provides very poor performance
" We can substantially improve the performance by bining the standard predictor with predictions using themean, median and mode of Wuqi as described in Section "
"Recall that we learn the weights for bining the standard predictor with the predictor using the mean, mode and median"
 The value of the weights depend on the dataset
 shows how the weights that have been learned change in the newuser coldstart scenario as we increase the number of ratings in the training set
 The new user coldstart scenario is taken asan example similar results were observed in both the new item coldstart and long tail scenarios
 The xaxis shows thenumber of ratings given by users selected as coldstart users and the yaxis shows the weights associated with differentpredictors
" We observe that the contribution of the mode, mean, and median predictors decreases with the increase inthe number of ratings, and ﬁnally bee zero when the maximum number of ratings are available, whereas, the contribution of the standard ratingsbased predictor increases with the increase in the number of ratings, and bees when themaximum number of ratings are available"
 Conclusion and future workRemender systems is a major research area in machine learning and data mining
" A number of approaches have beenproposed to solve the remender system problem including contentbased ﬁltering, Ontologybased approaches, supervised classiﬁcation techniques, unsupervised clustering techniques, memorybased collaborative ﬁltering, modelbased approaches spanning a number of algorithms including singular value deposition, matrix factorisation techniques, andprincipal ponent analysis"
" All these algorithms suffer from potential problems such as accuracy, scalability, sparsityand imbalanced dataset problems, coldstart problems, and long tail problems in one way or the other"
" Against this background, we propose a new class of kernelbased remendation algorithms that give stateoftheart performance andeliminates the recorded problems with the remender systems making the remendation generation techniquesapplicable to a wider range of practical situations and realworld scenarios"
"The proposed algorithm is petitive with what we believe to be the remender with the best performance proposedby ,"
 Interestingly both the proposed algorithm and the remender proposed in use kernelbased methodsthough in a very different way
" Although kernelbased techniques are known to give excellent performance, remendersystems are challenging because of the size of the datasets"
 By carefully choosing the constraints we have been able to createa kernelbased learning machine that can be trained in linear time in the number of data points
"The algorithm we have developed is very ﬂexible, thus we can easily adapt it so that it is either userbased or itembased"
In addition it can use other information such as textbased features and these features can be easily bined
 The best algorithm on the large datasets switches between the userbased and itembased information depending on the reliability of thepredictions as measured by the spread in the prediction of the algorithms
One interesting feature of the proposed approach is that we map the residues in the ratings onto a density function whichencodes the uncertainty in the residue
 For unseen residues we have interpreted the mapping Wuqi as an approximationto a density function for the residue
" Even though this function is not itself a density function it bees negative in someregions and is not normalised, nevertheless, it is very useful to consider the positive part of the function as a density function from which we can measure the mean, mode, median and variance"
" These measurements help in improving the performance, particularly in the case of sparse data"
One of the current drawbacks of the proposed algorithm is that the training occurs in one step
" Thus, when new data are addedit is costly to retrain the system"
 For practical remender systems this is a signiﬁcant problem as ratings are typically beingadded continuously
 We are currently investigating using a perceptronlike algorithm for updating the Lagrange multipliers
"AcknowledgmentsThe work reported in this paper has formed part of the Instant Knowledge Research Programme of Mobile VCE, the Virtual Centre of Excellence in Mobile & Personal Communications, www"
 The programme is cofunded by theM
 Information Sciences UK Technology Strategy Board’s Collaborative Research and Development programme
 Detailed technical reports on this research are available to all Industrial Members of Mobile VCE
" The third author has received funding from the European Community’s Seventh Framework Programme FP Speciﬁc Programme Cooperation, Theme , Information andCommunication Technologies under grant agreement No"
" Parameters brat, bfeat, and bdemoIn this section, we describe how we tuned the other parameters of the system"
"Parameters brat, bfeat, and bdemo brat bfeat determine the relative weights of rating, feature, and demographic kernelsin the ﬁnal prediction"
 Note that we assume the three bvalues are all positive and sum to one
" Sixtysix parameter sets weregenerated by producing all possible bination of parameters values, ranging from to "
 with differences of 
 Theparameter sets brat and bfeat gave the lowest MAE for all the datasets
Parameters q and q determine the relative weights of userbased and itembased CF in the ﬁnal prediction respectively
 We changed the value of q from to with a difference of 
 and the resulting MAE has been shown in A
" shows that for the SML dataset, the MAE is minimum at q "
", after which it starts increasing again whereas,for the FT dataset, the MAE keeps on decreasing, reaches its minimum at q "
", an then increases again"
 We choose theoptimal value of q to be 
 for SML and the FT dataset respectively
" Similarly, the value of q was trained for otherdatasets"
" It is worth noting that the itembased version got more weight except for the FT dataset in the ﬁnal prediction, forall datasets"
"In the hybrid variant, KMRCntHybrid the parameter hCnt determines the switching point between using the itembased anduserbased algorithms depending on the number of ratings of the item and the user"
 We determine the best value of hCntby varying it between and in steps of 
" shows the parameter hCnt learned for Case , as discussed in Section"
 over the validation set
" We observe that for the SML dataset, the MAE keeps on decreasing with the increase in the valueof hCnt, reaches its minimum at hCnt "
 and then either stays stable or starts increasing again
" For the FT dataset, theMAE decreases initially, when the value of hCnt changes from to "
 and then starts increasing when the value of hCnt increases beyond 
" For this reason, we choose the value hCnt to be "
 for SML and the FT datasets respectively
"Similarly, the value of hCnt was trained for other datasets"
"In the hybrid algorithm, KMRVarHybrid, the parameter hVar controls the switching from the userbased prediction to the itembased prediction depending on the uncertainty in the predictions measured by the variance in the Wuqi"
 To learn thisparameter we changed its value from to in steps of 
 and observed the corresponding MAE
 shows the parameter hVar learned for Case as discussed in Section 
" We observe that for the SML dataset, the MAEkeeps on decreasing with the increase in the value of hVar, reaches its peak at "
", and then starts increasing again"
" For the FTdataset, the decrease in the MAE is not very signiﬁcant, when hVar "
" however, afterwards, a sharp decrease in the MAE isobserved"
" The MAE keeps on decreasing, reaches its minimum at "
", and then either it stays stable or starts increasingagain"
 We choose the optimal value hVar to be 
 for SML and the FT datasets respectively
" Similarly, the valueof hVar was trained for other datasets"
 Parameter hVarAppendix B
 Mean Absolute Error MAEsigned by the user
" It is puted asMAE jpi aijnnXiMAE measures the average absolute deviation between a remender system’s predicted rating and a true rating aswhere pi and ai are the predicted and actual values of a rating, respectively, and n is the total number of rating records in thetest set"
" A rating record is a tuple consisting of a user ID, movie ID, and rating, huid,mid,ri, where r is the rating a reM"
 Information Sciences 
Value of Threshold Parameter ρ SML DataSet
EAM rorrlE etuosbA naeM
EAM rorrlE etuosbA naeM
Hybrid UBIBUBIBHybrid UBIBUBIB
Value of Threshold Parameter ρ FT DataSet
" Learning the optimal value of threshold parameter q, over the validation set, for the imbalanced datasets refer to Section "
mender system has to predict
" It has been used in ,,,,,,,,,"
 The aim of a remender system is tominimise the MAE score
"Normalised Mean Absolute Error NMAE has been used in ,, and is puted by normalising the MAE by a factor"
"The value of the factor depends on the range of the ratings for example, for the MovieLens dataset, it is "
" The motivationbehind this approach is that, the random guessing produces a score of "
" For further information, refer to "
"A closely related measure to the MAE is the Root Mean Squared Error RMSE, which is calculated as followsRMSE vuutnXpi ainiBoth MAE and RMSE are quoted in the literature"
 RMSE will be slightly more sensitive to large outliers
 The RMSE value willalways be greater than or equal to the MAE value
 Receiver Operating Characteristic ROC sensitivityROC measures the extent to which an information ﬁltering system can distinguish between good and bad items
 ROC sensitivity measures the probability with which a system accepts a good item
 The ROC sensitivity ranges from perfect to imperfect with 
" To use this metric for remender systems, we must ﬁrst determine which items are goodsignal and which are bad noise"
 We followed the procedure describe in while using this metric
" It has been used in,,"
 shows the ROCsensitive for the same set of algorithms on the same datasets as shown in Table 
" Precision, Recall, and FPrecision, recall, and F evaluate the effectiveness of a remender system by measuring the frequency with which ithelps users selectingremending a good item"
 Precision gives us the probability that a selected item is relevant
 Information Sciences EAM rorrlE etuosbA naeM
EAM rorrlE etuosbA naeM
Value of Threshold Parameter θ
 SML DataSetCntHybrid UBIBUBIBHybrid UBIBUBIB
Value of Threshold Parameter θ
" Learning the optimal value of threshold parameter, hCnt over the validation set, for the imbalanced datasets refer to Section "
gives us the probability that a relevant item is selected
" Precision and recall should be reported together, as increasing theprecision typically reduces the recall"
" The F Measure bines the precision and recall into a single metric and hasbeen used in many research projects, e"
" F is puted as followsF Precision RecallPrecision RecallThe ﬁrst step in puting the precision and recall is to divide items into two classes relevant and irrelevant, which isthe same as in ROCsensitivity"
" We calculated precision, recall, and F measures for each user, and reported the average results over all users"
 shows the F measure for the same set of algorithms on the same datasets as shown in Table 
" Sparse, skewed, and imbalanced datasetsIn this appendix we present results for the new item coldstart scenario, the long tail scenario and for sparse datasets"
 Performance evaluation under new item coldstart scenarioWe tested the new item coldstart scenario in exactly the same way we did the new user coldstart scenario
" That is, weselected random items, and kept the number of users in the training set who have rated the these item to , , , , and"
" shows again that the standard predictor fails under this scenario, whereas including the mean, mode and median predictor gives very good performance"
 We note that for new items the featurebased and demographicbased remenders work well for the coldstart scenario as these measures are not strongly inﬂuenced by a lack of ratinginformation for an item
 Information Sciences 
EAM rorrlE etuosbA naeM
EAM rorrlE etuosbA naeM
Value of Threshold Parameter θ
 SML DataSetvarHybrid UBIBUBIBHybrid UBIBUBIB
Value of Threshold Parameter θ
" Learning the optimal value of threshold parameter hvar, over the validation set, for the imbalanced datasets refer to Section "
A parison of the proposed algorithm with others in terms of ROCSensitivity metric
 The average with the respective standard deviation of results over fold is shown
 The best results are shown in bold font
AlgorithmBest ROCsensitivitySML
A parison of the proposed algorithm with others in terms of F measured over top remendations metric
 The average with the respective standarddeviation of results over fold is shown
 The best results are shown in bold font
Userbased CFItembased CFHybrid CFSVDKMRibKMRubKMRVarHybridAlgorithmUserbased CFItembased CFHybrid CFSVDKMRibKMRubKMRVarHybridM
 Information Sciences Table A
"Comparing the MAE observed in different approaches under new item coldstart scenario, for the SML dataset"
" The superﬁx M represents the correspondingversion of the KMR algorithm, where we take into account the max, mean, mode, and median of the output probability distribution"
 The average with therespective standard deviation of results over fold is shown
 The best results are shown in bold font
"Comparing MAE observed in different approaches under long tail scenario, for the SML dataset"
" The superﬁx M represents the corresponding version of theKMR algorithm, where we take into account the max, mean, mode, and median of the output probability distribution"
 The average with the respective standarddeviation of results over fold is shown
 The best results are shown in bold font
ApproachBest MAEMAEMAEMAEMAEMAEMAE
Comparing the performance of the algorithms under imbalanced and sparse datasets
" The superﬁx M represents the corresponding version of the KMRalgorithm, where we take into account the max, mean, mode, and median of the output probability distribution"
 The average with the respective standarddeviation of results over fold is shown
 The best results are shown in bold font
ApproachKMRibKMRubKMRFKMRDKMRMibKMRMubKMRMFKMRMDKMRMFibKMRibKMRubKMRFKMRDKMRMibKMRMubKMRMFKMRMDKMRMibFApproachKMRibKMRubKMRDKMRFKMRMibKMRMubKMRMDKMRMFKMRMibFC
 Performance evaluation under long tail scenarioThe long tail scenario is an important scenario for practical remender systems
" In a large Emerce system likeAmazon, there are huge numbers of items that are rated by very few users and hence the remendations generated forthese items would be poor, which could weaken the customers’ trust in the system"
To test the performance of the proposed algorithms under long tail scenario we created the artiﬁcial long tail scenario byrandomly selecting the of items in the tail
 The number of ratings given in the tail part were varied between andM
 Information Sciences —this ensures that the item is new and has very few ratings
" again shows the failure of the standard predictor inthe long tail scenario and the improvement obtained by using the mean, mode and median predictor"
" Performance evaluation under very sparse and imbalanced datasetsTo check the performance of the proposed approaches under very sparse and imbalanced datasets, we created subsets ofthe datasets by withholding x of the ratings from a rating proﬁle of useritem, where x xmin, xmax"
" We show results fortwo scenarios xmin , xmax , xmin , xmax "
" Changing the value of xmin creates different sparse subsets of the dataset, whereas keeping the value of xmax to ensures that the imbalanced dataset is created for eachscenario"
"For the SML and FT datasets, the results are shown in Table A"
 Again this follows the same pattern as the long tail andcoldstart scenarios
" Hyung Jun Ahn, A new similarity measure for collaborative ﬁltering to alleviate the new user coldstarting problem, Information Sciences Katja Astikainen, Liisa Holm, Esa Pitkanen, Sandor Szedmak, Juho Rousu, Towards structured output prediction of enzyme function, BMC Proceedings J"
" Hofmann, Unifying collaborative and contentbased ﬁltering, in Proceedings of the TwentyFirst International Conference on MachineLearning New York, NY, USA, ACM Press, , pp"
" Volinsky, The BellKor solution to the Netﬂix prize, in AT& T LabsResearch Technical report November, "
" Bell, Yehuda Koren, Lessons from the netﬂix prize challenge, SIGKDD Explorations Newsletters "
" Yolanda BlancoFernández, Martı´n LópezNores, Alberto GilSolla, Manuel RamosCabrer, José J"
" PazosArias, Exploring synergies between contentbased ﬁltering and spreading activation techniques in knowledgebased remender systems, Information Sciences "
" Breese, David Heckerman, and Carl Kadie, Empirical analysis of predictive algorithms for collaborative ﬁltering, in Proceedings of theFourteenth Conference on Uncertainty in Artiﬁcial Intelligence San Francisco, CA, USA, UAI’, Morgan Kaufmann Publishers Inc"
" Robin Burke, Integrating knowledgebased and collaborativeﬁltering remender systems, in AAAI Workshop on AI in Electronic Commerce, AAAI,, pp"
" Robin Burke, Hybrid remender systems Survey and experiments, User Modeling and UserAdapted Interaction "
" Mark Claypool, Anuja Gokhale, Tim Mir, Pavel Murnikov, Dmitry Netes, Matthew Sartin, Combining contentbased and collaborative ﬁlters in an onlinenewspaper, in Proceedings of ACM SIGIR Workshop on Remender Systems Berkeley, California, ACM, "
" Dennis DeCoste, Collaborative prediction using ensembles of maximum margin matrix factorizations, in Proceedings of the rd InternationalConference on Machine Learning New York, NY, USA, ICML ’, ACM, , pp"
" Ghazanfar, Adam PrügelBennett, An improved switching hybrid remender system using naive bayes classiﬁer and collaborativeﬁltering, in Lecture Notes in Engineering and Computer Science Proceedings of The International Multi Conference of Engineers and ComputerScientists , IMECS , March, , Hong Kong, , pp"
" Ghazanfar, Adam PrügelBennett, Building switching hybrid remender system using machine learning classiﬁers and collaborativeﬁltering, IAENG International Journal of Computer Science "
" Ghazanfar, Adam PrügelBennett, Novel signiﬁcance weighting schemes for collaborative ﬁltering generating improvedremendations in sparse environments, in DMIN, CSREA Press, , pp"
" Ghazanfar, Adam PrügelBennett, A scalable, accurate hybrid remender system, in Proceedings of the Third InternationalConference on Knowledge Discovery and Data Mining Washington, DC, USA, WKDD ’, IEEE Computer Society, , pp"
" Ghazanfar, Adam PrügelBennett, The advantage of careful imputation sources in sparse dataenvironment of remender systemsgenerating improved svdbased remendations, in IADIS European Conference on Data Mining, July "
" David Goldberg, David Nichols, Brian M"
" Oki, Douglas Terry, Using collaborative ﬁltering to weave an information tapestry, Communications of the Jonathan L"
" Riedl, Evaluating collaborative ﬁltering remender systems, ACM Transactions onACM "
" Thorsten Joachims, Training linear svms in linear time, in Proceedings of the th ACM SIGKDD International Conference on Knowledge Discovery andData Mining New York, NY, USA, KDD ’, ACM, , pp"
" HeungNam Kim, Abdulmotaleb ElSaddik, GeunSik Jo, Collaborative errorreﬂected models for coldstart remender systems, Decision Support Joseph A"
" Miller, David Maltz, Jonathan L"
" Gordon, John Riedl, Grouplens applying collaborative ﬁltering to usenetSystems "
"news, Communications of the ACM "
" Yehuda Koren, Factorization meets the neighborhood a multifaceted collaborative ﬁltering model, in Proceedings of the th ACM SIGKDDInternational Conference on Knowledge Discovery and Data Mining New York, NY, USA, KDD ’, ACM, , pp"
" Yehuda Koren, Factor in the neighbors scalable and accurate collaborative ﬁltering, ACM Transactions on Knowledge Discovery from Data TKDD "
" Csalogány, Methods for large scale SVD with missing values, in Proceedings of KDD Cup and Workshop, Citeseer, "
" Ken Lang, NewsWeeder learning to ﬁlter netnews, in Proceedings of the th International Conference on Machine Learning, Morgan Kaufmannpublishers Inc"
", San Mateo, CA, USA, , pp"
" Lawrence, Raquel Urtasun, Nonlinear matrix factorization with gaussian processes, in Proceedings of the th Annual InternationalConference on Machine Learning New York, NY, USA, ICML ’, ACM, , pp"
" DuenRen Liu, PeiYun Tsai, PoHuan Chiu, Personalized remendation of popular blog articles for mobile applications, Information Sciences Lester Mackey, David Weiss, Michael I"
" Jordan, Mixed membership matrix factorization, in Proceedings of the th International Conference on "
"Machine Learning, June "
" Benjamin Marlin, Collaborative ﬁltering a machine learning perspective, Master’s thesis, University of Toronto, "
" Benjamin Marlin, Modeling user rating proﬁles for collaborative ﬁltering, Advances in Neural Information Processing Systems "
" Rahul Mazumder, Trevor Hastie, Robert Tibshirani, Spectral regularization algorithms for learning large inplete matrices, Journal of MachineLearning Research "
" Prem Melville, Raymod J"
" Mooney, Ramadass Nagarajan, Contentboosted collaborative ﬁltering for improved remendations, in EighteenthNational Conference on Artiﬁcial Intelligence Menlo Park, CA, USA, American Association for Artiﬁcial Intelligence, , pp"
 Information Sciences Stuart E
" Middleton, Capturing knowledge of user preferences with remender systems, Ph"
" thesis, UNIVERSITY OF SOUTHAMPTON, UK, Stuart E"
" Middleton, Harith Alani, David C"
" De Roure, Exploiting synergy between ontologies and remender systems, in The Eleventh InternationalSeptember "
"World Wide Web Conference WWW, "
" Mooney, Loriene Roy, Contentbased book remending using learning for text categorization, in Proceedings of the Fifth ACMConference on Digital Libraries New York, NY, USA, DL ’, ACM, , pp"
" Pennock, Applying collaborative ﬁltering techniques to movie search for better ranking and browsing, in Proceedings of the th ACMSIGKDD International Conference on Knowledge Discovery and Data Mining, ACM, , pp"
" YoonJoo Park, Alexander Tuzhilin, The long tail of remender systems and how to leverage it, in Proceedings of the ACM Conference onRemender Systems New York, NY, USA, RecSys ’, ACM, , pp"
" Pazzani, A framework for collaborative, contentbased and demographic ﬁltering, Artiﬁcial Intelligence Review "
" Pazzani, Daniel Billsus, The Adaptive Web, SpringerVerlag, Berlin, Heidelberg, "
" Pennock, Eric Horvitz, Steve Lawrence, C"
" Lee Giles, Collaborative ﬁltering by personality diagnosis a hybrid memory and modelbasedapproach, in Proceedings of the th Conference on Uncertainty in Artiﬁcial Intelligence San Francisco, CA, USA, UAI ’, Morgan KaufmannPublishers Inc"
" HerreraViedma, A hybrid remender system for the selective dissemination of research resources in atechnology transfer ofﬁce, Information Sciences "
" Rennie Nathan Srebro, Fast maximum margin matrix factorization for collaborative prediction, in Proceedings of the nd InternationalConference on Machine Learning New York, NY, USA, ICML ’, ACM, , pp"
" Paul Resnick, Neophytos Iacovou, Mitesh Suchak, Peter Bergstrom, John Riedl, Grouplens an open architecture for collaborative ﬁltering of netnews,in Proceedings of the ACM Conference on Computer Supported Cooperative Work, CSCW ’, ACM, , pp"
" Mnih, Probabilistic matrix factorization, Advances in Neural Information Processing Systems "
" Badrul Sarwar, George Karypis, Joseph Konstan, John Reidl, Itembased collaborative ﬁltering remendation algorithms, in Proceedings of the thInternational Conference on World Wide Web New York, NY, USA, WWW ’, ACM, , pp"
" Badrul Sarwar, George Karypis, Joseph Konstan, John Riedl, Analysis of remendation algorithms for emerce, in Proceedings of the nd ACMConference on Electronic Commerce New York, NY, USA, EC ’, ACM, , pp"
" Badrul Sarwar, George Karypis, Joseph Konstan, John Riedl, Application of dimensionality reduction in remender systema case study, in ACMWEBKDD Workshop, Citeseer, "
" Badrul Sarwar, George Karypis, Joseph Konstan, John Riedl, Remender systems for largescale emerce scalable neighborhood formation usingclustering, in Proceedings of the Fifth International Conference on Computer and Information Technology, "
" Vincent SchickelZuber, Boi Faltings, Using an ontological apriori score to infer user’s preferences, in Workshop on Remender SystemsECAI, Jesus SerranoGuerrero, Enrique HerreraViedma, Jose A"
" Olivas, Andres Cerezo, Francisco P"
" Romero, A google wavebased fuzzy remender systemto disseminate information in university digital libraries "
", Information Sciences "
" Upendra Shardanand, Pattie Maes, Social information ﬁltering algorithms for automating word of mouth, in Proceedings of the SIGCHI Conference onHuman factors in Computing Systems New York, NY, USA, CHI ’, ACM PressAddisonWesley Publishing Co"
" Nathan Srebro, Jasson D"
" Jaakkola, Maximummargin matrix factorization, Advances in Neural Information Processing Systems , pp"
" Stern, Ralf Herbrich, Thore Graepel, Matchbox large scale online bayesian remendations, in Proceedings of the th InternationalConference on World Wide Web New York, NY, USA, WWW ’, ACM, , pp"
" Sandor Szedmak, Ni Yizhao, R"
" Gunn Steve, Maximum margin learning with inplete data learning networks instead of tables, Journal of MachineLearning Research Proceedings Track "
" Gábor Takács, István Pilászy, Bottyán Németh, Domonkos Tikk, Investigation of various matrix factorization methods for large remender systems,in Proceedings of the nd KDD Workshop on LargeScale Remender Systems and the Netﬂix Prize Competition New York, NY, USA, NETFLIX ’,ACM, , pp"
" Gábor Takács, István Pilászy, Bottyán Németh, Domonkos Tikk, Scalable collaborative ﬁltering approaches for large remender systems, Journal of Loren Terveen, Will Hill, Brian Amento, David McDonald, Josh Creter, Phoaks a system for sharing remendations, Communications of the ACM Machine Learning Research "
" Robin van Meteren, Maarten van Someren, Using contentbased ﬁltering for remendation, in Proceedings of the Machine Learning in the NewInformation Age MLnetECML Workshop, Citeseer, "
" Manolis Vozalis, Konstantinos G"
" Margaritis, Applying SVD on generalized itembased ﬁltering, International Journal of Computer Science andApplications "
" Manolis Vozalis, Konstantinos G"
" Margaritis, Using svd and demographic data for the enhancement of generalized collaborative ﬁltering, Information Mingru Wu, Collaborative ﬁltering via ensembles of matrix factorizations, in Proceedings of KDD Cup and Workshop, Citeseer, "
" GuiRong Xue, Chenxi Lin, Qiang Yang, WenSi Xi, HuaJun Zeng, Yong Yu, Zheng Chen, Scalable collaborative ﬁltering using clusterbased smoothing,in Proceedings of the th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval New York, NY, USA,SIGIR ’, ACM, , pp"
" Daoqiang Zhang, ZhiHua Zhou, Songcan Chen, Nonnegative matrix factorization on kernels, in Proceedings of the th Paciﬁc Rim InternationalConference on Artiﬁcial Intelligence Berlin, Heidelberg, PRICAI’, SpringerVerlag, , pp"
"See discussions, stats, and author profiles for this publication at httpswww"
"netpublicationThe Unreasonable Effectiveness of DataArticle in Intelligent Systems, IEEE · May DOI "
" · Source IEEE XploreCITATIONS authors, includingAlon HalevyGoogle Inc"
"SEE PROFILEREADS,Peter NorvigGoogle Inc"
"SEE PROFILE PUBLICATIONS , CITATIONS PUBLICATIONS , CITATIONS Some of the authors of this publication are also working on these related projectsBiomedical data integration View projectAll content following this page was uploaded by Peter Norvig on December "
The user has requested enhancement of the downloaded file
"E X P E R T O P I N I O NContact Editor Brian Brannon, bbrannonputer"
"orgThe Unreasonable Effectiveness of DataAlon Halevy, Peter Norvig, and Fernando Pereira, GoogleEugene Wigner’s article “The Unreasonable Effectiveness of Mathematics in the Natural Sciences” examines why so much of physics can be neatly explained with simple mathematical formulassuch as f ma or e mc"
" Meanwhile, sciences that involve human beings rather than elementary particles have proven more resistant to elegant mathematics"
 Economists suffer from physics envy over their inability to neatly model human behavior
" An informal, inplete grammar of the English language runs over , pages"
" Perhaps when it es to natural language processing and related fi elds, we’re doomed to plex theories that will never have the elegance of physics equations"
" But if that’s so, we should stop acting as if our goal is to author extremely elegant theories, and instead embrace plexity and make use of the best ally we have the unreasonable effectiveness of data"
"One of us, as an undergraduate at Brown University, remembers the excitement of having access to the Brown Corpus, containing one million English words"
" Since then, our fi eld has seen several notable corpora that are about times larger, and in , Google released a trillionword corpus with frequency counts for all sequences up to fi ve words long"
" In some ways this corpus is a step backwards from the Brown Corpus it’s taken from unfi ltered Web pages and thus contains inplete sentences, spelling errors, grammatical errors, and all sorts of other errors"
 It’s not annotated with carefully handcorrected partofspeech tags
 But the fact that it’s a million times larger than the Brown Corpus outweighs these drawbacks
" A trillionword corpus—along with other Webderived corpora of millions, billions, or trillions of links, videos, images, tables, and user interactions—captures even very rare aspects of human behavior"
" So, this corpus could serve as the basis of a plete model for certain tasks—if only we knew how to extract the model from the data"
Learning from Text at Web ScaleThe biggest successes in naturallanguagerelated machine learning have been statistical speech recognition and statistical machine translation
 The reason for these successes is not that these tasks are easier than other tasks they are in fact much harder than tasks such as document classifi cation that extract just a few bits of information from each document
 The reason is that translation is a natural task routinely done every day for a real human need think of the operations of the European Union or of news agencies
 The same is true of speech transcription think of closedcaption broadcasts
" In other words, a large training set of the inputoutput behavior that we seek to automate is available to us in the wild"
" In contrast, traditional natural language processing problems such as document classifi cation, partofspeech tagging, namedentity recognition, or parsing are not routine tasks, so they have no large corpus available in the wild"
" Instead, a corpus for these tasks requires skilled human annotation"
" Such annotation is not only slow and expensive to acquire but also diffi cult for experts to agree on, being bedeviled by many of the diffi culties we discuss later in relation to the Semantic Web"
 The fi rst lesson of Webscale learning is to use available largescale data rather than hoping for annotated data that isn’t available
" For instance, we fi nd that useful semantic relationships can be automatically learned from the statistics of search queries and the corresponding results or from the accumulated evidence of Webbased text patterns and formatted tables, in both cases without needing any manually annotated data"
 IEEEPublished by the IEEE Computer SocietyiEEE iNTElliGENT SYSTEMSAuthorized licensed use limited to Univ of Calif Berkeley
" Downloaded on February , at from IEEE Xplore"
 Another important lesson from statistical methods in speech recognition and machine translation is that memorization is a good policy if you have a lot of training data
 The statistical language models that are used in both tasks consist primarily of a huge database of probabilities of short sequences of consecutive words ngrams
 These models are built by counting the number of occurrences of each ngram sequence from a corpus of billions or trillions of words
" Researchers have done a lot of work in estimating the probabilities of new ngrams from the frequencies of observed ngrams using, for example, GoodTuring or KneserNey smoothing, leading to elaborate probabilistic models"
" But invariably, simple models and a lot of data trump more elaborate models based on less data"
" Similarly, early work on machine translation relied on elaborate rules for the relationships between syntactic and semantic patterns in the source and target languages"
" Currently, statistical translation models consist mostly of large memorized phrase tables that give candidate mappings between specific source and targetlanguage phrases"
"Instead of assuming that general patterns are more effective than memorizing specific phrases, today’s translation models introduce general rules only when they improve translation over just memorizing particular phrases for instance, in rules for dates and numbers"
 Similar observations have been made in every other application of machine learning to Web data simple ngram models or linear classifiers based on millions of specific features perform better than elaborate models that try to discover general rules
 In many cases there appears to be a threshold of sufficient data
" For example, James Hays and Alexei A"
" Efros addressed the task of scene pletion removing an unwanted, unsightly automobile or exspouse from a photograph and filling in the background with pixels taken from a large corpus of other photos"
" With a corpus of thousands of photos, the results were poor"
" But once they accumulated millions of photos, the same algorithm performed quite well"
" We know that the number of grammatical English sentences is theoretically infinite and the number of possible Mbyte photos is ,,"
" However, in practice we humans care to make only a finite number of distinctions"
" For many tasks, once we have a billion or so examples, we essentially have a closed set that repreFor many tasks, words and word binations provide all the representational machinery we need to learn from text"
"sents or at least approximates what we need, without generative rules"
"For those who were hoping that a small number of general rules could explain language, it is worth noting that language is inherently plex, with hundreds of thousands of vocabulary words and a vast variety of grammatical constructions"
" Every day, new words are coined and old usages are modified"
 This suggests that we can’t reduce what we want to say to the free bination of a few abstract primitives
" For those with experience in smallscale machine learning who are worried about the curse of dimensionality and overfitting of models to data, note that all the experimental evidence from the last decade suggests that throwing away rare events is almost always a bad idea, because much Web data consists of individually rare but collectively frequent events"
" For many tasks, words and word binations provide all the representational machinery we need to learn from text"
 Human language has evolved over millennia to have words for the important concepts let’s use them
 Abstract representations such as clusters from latent analysis that lack linguistic counterparts are hard to learn or validate and tend to lose information
 Relying on overt statistics of words and word cooccurrences has the further advantage that we can estimate models in an amount of time proportional to available data and can often parallelize them easily
" So, learning from the Web bees naturally scalable"
The success of ngram models has unfortunately led to a false dichotomy
" Many people now believe there are only two approaches to natural language processinga deep approach that relies on handcoded grammars and ontologies, represented as plex networks of relations and a statistical approach that relies on learning ngram statistics from large corpora"
"	In reality, three orthogonal problems arise choosing a representation language,encoding a model in that language, and performing inference on the model"
" Each problem can be addressed in several ways, resulting in dozens of approaches"
" The deep approach that was popular in the s used firstorder logic or something similar as the representation language, encoded a model with the labor of a team of graduate students, and did inference with plex inference rules appropriate to the representation language"
" In the s and s, it became fashionable to Marchapril www"
orgintelligent Authorized licensed use limited to Univ of Calif Berkeley
" Downloaded on February , at from IEEE Xplore"
" use finite state machines as the representation language, use counting and smoothing over a large corpus to encode a model, and use simple Bayesian statistics as the inference method"
"But many other binations are possible, and in the s, many are being tried"
" For example, Lise Getoor and Ben Taskar collect work on statistical relational learning—that is, representation languages that are powerful enough to represent relations between objects such as firstorder logic but that have a sound, probabilistic definition that allows models to be built by statistical learning"
 Taskar and his colleagues show how the same kind of maximummargin classifier used in support vector machines can improve traditional parsing
" Stefan Schoenmackers, Oren Etzioni, and Daniel S"
 Weld show how a relational logic and a millionpage corpus can answer questions such as “what vegetables help prevent osteoporosis” by isolating and bining the relational assertions that “kale is high in calcium” and “calcium helps prevent osteoporosis
”Semantic Web versus Semantic InterpretationThe Semantic Web is a convention for formal representation languages that lets software services interact with each other “without needing artificial intelligence
"” A software service that enables us to make a hotel reservation is transformed into a Semantic Web service by agreeing to use one of several standards for representing dates, prices, and locations"
 The service can then interoperate with other services that use either the same standard or a different one with a known translation into the chosen standard
" As Tim BernersLee, James Hendler, and Ora Lassila write, “The Semantic Web will enable machines to prehend semantic documents and data, not human speech and writings"
”The problem of understanding human speech and writing—the semantic interpretation problem—is quite different from the problem of software service interoperability
" Semantic interpretation deals with imprecise, ambiguous natural languages, whereas service interoperability deals with making data precise enough that the programs operating on the data will function effectively"
" Unfortunately, the fact that the word “semantic” appears in both “Semantic Web” and “semantic interpretation” means that the two probBecause of a huge shared cognitive and cultural context, linguistic expression can be highly ambiguous and still often be understood correctly"
"lems have often been conflated, causing needless and endless consternation and confusion"
 The “semantics” in Semantic Web services is embodied in the code that implements those services in accordance with the specifications expressed by the relevant ontologies and attached informal documentation
 The “semantics” in semantic interpretation of natural languages is instead embodied in human cognitive and cultural processes whereby linguistic expression elicits expected responses and expected changes in cognitive state
" Because of a huge shared cognitive and cultural context, linguistic expression can be highly ambiguous and still often be understood correctly"
" Given these challenges, building Semantic Web services is an engineering and sociological challenge"
" So, even though we understand the required technology, we must deal with significant hurdles Ontology writing"
 The important easy cases have been done
" For example, the Dublin Core defines dates, locations, publishers, and other concepts that are sufficient for card catalog entries"
"org defines chromosomes, species, and gene sequences"
 Other organizations provide ontologies for their specific fields
 But there’s a long tail of rarely used concepts that are too expensive to formalize with current technology
" Project Halo did an excellent job of encoding and reasoning with knowledge from a chemistry textbook, but the cost was US, per page"
 Obviously we can’t afford that cost for a trillion Web pages
 PubDifficulty of implementation
 lishing a static Web page written in natural language is easy anyone with a keyboard and Web connection can do it
" Creating a databasebacked Web service is substantially harder, requiring specialized skills"
 Making that service pliant with Semantic Web protocols is harder still
" Major sites with petent technology experts will find the extra effort worthwhile, but the vast majority of small sites and individuals will find it too difficult, at least with current tools"
" In some domains, peting factions each want to promote their own ontology"
" In other domains, the entrenched leaders of the field oppose any ontology because it would level the playing field for their petitors"
" This is a problem in diplomacy, not technology"
" As Tom Gruber says, “Every ontology is a treaty—a social agreement—among people with some mon motive in sharing"
"” When a motive for sharing is lacking, so are mon ontologies"
Inaccuracy and deception
orgintelligent iEEE iNTElliGENT SYSTEMSAuthorized licensed use limited to Univ of Calif Berkeley
" Downloaded on February , at from IEEE Xplore"
 know how to build sound inference mechanisms that take true premises and infer true conclusions
" But we don’t have an established methodology to deal with mistaken premises or with actors who lie, cheat, or otherwise deceive"
" Some work in reputation management and trust exists, but for the time being we can expect Semantic Web technology to work best where an honest, selfcorrecting group of cooperative users exists and not as well where petition and deception exist"
The challenges for achieving accurate semantic interpretation are different
 We’ve already solved the sociological problem of building a network infrastructure that has encouraged hundreds of millions of authors to share a trillion pages of content
 We’ve solved the technological problem of aggregating and indexing all this content
" But we’re left with a scientific problem of interpreting the content, which is mainly that of learning as much as possible about the context of the content to correctly disambiguate it"
 The semantic interpretation problem remains regardless of whether or not we’re using a Semantic Web framework
" The same meaning can be expressed in many different ways, and the same expression can express many different meanings"
" For example, a table of pany information might be expressed in ad hoc HTML with column headers called “Company,” “Location,” and so on"
" Or it could be expressed in a Semantic Web format, with standard identifiers for “Company Name” and “Location,” using the Dublin Core Metadata Initiative pointencoding scheme"
" But even if we have a formal Semantic Web “Company Name” attribute, we can’t expect to have an ontology for every possible value of this attribute"
" For example, we can’t know for sure what pany the string “Joe’s Pizza” refers to because hundreds of businesses have that name and new ones are being added all the time"
 We also can’t always tell which business is meant by the string “HP
” It could refer to Helmerich & Payne Corp
 when the column is populated by stock ticker symbols but probably refers to HewlettPackard when the column is populated by names of large technology panies
 The problem of semantic interpretation remains using a Semantic Web formalism just means that semantic interpretation must be done on shorter strings that fall between angle brackets
belong in the same column of a table
 We’ve never before had such a vast collection of tables and their schemata at our disposal to help us resolve semantic heterogeneity
" Using such a corpus, we hope to be able to acplish tasks such as deing when “Company” and “Company Name” are synonyms, deing when “HP” means Helmerich & Payne or HewlettPackard, and determining that an object with attributes “passengers” and “cruising altitude” is probably an aircraft"
"The same meaning can be expressed in many different ways, and the same expression can express many different meanings"
 What we need are methods to infer relationships between column headers or mentions of entities in the world
" These inferences may be incorrect at times, but if they’re done well enough we can connect disparate data collections and thereby substantially enhance our interaction with Web data"
" Interestingly, here too Webscale data might be an important part of the solution"
 The Web contains hundreds of millions of independently created tables and possibly a similar number of lists that can be transformed into tables
 These tables represent structured data in myriad domains
 They also represent how different people organize data—the choices they make for which columns to include and the names given to the columns
" The tables also provide a rich collection of column values, and values that they deed ExamplesHow can we use such a corpus of tables Suppose we want to find synonyms for attribute names—for example, when “Company Name” could be equivalent to “Company” and “price” could be equivalent to “discount”"
" Such synonyms differ from those in a thesaurus because here, they are highly context dependent both in tables and in natural language"
" Given the corpus, we can extract a set of schemata from the tables’ column labels for example, researchers reliably extracted "
" million distinct schemata from a collection of million tables, not all of which had schema"
 We can now examine the cooccurrences of attribute names in these schemata
" If we see a pair of attributes A and B that rarely occur together but always occur with the same other attribute names, this might mean that A and B are synonyms"
 We can further justify this hypothesis if we see that data elements have a significant overlap or are of the same data type
" Similarly, we can also offer a schema autoplete feature for database designers"
" For example, by analyzing such a large corpus of schemata, we can discover that schemata that have the attributes Make and Model also tend to have the attributes Year, Color, and Mileage"
" Providing such feedback to schemata creators can save them time but can also help them use more mon attribute names, thereby decreasing a possible Marchapril www"
orgintelligent Authorized licensed use limited to Univ of Calif Berkeley
" Downloaded on February , at from IEEE Xplore"
 source of heterogeneity in Webbased data
" Of course, we’ll find immense opportunities to create interesting data sets if we can automatically bine data from multiple tables in this collection"
 This is an area of active research
"Another opportunity is to bine data from multiple tables with data from other sources, such as unstructured Web pages or Web search queries"
" For example, Marius Paşca also considered the task of identifying attributes of classes"
" That is, his system first identifies classes such as “Company,” then finds examples such as “Adobe Systems,” “Macromedia,” “Apple Computer,” “Target,” and so on, and finally identifies class attributes such as “location,” “CEO,” “headquarters,” “stock price,” and “pany profile"
"” Michael Cafarella and his colleagues showed this can be gleaned from tables, but Paşca showed it can also be extracted from plain text on Web pages and from user queries in search logs"
" That is, from the user query “Apple Computer stock price” and from the other information we know about existing classes and attributes, we can confirm that “stock price” is an attribute of the “Company” class"
" Moreover, the technique works not just for a few dozen of the most popular classes but for thousands of classes and tens of thousands of attributes, including classes like “Aircraft Model,” which has attributes “weight,” “length,” “fuel consumption,” “interior photos,” “specifications,” and “seating arrangement"
"” Paşca shows that including query logs can lead to excellent performance, with percent precision over the top attributes per class"
" Choose a representation that can use unsupervised learning on unlabeled data, which is so much more plentiful than labeled data"
" Represent all the data with a nonparametric model rather than trying to summarize it with a parametric model, because with very large data sources, the data holds a lot of detail"
" For natural language applications, trust that human language has already evolved words for the important concepts"
" See how far you can go by tying together the words that are already there, rather than by inventing new concepts with clusters of words"
" Now go out and gather some data, and see what it can do"
"Choose a representation that can use unsupervised learning on unlabeled data, which is so much more plentiful than labeled data"
" Wigner, “The Unreasonable Effectiveness of Mathematics in the Natural Sciences,” Comm"
" Pure and Applied Mathematics, vol"
", A Comprehensive Grammar of the English Language, Longman, "
" Carroll, Computational Analysis of PresentDay American English, Brown Univ"
" Franz, Web T Gram Version , Linguistic Data Consortium, "
" Vasserman, “Translating Queries into Snippets for Improved Query Expansion,” Proc"
" Computational Linguistics Coling , Assoc"
" Computational Linguistics, , pp"
", “Learning to Create DataIntegrating Queries,” Proc"
" Very Large Databases VLDB , Very Large Database Endowment, , pp"
" Efros, “Scene Completion Using Millions of Photographs,” Comm"
" Taskar, Introduction to Statistical Relational Learning, MIT Press, "
", “MaxMargin Parsing,” Proc"
" Empirical Methods in Natural Language Processing EMNLP , Assoc"
" for Computational Linguistics, , pp"
" Weld, “Scaling Textual Inference to the Web,” Proc"
" Empirical Methods in Natural Language Processing EMNLP , Assoc"
" for Computational Linguistics, , pp"
" Lassila, “The Semantic Web,” Scientific Am"
", “Towards a Quantitative, PlatformIndependent Analysis of Knowledge Systems,” Proc"
" Principles of Knowledge Representation, AAAI Press, , pp"
" “Interview of Tom Gruber,” AIS SIGSEMIS Bull"
", “WebTables Exploring the Power of Tables on the Web,” Proc"
" Very Large Data Base Endowment VLDB , ACM Press, , pp"
" Paşca, “Organizing and Searching the World Wide Web of Facts"
" Step Two Harnessing the Wisdom of the Crowds,” Proc"
 th Int’l World Wide Web Conf
Alon Halevy is a research scientist at Google
 Contact him at halevygoogle
Peter Norvig is a research director at Google
 Contact him at pnorviggoogle
Fernando Pereira is a research director at Google
 Contact him at pereiragoogle
orgintelligent iEEE iNTElliGENT SYSTEMSView publication statsView publication statsAuthorized licensed use limited to Univ of Calif Berkeley
" Downloaded on February , at from IEEE Xplore"
" IEEE First International Conference on Data Science in Cyberspace IEEE First International Conference on Data Science in CyberspaceOPSDS a semantic data integration and service system based on domain ontology Liu Xin School of Computer and Communication Engineering University of Science and Technology Beijing Beijing, China liuxin_ustb"
" Hu Chungjin School of Computer and Communication Engineering University of Science and Technology Beijing Beijing, China hucj"
" Huang Jianyi School of Computer and Communication Engineering University of Science and Technology Beijing Beijing, China huangjianyi_ustb"
" Liu Feng School of Computer and Communication Engineering University of Science and Technology Beijing Beijing, China m"
" Abstract—For the distributed, heterogeneous, relational plex data sources of petroleum engineering, we present an oilproduction engineering semanticbased data integration system OPSDS"
 OPSDS establishes a semantic data integration and service system based on domain ontology on the premise of building a global semantic model and realizing the global semantic search
" The global semantic data model applied to various oil fields is set up by ontology extraction, ontology evolution, ontology bination and semantic constraints"
" The domainoriented data integration to provide the data access and is realized by ontology mapping, query shared service transformation, and data cleaning"
" Users and upper applications can have a direct access to underlying plex data sources in times of need through the global semantic data model, and the cleaned data can be returned in a unified format"
 OPSDS has been realized and got extensive use in many platforms of China National Petroleum CorporationCNPC
" It has been found that the method can not only provide the prehensive and realtime data support for oil and gas wells, but also improve the production and recovery efficiency with good application"
 Keywords petroleum information system data service technology of domain data semantic integration ontology petroleum engineering distributed data processingI
INTRODUCTION Oil is an important strategic resource of a country
 But oil and gas production of many oil fields around the world is trending downward and more fields are transiting into decline each year
 Reports indicate that a hybrid average production decline rate for oil fields worldwide is more or less
" Currently, various research methods to improve oil and gas production are booming"
" Production design, decision analysis, diagnosis and management of oil and gas wells are the keys to enhance productivity, reduce costs and increase profits"
" Optimal design of oil and gas wells involves large amounts of data, such as production data, well profiles, equipment data, geological structures, seismic data, reservoir data, etc"
"data with huge value have drawn attentions of academia, industry and government"
" For the purpose of improving production and saving energy, it is a highly advocated idea to dig out the value to utilize the data more efficiently"
 We focus on the features of oil data firstly
" Oil field is posed of a number of oilproduction plants, exploration institutes, geophysical research institutes and other units"
" Different units collect, collate, process, analysis and apply different kinds of data, and store corresponding data in their own databases which results in that different types of data are stored in different professional databases"
" Each database has its own specialized data organization and naming conventions, leading to system heterogeneity, syntax heterogeneity, structure heterogeneity and semantic heterogeneity"
 System Heterogeneity means operating environments and hardware platforms of data are various in different oil panies
 Syntax Heterogeneity indicates that oil panies take different storage methods for different types of data
" For example, some data are stored in relational databases, and some are stored in forms of text files"
 that different oil fields intends Structure Heterogeneity represent the same type of data with different data schemas
 A typical example is shown in Figure 
 Semantic Heterogeneity mainly refers to different words with the same meaning or the same words with different meanings
 Sucker rod data with structure heterogeneity
 Authorized licensed use limited to Hochschule Heilbronn
" Downloaded on June , at UTC from IEEE Xplore"
 Figure shows an instance of sucker rod structural data stored in relational database
" A well requires a set of sucker rod data, which contain rod level, length, diameter and other information, and different rod levels correspond to different rod lengths"
" For a multilevel sucker rod, D bines the threelevel rod length and saves as one field, D saves rod lengths into three rows according to different rod level, and D represents three fields in one row"
 that information And new features of data integration are concluded
" integration Bernstein and Haas say bines information from different sources into a unified format, and they specify the plicacy of integration after investigating the tools and technologies of data integration in the enterprise"
 They also indicate that every step of the integration process requires a good deal of manual intervention and more automation is surely possible
" Oil production engineering data are dynamic, updated in real time, and in critical instant need"
" Each oil field has not only production data every day, but also constantly updated basic data and regularly updated equipment data"
 So it is vital to ensure the realtime of data for upper applications
 Complex semantic associations
 It mainly refers to the plex associations between different data
" For example, we regard the well whose deviation angle is less than degrees as a vertical well, the well whose deviation angle is greater than degrees as a horizontal well, and the well whose deviation angle is between degrees and degrees as a inclined well"
 Focusing on the characteristics of the data leads us to discuss the challenge of traditional data management
" On one hand, data of oil fields are scatteredly stored, the logical organization lacks of ‘soul’, and the data schemas are various without naming rules and management methods"
 Thus it is urgent to establish a global semantic data model which is suitable for multiple oil fields to achieve the unification of data management platform
" On the other hand, data of oil panies are considerable autonomy, which increas the difficulty of data exchange and sharing"
 But data from different professional databases are increasingly need to work together to support upper applications of the domain
 So semantic data integration and building uniform interfaces directly accessing to the underlying data resources is of great significance
" In this paper, a petroleumengineering semanticbased data service OPSDS is presented to achieve a semantic data integration and service system based on domain ontology"
 The system provides a semantically richer global ontology and querybased access to the distributed and heterogeneous data
 OPSDS shields the plexity and sources of data to enable users and upper applications to take full advantage of data resources in a payasyougo approach everywhere
" Besides, a reasoning function is available for inferring the hidden information behind the semantic associations"
" RELATED WORKAs the plexity of data leads to a raising challenge for traditional data management, it is of utmost importance to generate a new way of data service"
 Data services provide access to data drawn from one or more underlying information sources
" Serviceenabling data stores, integrated data services and cloud data services inthe enterprise world are introduced in detail, but semantic relationships are not considered"
 propose a framework for scientific data services
 Data integration is a pervasive challenge faced in applications that need to query data residing at multiple autonomous and heterogeneous data sources 
" present a collaborative environment called distributed interoperable manufacturing platform, in which the STEPNC data model is built to promote data exchange among heterogeneous systems"
" propose a serviceoriented framework for integration of domainspecific data models in scientific workflows, which links the data sources and upper applications"
" However, the data model is built by domain experts, which is subjectivity and lacking in semantic relations between the data elements"
" Mapping of data with association relationships are constructed, but a certain inaccuracy exists"
 present three existing enterprise ontologies with different levels of expressivity
" Apparently, their work is not for specific domain, especially for the oil field"
 ARCHITECTURE OF OPSDS A
 OPSDS architecture OPSDS provides a rich semantic view of the underlying data and interfaces enabling users and upper applications to access data
 The architecture of OPSDS is shown in Figure 
" The bottom of the architecture is data sources storing in different databases, such as Oracle, SQL Server, etc"
 The middle layer is local ontologies extracting from the data sources below
" And then, the global ontology is formed as the local ontologies"
 Users and upper result of bining applications can access the data resources easily
" The only thing service consumers need to do is to send queries according to the global ontology, and then the desired data can be received"
 Architecture of OPSDS
 Authorized licensed use limited to Hochschule Heilbronn
" Downloaded on June , at UTC from IEEE Xplore"
 Construction of global ontology We adopt a hybrid strategy to construct the global ontology
" On the one hand, a topdown approach is used to filter the demand data"
" Entities, attributes and relationships"
 For Peer Review Only between entities can be got by classifying and organizing the data
" On the other hand, take a bottomupmethod to build local ontologies, which are results of extracting schemas of databases and items of synonym list"
" And then the global ontology is established according to ontology evolution, ontology mapping and imposed semantic constraints"
 Figure shows the construction process of global ontology
 The construction process of a global ontology
" Data of petroleum exploration and development vary in many aspects, such as exploration, production, geology, seismology, well logging, well drilling, etc, while data of petroleum engineering are just a part of them"
" Thus, firstly, we filter data to define the basic requirements, and classify, organize and aggregate the data to form entities, attributes and the relationships between entities labels"
 Then referring to the data dictionary
 identify Take production data entity and equipment data entity as examples
 The entity models are as follows
" Production data entity Production oil_output, gas_output, flow_pressure…… Equipment data entity Equipment pumping_unit, sucker_rod, defueling_pump……Since the majority of petroleum engineering data are stored in relational databases, we are here to study mapping fromRelational Database to OWL ontology"
" A relational database is posed of a set of relational schemas, including basic table structures and integrity constraints"
" An OWL ontology consists of classes, properties, individuals and axioms"
" As we aim at providing mappings between data models and ontologies, classes and properties are considered in this step"
" Because of individuals are widely exist in underlying database, individuals are not taken into consideration"
 Axioms are covered later in this paper
 The synonym list of petroleum engineering is built by domain experts and DBAs by reference to exploration and development handbooks of oil fields
 The synonymous items with different names and same meaning in the handbooks are gathered together in the synonym list to solve the phenomena of semantic heterogeneity
" Based on the schemas of tables in the specialized databases, we analyze characteristics of tables and constraints between tables, and then define an oil production engineering data source ontology OPDSOnto, which maps synonyms in the synonym list and schemas of tables to classes and properties in the ontology"
 The local ontology can be generated automatically through the program
 Getting innovations from Relational
"OWL, OWLRDBO and ProInnovator, we design OPDSOnto to describe tables, columns relations of tables and synonymy"
 Then extraction rules are defined as follows
" Convert tables and columns in databases to classes OPDSOnto Table or OPDSOnto Column owl Class, which express main concepts of the domain"
 Hierarchical relationships between tables and columns are presented by OPDSOnto hasParent and OPDSOnto hasChild owl ObjectProperty with owl inverseOf constructs
" OPDSOnto hasChild has a direction from domain Table to range Column, while OPDSOntohasParent has an opposite direction"
" Relationships between columns in one table are presented by OPDSOnto hasBrother, which is defined in owl ObjectProperty"
" If a column C in table A is the foreign key to table B, OPDSOnto hasChild represents the foreign key constraint, from domain column C to range Table B, while OPDSOntohasParent is the reverse semantic association"
" Datatype Properties of classes are defined, such as OPDSOnto isPK, OPDSOnto isFK, OPDSOnto isNullable, OPDSOnto dataType, to describe the primary key, the foreign key, nullable and data type of the individual"
" Extract the items which express the same meaning from the synonym list to convert into classes, and the relationships between classes are defined as OPDSOntohasSynonymy, which is built in owl ObjectProperty"
 Semantic Relationship is defined as shown below
 DEFINITIONSemantic Relationship
" ∀ x,x……xn,z, if xz, xz, …… , xnz, then xx"
"xn, where indicates the relation between two classes and identifies the semantic relationships between classes"
" According to the definition, the relationships that are hasParent, hasChild, hasBrother and hasSynonymy defined above among classes can be enriched"
 Tables from production database partial
 Authorized licensed use limited to Hochschule Heilbronn
" Downloaded on June , at UTC from IEEE Xplore"
 Figure shows schemas of four tables from production database
" Take Table well_info, Column well_name from Table well_info and Table output as examples to specify the process of ontology extraction, which is shown in Figure "
" information, that is, the two classes correspond to different attributes of one entity formed in the step of data filtering, the two classes evolve to a relation of hasBrother, parents of the two classes evolve to a relation of hasSynonymy"
 hasParenthasChildhasSynonymyhasBrotherhasBrotherhasBrotherhasParenthasChild 
 The steps of local ontology extraction
 The number in Figure corresponds to the rule number
 indicates that convert table names well_info and column name well_name into classes well_info
 means the relational schema of well_info and well_name is turned to a parentchild relationship in the local ontology
 is converting the two columns well_name and well_class from the same table into a hasBrother relation
 represents that the foreign key constraint of well_name
 Table well_info and table output is converted into a parentchild relationship
" Rule defines datatype properties of class well_name, while Rule extracts synonyms of well_name from synonym list and defines relationships between synonyms as hasSynonymy"
 Figure shows the local ontology after extracting schemas of production database and synonym list
 The arrows in Figure represent the relation of hasChild
 The classes with synonymy are circled in one node
 Local ontology of production database partial classes
" There are two steps from local ontologies to global local ontology and ontology, which are evolution of bination of to data storage local ontologies"
" Due characteristics of specialized databases in the petroleum engineering field, evolution of local ontology means if two classes with different parent nodes describe the same kind of When it es to bination of local ontologies, definite relationships must exist between the local ontologies"
 The global ontology can be built by mapping relevant local ontologies
" After analyzing schemas of different databases in the domain, the relationship between two local ontologies, which have the same class, can be established as a foreign key constraint"
 The parent node of the class with isPK property is mapped to the subclass of the class without isPK property
" For example, equip_info is a table of production database, and pump_parameters is a table of equipment database"
" The schemas of the two tables are as follows equip_info well_name, pumping_unit, pumping_rod…… pump_parameters pumping_unit, manufacturer, power……"
" Well_name is the primary key of table equip_info, and pumping_unit is the primary key of table pump_parameters"
" In our method, we regard pumping_unit as a foreign key of table equip_info linking to table pump_parameters"
" Thus, production ontology and equipment ontology are bined into one ontology"
 Figure shows the bined ontology
 Global ontology partial classes
" In Figure , the node output, production theevolutionary result of class output and class production"
" The relationship between output and production is evolved into hasSynonymy, and the relations of subclasses of output and production are hasBrother"
" is After local ontology evolution and local ontologies bination, local ontologies can be converted into a global ontology"
" We add some semantic constraints to strengthen the relations of terminology, and inference engine can reason and deduce the global ontology to reorganize the concepts using the constraints"
 Thus the implied semantic information can be got and valueadded services can be provided to users
 Semantic constraints are defined as follows
 Rule x OPDSOnto has Child y y OPDSOntohasSynonymy z x OPDSOnto has Child z Rule x OPDSOnto hasSynonymy y y OPDSOnto hasBrother z x OPDSOnto hasBrother z Authorized licensed use limited to Hochschule Heilbronn
" Downloaded on June , at UTC from IEEE Xplore"
 Rule x OPDSOnto hasSynonymy y y OPDSOnto hasParent z x OPDSOnto hasParent z C
" The process of semantic query Users and upper applications can submit query requests according to the global ontology, and OPSDS converts the requests to SPARQL statements to query the global ontology"
 Then the SPARQL statements can be rewrited as SQL statements to access underlying data sources
" Finally, the query results are returned in a uniform format after data cleaning and converting"
 Figure shows the process of semantic query
 Rewrite of SPARQL statements indicates that query requests based on the global ontology are converted into SQL statements to access underlying data sources
" Mapping from global ontology to data sources is divided into onetoone and onetomany, which include the following three types"
 Required data are from one table of a database
 Required data are from two or more tables of a database
 Query requests need to access more tables from multiple databases
 The process of semantic query implementation is as follows Step 
 Get the query request from service consumers to generate QueryGQG to query the global ontology
 QG is described by SPARQL
 Inference engine reasons names of classes and properties of QG to the names in the relevant ontologies
 Query resolver deposes QG into SubQueryLSQL to query each local ontology
" SQL {SQL , SQL , ……, SQLn , where n is the number of local ontologies"
 Query rewriter converts SQ L into SubQueryD SQDto query underlying database
" SQD {SQD , SQD , ……, SQDn , and SQD is described by SQL"
 Execute SQD and return SubResultD SRD 
" SRD {SRD , SRD , ……, SRDn "
" Result transformer cleans the query results SRDreferring to rules, and the normalized results can be got"
" Result biner bines the normalized results, and returns the final query result"
 QueryG SPARQL Result SubQueryL SPARQL SubQueryD SQL SubResultD SubResultD unified 
 shows the process of semantic query
 CONCLUSION AND FUTURE WORKintegration in specific area Semanticbased data is being a key research currently
" Build the global semantic data model for distributed, heterogeneous and plex semantic correlation data and provide prehensive and realtime data services using ontology technology are efficient and feasible"
" integration For data intensive industries, establishment of a global semantic data model based on domain ontology and realization to serve for upper of semanticbased data applications can get good results"
" Oil production engineering isa typical data intensive field, and OPSDS, which has realized data shared and reused, plays a key role in production practices"
" Through OPSDS, upper applications can directly access the underlying data resources and get the normalized data to beused for industrial production"
" The data of petroleum industrial applications show that our method can not only improve the production and recovery efficiency, but also save energy and reduce costs"
" Driven by application requirements, OPSDS connects production, study and research tightly, which is aneffective way to promote scientific and technological progress and prove that science and technology is the first productive force"
" In the future work, OPSDS will be used in more fields, and promoted to other oil areas like exploration and geology"
 Managing Scientific Data From Data Integration to Scientific WorkflowsJ
" GSA Today, Special Issue GSA Today, Special Issue"
" Carey M J, Onose N, Petropoulos M"
" Communications of the ACM, , "
" Dong B, Byna S, Wu K"
 SDS a framework for scientific data servicesCProceedings of the th Parallel Data Storage Workshop
" Halevy A, Rajaraman A, Ordille J"
 Data integration the teenage yearsCProceedings of the nd international conference on Very large data bases
" Smoot M E, Ono K, Ruscheinski J, et al"
 new features for data integration and network visualizationJ
" Bernstein P A, Haas L M"
 Information integration in the enterpriseJ
" Communications of the ACM, , "
 DIMP an interoperable solution for software integration and product data exchangeJ
" Enterprise Information Systems, , "
" Bender A, Poschlad A, Bozic S, et al"
 A serviceoriented framework for integration of domainspecific data models in scientific workflowsJ
" Procedia Computer Science, , "
" Das Sarma A, Fang L, Gupta N, et al"
 Finding related tablesCProceedings of the ACM SIGMOD International Conference on Management of Data
" Zdravković M, Panetto H, Trajanović M, et al"
 An approach for formalising the supply chain operationsJ
" Enterprise Information Systems, , "
 Authorized licensed use limited to Hochschule Heilbronn
" Downloaded on June , at UTC from IEEE Xplore"
 Information Sciences Contents lists available at SciVerse ScienceDirectInformation Sciencesj o u r n a l h o m e p a g e w w w 
" c o m l o c a t e i n sKernelMapping Remender system algorithmsMustansar Ali Ghazanfar a,, Adam PrügelBennett a, Sandor Szedmak ba School of Electronics and Computer Science, University of Southampton, Highﬁeld Campus, Southampton SO BJ, United Kingdomb Intelligent and Interactive Systems, University of Innsbruck, Innsbruck, Austriaa r t i c l ei n f oa b s t r a c tArticle historyReceived August Received in revised form March Accepted April Available online May KeywordsRemender systemsStructure learningLinear operationMaximum marginKernel"
 IntroductionRemender systems apply machine learning techniques for ﬁltering unseen informationand can predict whether a user would like a given item
" In this paper, we propose a newalgorithm that we call the KernelMapping Remender KMR, which uses a novelstructure learning technique"
" This paper makes the following contributions we showhow userbased and itembased versions of the KMR algorithm can be built userbased and itembased versions can be bined more information—features,genre, etc"
"—can be employed using kernels and how this affects the ﬁnal results and to make reliable remendations under sparse, coldstart, and long tail scenarios"
" Byextensive experimental results on ﬁve different datasets, we show that the proposedalgorithms outperform or give parable results to other stateoftheart algorithms"
"In this paper, we proposed a new class of kernelbased methods for solving the remendation problem that gives stateoftheart performance"
 The main idea is to ﬁnd a multilinear mapping between two vector spaces
" The ﬁrst vector spacemight, for example, have vectors encoding information about the items that we wish to rate, while the second vector spacemay contain a probability density function describing how a particular user will rate an item"
 Learning an appropriate mapping can be expressed as a quadratic optimisation problem
" As the problem involves a linear mapping, the solution to theoptimisation problem involves inner products in the two vector spaces"
 This allows us to use the kernel trick
 Directly solvingthe optimisation problem using quadratic programming would be too slow for most remendation datasets
" Instead, weﬁnd an approximate solution iteratively, following an idea ﬁrst developed by "
 This allows us to train the remenderin linear time
 The method described here is a specialisation of a general structure learning framework developed bySzedmak and used in for handling inplete data sources
The approach we have adopted is easily adapted to different sources of information
" We can, for example, use either rating information from other users or textual information about the items"
" Similarly, we are able to build either an itembasedor a userbased version of the algorithm"
" Because we have chosen to build a mapping to a space of functions approximatingthe probability density of the ratings, we have an intuitive interpretation of the remendations produced by the algorithm"
" This gives us ﬂexibility in how we make our ﬁnal remendation, which we can exploit to improve the ﬁnal prediction for different datasets"
"A main requirement of remender systems is to provide high quality predictions of the rating that a user would giveto an item, based on their previous rating history"
" Thus in testing remender systems, a dataset is used where somesets of ratings are treated as unseen while the other ratings are used for learning"
" To obtain accurate results, datasets Corresponding author"
 see front matter Elsevier Inc
 Information Sciences are usually selected with users that have made a relatively high number of ratings
" In real applications, however, the datasets are often highly skewed for example, a large number of users may have made only a small number of ratings, and alarge number of items may have received very few ratings"
 These are important scenarios in practical systems as givingreasonable remendations to new users can be crucial in attracting more users
" Similarly, giving a sensible rating to anew item may be necessary for those items to be taken up by the munity sufﬁciently to collect more ratings"
" Often,remender system algorithms that have been optimised to give good remendations on dense datasets performpoorly on these skewed datasets"
 We have generated highly skewed datasets to test our algorithm under these scenarios
"In particular we consider the newitem coldstart problem, the new user coldstart problem , and the long tail problem"
" We ﬁnd that the standard algorithm we developed performs poorly for these skewed datasets however, we showthat by using the ﬂexibility of our approach we can easily modify the algorithm so that it performs well under thesescenarios"
Remender systems have been a very active topic of research for around years
" This, in part, has been spurred onby the Netﬂix petition to improve the performance in terms of the root mean square error of a baseline algorithm by"
 One lesson to emerge from this was that a highly effective way to achieve a very high remendation performanceon a static dataset is to bine a large number of different algorithms
" Although such systems are interesting, they are notvery ﬂexible and may not be ideal algorithms for most real applications with rapidly changing users and items"
 Our algorithm relies on a single coherent method albeit with several variants that has not been designed for a speciﬁc dataset
 Wehave thus pared our approach with other general purpose remenders
" The best general purpose collaborative ﬁltering algorithms that we are aware of are by ,"
 These achieve a considerable gap in performance advantage over otheralgorithms
" The proposed algorithm achieves similar performance in terms of mean absolute error to these approaches,although it is outperformed by on a dataset with ,, ratings and by on a dataset of ,, ratings"
The proposed approach is however very different
" The other two approaches are based on matrix factorisation, although also uses kernel functions"
 There has been considerable work on developing matrix factorisation techniques whichare at the heart of many of the most petitive algorithms for this problem
" Part of the interest of the proposed algorithmis that it takes a very different viewpoint from the matrix factorisation approaches, yet still has very petitiveperformance"
The rest of the paper has been organised as follows In the next section we brieﬂy outline related work
 Section outlinesthe proposed algorithm using an itembased approach
 In Section we describe extensions to the basic algorithm
" Section presents details of the datasets we use for evaluation, the metrics we use and the procedure for tuning parameters of thealgorithm"
 This is followed in Section by a presentation of results from our experimental evaluation
 We conclude in Section 
 Some of the details and more extensive results are given in appendices
 Related workThere are two main types of remender systems collaborative ﬁltering and contentbased ﬁltering remender systems
" Collaborative ﬁltering CF remender systems ,,,,,, remend items by taking into account thetaste in terms of preferences of items of users, under the assumption that users will be interested in items that users similarto them have rated highly"
" Examples of these systems include GroupLens system , Ringo www"
 Collaborative ﬁltering systems are classiﬁed into two subcategories memorybased CF and modelbased CF
" Memorybased approaches make a prediction by taking into account the entire collection of previous items rated by a user, for example,the GroupLens remender systems "
" Modelbased approaches use rating patterns of users in the training set, groupusers into different classes, and use ratings of predeﬁned classes to generate remendations for an active user That is, the userfor whom the remendations are puted on a target item That is, the item a system wants to remend"
" Examples include itembased CF , Singular Value Deposition SVD based models ,,, matrix factorisation,,,,,,,,, nuclear norm regularisation , Bayesian networks , and clustering methods ,,"
"Contentbased ﬁltering remender systems ,,, remend items based on the content information of an item,under the assumption that users will like similar items to the ones they liked before"
" In these systems, an item of interest isdeﬁned by its associated features for instance, NewsWeeder , a newsgroup ﬁltering system uses the words of text asfeatures"
" Other wellknown types of remender systems include knowledgebased systems ,, Ontologybased systems, and hybrid systems ,"
"Hybrid remender systems have been proposed elsewhere ,,,,,,,,, which bine individual recommender systems to avoid certain limitations of individual remender systems"
" In the proposed approach we can addmore information about items in the forms of additional kernels, which can be thought of as bining collaborative ﬁltering with contentbased ﬁltering"
" A related approach has been proposed in , where the authors employed a uniﬁed approach for integrating the useritem ratings information with useritem attributes using kernels"
 They learned a predictionfunction using an online perceptron learning algorithm
" They claimed that adding more kernels increases the performance,which is in contrast with our ﬁndings"
" It might be due to the reasons that they used very simple kernels, such as correlation,and identity however, we used polynomial kernels, which are in turnare addition of correlation, identity, etc"
" Information Sciences In , the authors proposed a structured learning algorithm for learning from inplete dataset"
" The idea of the structure learning has been used in , where the authors employed it for enzyme prediction"
 We show how the structure learning approach can also be used to solve the remender system problem effectively
Remendations can be presented to an active user in the following two different ways by predicting ratings ofitems a user has not seen before and by constructing a list of items ordered by their preferences
" In this paper, we focuson both of them"
" KernelMapping RemenderA remender system consists of two basic entities users and items, where users provide their opinions ratings aboutitems"
 We denote these users by U fu u 
" uMg, where the number of people using the system is jUj M, and denotethe set of items being remended by I fi i "
" The users will have rated some, but not all, of theitems"
" We denote these ratings by riuji u D, where D I U is the set of useritem pairs that have been rated"
 We denote the total number of ratings made by jDj T
" Typically each user rates only a small number of the possible items, so thatjDj T jI Uj N M"
 It is not unusual in practical systems to have TN M u 
 The set of possible ratings madeby the users can be thought of as elements of an M N rating matrix R
" We denote the items for which there are ratings byuser u as Du, and the users who have rated an item i by Di"
" The task is to create a remendation algorithm that predicts anunseen rating riu, That is, for i u R D"
In this section we describe an itembased remender
 In the next section we show how we can adapt the approach to auserbased remender
 To perform the remendation task we consider building the additive and multiplicative modelsfor the residual ratings
" The residue in the additive model is given byriu riu ri ru rwhere ri, ru and r are respectively the mean rating for the item, of the user, and the overall meanri jDijXuDiriuru riur jDujXiDujDjXuDriuThe multiplicative model can be expressed as followsriu riurriru"
" Itembased KMRwhere r, ri and ru are the geometric means for all the ratings, the ratings for item i, and the rating of user u, respectively"
" Wefound the additive model to be marginally better than the multiplicative one, and hence this work is based on the additivemodel"
We use a technique developed by Szedmak and coworkers for learning structured data 
 In the following we outlinehow this approach is adapted for solving the collaborative ﬁltering problem
 We assume that we have some informationabout the items which we denote by qi
" This may, for example, be the set of ratings riu for u Di, or it could be text describing the item i"
 We map the information to some vector qi in some extended feature Hilbert space
" Similarly, we mapthe rating residues, riu, to ‘vectors’ in some other Hilbert space"
" In this paper, we consider these objects to lie in the function space LR"
" In particular we represent each residual riu, by a normal distribution with mean riu and variance r"
" Thatis,wriu N xjriu rThe motivation of this choice is to model possible errors in the rating either due to the discretisation of the rating scale or thevariability in assigning a rating e"
 due to the mood of the user on the day they made the rating
The method developed by Szedmak is to seek a linear mapping between these two spaces which can be used for makingpredictions
" More speciﬁcally, in our application, we look for a linear mapping Wu from the space of vectors to the space ofw vectors refer to "
 We will use the mapping Wuqj to make a prediction for the rating of a new item j by the user u
To learn the mappings Wu we will minimise the Frobenius norm of Wu subject to the constraintshwriu Wuqii P fiwhere fi P is a slack variable and where we have a constraint for each pair i u D
 This ensures that Wuqj is alignedwith wriu for the ratings in the training set
 We can write the training problem for the mappings Wu as a quadratic programming problemM
 Information Sciences minimiseXuUkWuk CXfiiIwith respect to Wu u U fi i Isubject tohwriu Wuqii P fifi P i I u DiNote that minimisation will be achieved when the vectors Wuqi are as uniformly aligned as possible with the vector wriu
Having learned the mappings Wu we can then make predictions for a new item j using Wuqj
 This outputs a functionwhich informally we can think of as an estimate for the probability density of the residue rju
" However, Wuqj does notneed to be, and typically is not, positive everywhere or normalised"
" Thus, it is not itself a probability density"
 We discuss laterdifferent methods for interpreting Wuqj
"To solve this constrained optimisation problem, we deﬁne the LagrangianL XuUkWuk CXfi Xaiuhwriu Wuqii fi iIiuDXiIkifiwhere aiu P are Lagrange multipliers introduced to ensure that hwriu Wuqii P fi and ki P are Lagrange multipliers introduced to ensure that fi P "
 The optimum mapping is found by solvingsubject to the constraints that aiu P for all i u D and ki P for all i I 
" For a general linear mapping, Wu, we have thatwhere is the tensorproduct of the two vectors"
" This is clearly the case when the Hilbert spaces are ﬁnite dimensions sothat the mapping Wu can be represented by a matrix, but this can be extended for linear mappings between more generalHilbert spaces"
" Using this result we ﬁndThat is, the Lagrangian is minimised with respect to Wu when Wu Pfi we ﬁndiDiaiuwriu qi"
 Taking derivatives with respect tominfWugffigmaxfaiugfkigLhwriu Wuqii wriu qiWuLWuLfiXuDi Wu aiuwriu qiXiDu C aiu kiXuDiaiu C ki CSetting these derivatives to we ﬁnd that the Lagrangian is maximised with respect to fi whenwhere the inequality arises because ki P 
After substituting back the expressions containing only the Lagrange multipliers into the Lagrangian we obtain the dualproblem of which is a maximisation problem with respect to the variables aiuf a aiuaiuhwriu wriuihqi qi i XXuUiiDuXaiuiuDsubject to the constraint that a Za whereZa aji IXuDiaiu C i u D aiu P Krriu riu hwriu wriuiK qqi qi hqi qi ithen we can write fa asf a aiu aiu Krriu riu K qqi qi XXuUiiDuXaiuiuDWe are now in the position where we can apply the usual kernel trick
 Deﬁning the kernel functionswhere we are free to choose any pair of positive deﬁnite kernel functions
" With our choice of mapping the rating residual, r,to wr N xjr r, we note thatM"
 Information Sciences Krr r hwr wri N r rjprwhich is inexpensive to pute
" We could build more plex kernels for Krr r, by mapping wr into another extendedfeature space, although we would then lose the interpretation of Wuqi as an approximation to the density function for riu"
" Learning the Lagrange multipliersFor largescale remender systems, solving this quadratic programming problem using a general quadratic programming solver would be impractical due to the large number of data points"
" However, we can ﬁnd an approximate solutioniteratively using the conditional gradient method"
 To understand this method it is helpful to write fa in matrix formf a aTMa bTawith a Za
 We obtain a series of approximations at for the optimal parameters starting from some initial guessa Za
 At each step we use a linear approximation for fa about the current position atf a f at a f at a atf atWe pute the next approximation using two stages
" We ﬁrst solve the linear programming problema argmaxaZaf at a argmaxaZaaTMat b constWe then ﬁnd the new approximation at to beat at sa atwhere we choose s to bes argmaxf at sa ats½smax b MatTa ata atTMa atNote that we can pute the unconstrained maximum for sf a XXaiugiu constuDiiIf ataiu XiDugiu atiu Krriu riu K qqi qi maximiseaiugiuXuDiXuDisubject toaiu C and u Di aiu P By truncating smax if necessary to ensure that it lies in the interval , we can ensure that at is the maximum value of aalong the line segment from at to a"
" Since this segment includes the current point, at, we are guaranteed that no stepdecreases the objective function"
We note that in the linear programming problem we have an objective function of the formwhich decouples for every set of Lagrange multipliers Ai fauiju Dig
 The linear constraints Za also decouple into a set ofconstraints for each set of Lagrange multipliers Ai
" Thus, the linear programming problem bees a series of linear programming problems for each i IThis linear programming problem is trivial to solve see "
 If giu has positive ponents then a maximum will occurwhen we set aiu C where giu P giu for all u Di and aiu otherwise
Since ﬁnding the largest ponent of giu can be puted in linear time we are able to perform one step of the optimisation procedure in HjDj time
 Note that at each step we have to pute a vector matrix products involving the matrix M
These products involve the sum over all u U and the sum over all i Du
" However jDuj does not grow with the number ofusers, thus these products can also be puted efﬁciently"
" Note, that solving the optimisation problem this way makes itfeasible to obtain remendation for databases with up to million ratings"
" Predicting unseen ratingsTo make a prediction for the rating riu where i u R D, we estimate the residue riu riu ri ru r using the functionM"
 Information Sciences 
 Schematic showing the linear programming problem
 The feasible region is shown as a shaded triangle
 The vector gi shows the direction of theobjective function
 The maximum occurs at the vertex corresponding to the largest ponent of gi
 Schematic showing the aim of the algorithm
" Information, qi in this case a rating vector about an item i, is ﬁrst mapped to a vector in an extendedfeature space qi"
" We then try to ﬁnd the best linear mapping, W, for user u, to the ‘vector’, wriu , describing the residual"
piur hwr Wuqii aiuKrr riuK qqi qi XiDuriu argmaxpiurrwhere wr N r r
 We have a choice in how to obtain a single prediction from this function
 Our standard predictor will beto ﬁnd the maximum argument of piurThis works well when we have a sufﬁcient number of ratings for the user and the item
 However as we will see it givespoor predictions in scenarios where we have a small amount of training data
 Recall that we argued earlier Wuqi can beregarded as an approximation for the probability density of riu
" It will not generally be positive everywhere, but by removingthe negative part of the function we can treat the remaining function as a probability density"
" In this case, we can considerthe mean, mode, or median as approximations for the most likely value of riu"
" Under conditions where we lack sufﬁcient datawe ﬁnd that using a bination of the mean, mode and median together with the standard max prediction gives a considerable improvement in accuracy"
" In particular, we consider a predictorrM wmaxrmax wmeanrmean wmodermode wmedianrmedianwhere rm with m {max, mean, mode, median are the standard predictors and the predictors using the mean, mode andmedian, while wm are a set of weights that are learnt from a validation set"
 We consider the weights to be constrained sothat wm P and they sum to 
 In the results shown later we denote those results obtained using this predictor by the superscript M
" A small scale exampleSuppose a remender system has four users That is, U fu, u, u, u and three items That is, I fi, i, i"
" The information about each item is a column vector of the useritem rating matrix, shown in Table "
" The users’, items’, and overall averages areru ri ru ri ru ri ru r M"
 Information Sciences Table Example a subset of the useritem rating matrix in a movie remender system
 We have four users rows and three moviescolumns
" The case, where a user has not rated a particular movie is shown by symbol"
" The rating scale, consisting of integervalues between and , captures the extreme like and extreme dislike behaviour of a user"
 The rating we want to predict isshown by ‘‘’’ symbol
iiiAfter applying the additive model Eq
" , the useritem rating matrix can be represented in the residual form as shown inTable "
" The input feature kernel, Kq, using the polyGaussian kernel refer to Section "
"We can pute the residual kernel, Kr, based on the inner products between Gaussian densities functions with expectedvalues r and r, and sharing the mon standard deviation r"
K q Krr r hwr wri p errrr pAssume that r 
" then we haveKruKruK residual KruKruwhereKru Kru Kru Kru The optimal values for the design variables, a, are learnt using the conditional gradient method, and are shown in Table "
" To make aAfter learning the a parameters, the mapping Wu, can be deﬁned for each user recall Wu Pprediction for the rating riu, where i u R DiDiWuqi aiuwriu qi qi aiuwriuK qqi qiXiDuXiDuIn this case, we have u u and i i, soWu qi aiu wriu K qqi qi aiu wriu K qqi qi wriu wriu wriu wriu N riu r N riu rwhich is an unnormalised probability density function of mixture of two Gaussians"
 The optimal rating then can be derivedbyTable Example the matrices of rating residues riu
" Information Sciences Table The optimal values of design variable, a, for each user and item"
"piu r hwr Wu qi i arg maxhwr wriu wriu i arg maxhKrr riu Krr riu i arg maxhN rjriu pr N rjriu prrirrTaking the optimum solution refer to , riu , the prediction for the residual is "
" Hence, user u would rate item i withrating of riu ri ru r "
 Extensions to the basic algorithm
 Userbased KMRIn this section we describe extensions to the basic algorithm which are relevant to practical remender systems
Depending on the dataset characteristics e
" number of items rated by the active user, number of users which have ratedthe target item, etc"
 different models can be trained along the rows or columns of the data matrix
" A related algorithm isproposed, which solves the problem from the user point of view, hence it is named as the userbased KMR KMRub"
" To perform a userbased remendation, we use information qu about users u and try to ﬁnd a linear mapping Wi to align someextended feature vectors qu to the residue vector wriu"
 The derivation is identical to that for the itembased remender when we interchange the subscripts i and u
 Combining user and itembased KMRUser and itembased versions provide plementary roles in generating predictions as they focus on different types ofiur be the predictions made by the user and itembased versions respectively
 Werelationships in a dataset
 Let pubhave considered three different ways of bining user and itembased predictions
"iu r and pib Using the simple linear bination In this approach, the user and itembased versions are linearly bined, where theparameter q is learned from a validation set"
piur qpubiu r qpibiurWe denote the resulting hybrid remender system by KMRLinearHybrid
" Switching on number of ratings Here, we take into account the information about user and item proﬁles"
" The rationalebehind this approach is the intuition that if we have a large number of ratings for an item pared to the number ofratings made by the active user, then the userbased version is likely to give better results than the itembased versionand vice versa"
" Rather than using the raw number of ratings, we normalise by the number of ratings given by the poweruser, up That is, the user that has rated the most number of items and by the power item iP That is, the item with the most number of ratings"
 Plotting the probability density function of mixture of two Gaussians with r f g
 The optimal solution is found to be 
 Information Sciences Table Characteristics of the datasets used in this work
" FT, SML, ML, ML, and NF represent the FilmTrust, MovieLens k, MovieLens M, MovieLens M, andNetﬂix dataset respectively"
 Average rating represents the average rating given by all users in the dataset
CharacteristicsNumber of usersNumber of moviesNumber of ratingsRating scaleSparsityMax number of ratings from a userMax number of ratings for a movieAverage ratingDatasetFT 
piur iu r pubiur pibjIup j hCntifjUijjUip j jIujotherwiseSML 
We denote the resulting hybrid remender system by KMRCntHybrid
 Switching on uncertainty in prediction Here we use a different strategy for switching between the user and itembasedpredictors
 We try to estimate the uncertainty in the prediction by examining the ‘‘variance’’ in Wuqi and Wiqu
"Since they are not real probability distributions, we must ﬁrst exclude the regions where the functions go negativeand normalise the output so that we can treat them as densities and pute their variance"
" We denote the varianceby Varub and Varib for the user and itembased versions, respectively"
 We then switch the remendation according topiur iu r pubiur pibif Varub Varib hVarotherwiseWe denote the resulting hybrid remender system by KMRVarHybrid
 Combining kernelsIn many applications there are multiple sources of information that can be used to make a remendation
 We can easily acmodate different sources of information by bining kernels
 To illustrate this we will test our algorithm on datasets consisting of ﬁlm ratings where we have three types of information available refer to Section 
 The ratings of other users from which we can construct a kernel Krat ‘‘Demographic’’ information obtained from genre about the ﬁlms from which we can construct a kernel Kdemo ‘‘Feature’’ information obtained from a textual description of the ﬁlms from which we construct a kernel Kfeat
"These kernels can be bined linearlyK bratK rat bdemoK demo bfeatK featwhere the parameters brat, bdemo and bfeat brat bdemo can be tuned by measuring the generalisation performance on avalidation set"
 This way of bining kernels can be viewed as a concatenation of the feature vectors p ratbratpbdemodemoqbfeatfeatp rat bratpbdemodemo qbfeatfeatwhere represents the direct sum
 Alternatively we can bine the kernels nonlinearlywhere the denotes the pointwise product of the kernel matrices
 This corresponds to taking a tensor product of the featurevectorsK K rat K demo K feat rat demo feat
 Experimental setupalgorithms
In this section we describe the datasets we used and the setup of the experiments for benchmarking the proposed
 Information Sciences As is mon in the ﬁeld of remender systems we used data from ﬁlm remendation sites to test the proposedalgorithm
 These provide some of the largest available datasets allowing us to test the scaling performance of the algorithm
"In addition, as these datasets are very monly used in the literature, it allows us to benchmark our algorithm against petitor algorithms"
 We used the following datasets FilmTrust denoted by FT obtained by crawling on th March the FilmTrust website httptrust
 Only users and movies having more than ﬁve ratings were used
" This has been used before in ,"
" MovieLens which we split into three groups Small MovieLens denoted by SML with million rating dataset denoted by ML million rating dataset denoted by MLThis has been widely used ,,,,"
" Random subsample of , users from the Netﬂix dataset denoted by NF"
 This dataset has been very widely used e
"see ,,, in part because of the prize offered for achieving a level of improvement over a benchmark"
 We have notattempted to pare our algorithm against the stateoftheart Netﬂix algorithms for two reasons
" Firstly they havebeen highly tuned to that particular dataset, while we have concentrated on developing a general purpose remendation algorithm"
" Secondly, the full Netﬂix dataset is so large that it is difﬁcult to process on a normal desktop machinewithout spending signiﬁcant time on optimising memory management"
The characteristics of these datasets are given in Table 
 Feature extraction and selectionTo test the remendation algorithm using textual information we also obtained information about each movie
 Thiswas used to construct two additional information vectors a ‘‘feature’’ vector and a ‘‘demographic’’ vector
 We downloadedinformation about each movie in the MovieLens SML dataset and FilmTrust dataset from IMDB
" For the ML dataset, weused the tags and genre information that is provided with this dataset"
" After stop word removal and stemming, we constructed a vector of keywords, tags, directors, actorsactresses, producers, writers, and user reviews given to a movie in IMDB"
"We used TFIDF Term FrequencyInverse Document Frequency approach for determining the weights of words in a documentThat is, movie"
" The document frequency DF thresholding feature selection technique was used to reduce the feature space byeliminating useless noise words having little or no discriminating power in a classiﬁer, or having low signaltonoise ratio"
"To construct the demographic vector, we take the genre information about movies as employed in , with the exception that we used the hierarchy of genre as shown in "
" To determine the weight of a genre in the genre vector, we used asimple weighting scheme as employed in QuickStep, an Ontologybased remender system "
" To pute an innerproduct between demographic vectors the immediate super class is assigned of a subject’s value, the next super classis assigned , and so on until the most general subject in the Ontology is reached"
" By making a hierarchy of the genreand assigning different weights to sub and superclasses, we hope to enrich an item’s proﬁle"
" MetricsIn the majority of the paper, we have used the Mean Absolute Error MAE as our measure of performance as this is themost monly used measure and de facto standard for benchmarking remender systems"
" In practice, however, remender systems are monly used for helping users in selecting high quality items"
" Thus, arguably, a more appropriatemeasure of accuracy is to study an algorithm’s ability to predict highly rated items"
 There are a number of metrics thatare more speciﬁcally designed to measure how well a remender classiﬁes good quality relevant items
 These includethe ROCsensitivity and F measure
 The details of all these metrics are given in Appendix B
" Furthermore, we also give tablesof results for these last two measures in that Appendix"
 Evaluation methodologyWe performed fold cross validation by randomly dividing the dataset into a test and training set and reported the average results
 We further subdivided the training set into a test and training set for measuring the sensitivity of the parameters
"For learning the parameters, we conducted fold cross validation on the training set"
" We matched the movie titles, provided by the SML and FT dataset, against the titles in the IMDB www"
 We used Google’s stop word list www
 We used Porter Stemming algorithm for stemming
 Information Sciences 
 Hierarchy of genres based on 
 All the super classes of a genre get a share when a genre receives some interest
" For instance if a rated movie fallsinto ‘‘crime’’ genre, then the ‘‘crime’’ subject will get a weight of q, the immediate super class, ‘‘Thriller’’ will get a weight of q and the next super class‘‘Unknown’’ will get a weight of q"
 Learning system parametersThere are a number of parameters that need to be learned
" Below, we discuss the training of these parameters"
" Number of iterationsThe algorithm we develop uses an iterative technique to learn the Lagrange multipliers, a"
 As we increase the number ofiterations the mean absolute error improves
 The speed of convergence will depend on the dataset and the type of information we are using e
 userbased or itembased
" and , show the mean absolute error and the time taken to learn theLagrange multipliers versus the number of iterations for the FT and SML datasets, respectively"
"We note that for the FT dataset, the performance of the itembased version suffers badly when the number of iterationsare very small"
" However, the performance of the userbased version is quite good even after a few iterations"
" Hence, if onehas a constraint on the time required to build the model, then it is better to switch to the userbased version rather than theitembased version for the dataset"
" In contrast, in the SML dataset, the convergence of all the methods was relatively quick"
The convergence clearly depends on the number of usersitems and the useritem proﬁle length e
" rating proﬁle, featureproﬁle length, etc"
 It is not obvious a priori how many iterations are needed to get good rating predictions
" Based on ourinitial experiments, we chose the number of iterations to be for the SML dataset, for FT, for ML, and forML and NF"
"We trained linear, polynomial, and polyGaussian kernels and chose the one giving the most accurate results"
" The optimal kernel parametersnomial kernel is of the formKx y hx yi RdFor the ratingbased version, the best polynomial kernel parameters d, R are found to be, for userbased and itembasedversions respectively , "
" for the SML dataset , "
" for the FT dataset and , "
 Information Sciences Number of Iterations FT DataSetNumber of Iterations FT DataSet 
 The number of iterations and time required to converge the proposed algorithms for the FT dataset
Number of Iterations SML DataSetUBIBUBIBUBIBFeatureDemoUBIBFeatureDemo rorrlE etuosbA naeMEAM sm emTirorrlE etuosbA naeMEAM sm emTi
Number of Iterations SML DataSet 
 The number of iterations and time required to converge the proposed algorithms for the SML dataset
" For the featurebased version, the best polynomial kernel parameters were found to be , "
" for theSML dataset and , "
"We did not tune the parameters for the ML and NF datasets, as it was putationally expensive"
" for user and itembased versions for both datasets and , "
 for the featurebased version for the ML dataset
"For the demographicbased version, we found the best kernel was the polyGaussian kernel which is a simple extensionof the Gaussian one given byM"
" Information Sciences Kx y exp kx ykqswhere the best parameters q, s were found to be "
 for the SML dataset and 
 Again we didnot tune parameters for ML dataset and they were ﬁxed to 
"The other parameter in setting up the kernel was the standard deviation, r, used in mapping wr N xjr r"
" We experimented with learning this parameter for each user, but found this putationally very expensive"
 We then tried groupingthe users according to the variance in their ratings into groups and tuned r for each group
" Although this gave improvedperformance, it was not found to be statistically signiﬁcant"
 We therefore just used a single parameter r which we tunedusing a validation set
"The parameter C that punishes the slack variables in the Lagrange formulation was ﬁxed to , after initial experimentation"
" In the extension of the basic remender there are other parameters, such as the weights for bining kernels andvarious thresholds for switching between remenders"
 The tuning of these parameters are described in Appendix A
" ResultsIn this section, we describe the results obtained from our experiments"
" In the tables we have denoted the proposed algorithm by KMRsupsub, where the subscript denotes the variant of the algorithm and the occasional superscript describes the variantin more detail where necessary"
" The main variants are itembased ib, userbased ub, featurebased F that use feature vectors rather than rating vectors, demographic D that use demographic vectors rather than rating vectors, and hybrid Hybridthat uses a mixture of userbased and itembased predictions"
 For the hybrid algorithm we use the superscript to denote thedifferent mechanisms for bining userbased and itembased predictions
" When we use binations of information, e"
"itembased ratings and features, we use KMRibF to denote the case when we add the kernels and KMRibF when we multiplythe kernels"
" Finally, for the datasets with a limited amount of ratings, instead of using the standard approach to predicting anew rating, we bined the standard approach value of r that maximises the predictor pr with the mean, mode and median of Wuqi for the itembased approach"
 We denote this version of the algorithm with a superscript M
We pare the proposed algorithms with other algorithms described in the literature
" We chose several other algorithms based on the number of citations given in the literature the algorithm classiﬁcation space That is, memorybased ormodelbased approaches and whether the algorithm claims to give stateoftheart results"
" Direct parisonWe pared the proposed algorithms with three different algorithms userbased collaborative Filtering CF with Default Voting DV proposed in which provides a useful baseline for paring algorithms, itembased CF proposed in shown by ItemBased CF, and a SVD based approach proposed in shown by SVD"
" To provide as fair a parisonas possible, we tuned all parameters of the algorithms"
Table shows that the KMRbased algorithms outperform all the aforementioned algorithms
" The percentage decrease inerror of KMRib, KMRub, and KMRVarHybrid over the baseline approach is found to be "
 for the SML dataset 
 for the MLdataset and 
 The ROCsensitivity and F measure on the same dataset are shownin Tables A
" Indirect parisonIn this section, we pare our results with other algorithms indirectly, That is, we take the result from the respective paperswithout reimplementing them, which might make the parison less than ideal"
 We conducted the weak generalisationtest procedures of using the AllButOne protocol—for each user in the training set a single rating is withheld for the testset
" We averaged the results over the three random traintest splits as used in ,,"
Table A parison of the proposed algorithm with others in terms of MAE
 The average with the respective standard deviation of results over fold is shown
 Thebest results are shown in bold font
AlgorithmUserbased CFItembased CFHybrid CFSVDKMRibKMRubKMRVarHybridBest MAESML
 Information Sciences Table A parison of different algorithms in terms of NMAE Normalised MAE for theML dataset
" The proposed algorithms outperform URP , Attitude ,MatchBox , MMMF , ImputedSVD , and Item "
 They give theparable results to EMMF and NLMF 
 Our results and the best resultsare shown in bold font
AlgorithmURPAttitudeMatchBoxImputedSVDMMMFItemEMMFNLMF LinearNLMF RBFKMRibKMRubKMRVarHybridAlgorithmNLMFMFTIBKMRibKMRubKMRVarHybridNMAE
Table A parison of different algorithms in terms of RMSE for the ML dataset
NLMF represents the nonlinear matrix factorisation technique as proposed in and M FTIB represents the Mixed Membership Matrix factorisation modelas proposed in 
 Our results and the best results are shown in bold font
A parison in terms of Normalised MAE NMAE—see Appendix B—of the algorithms is given in Table 
" In Table , URPrepresents the algorithm proposed in , Attitude represents the algorithm proposed in , MatchBox is proposed in ,MMMF represents the maximum margin matrix factorisation algorithm proposed in , ImputedSVD is proposed in , Itemis proposed in , EMMF represents the ensemble maximum margin matrix factorisation technique proposed in , andNLMF represents the nonlinear matrix factorisation technique with linear and RBF versions as proposed in "
Table shows that the NLMF and EMMF perform better than the rest
 The proposed hybrid algorithm gives slightlypoorer results to them with NMAE 
" It is worth mentioning that the EMMF is an ensemble of about predictors,which makes this algorithm unattractive"
" From this table, we may conclude that the proposed algorithm is parable to thestateoftheart algorithm for the MovieLens M dataset"
"To the best of our knowledge, the best results for the MovieLens M dataset that have been reported in the literature arethose proposed in ,"
 They claimed their proposed algorithm gives RMSE accuracy of 
 We followed their experimental setup and the results have been shown in Table 
 Table shows that the proposed algorithms outperform ’s results
 The percentage improvement is found to be 
 in the case of KMRVarHybrib
 TheMFTIB algorithm gave the best results outperforming our best algorithm KMRVarHybrid with 
" Actuallythe MFTIB integrates two plementary algorithms—discrete mixed membership modelling and continuous latent factormodelling That is, matrix factorisation—into a mon framework using the Bayesian approach, which illustrates the power ofcarefully bining different algorithms"
"Unfortunately, no NMAE or MAE was provided for MFTIB technique over Movielens M dataset, which makes itharder to pare different algorithms’ results with MFTIB"
" Considering these results, we conclude that the proposed approach appears to be petitive with the current stateoftheart approaches"
 Combining different kernelsAs discussed in Section 
", there can be different sources of information that can be used for making remendations"
"The proposed framework allows these different sources to be exploited by bining different kernels built from different The authors did not provide any numeric value, only a graph is presented showing the minimum value approximately to "
 Information Sciences Table Comparing the performance found with different binations of kernel for the SML dataset
 The average with the respective standard deviation of results overfold is shown
 The best results are shown in bold font
AlgorithmKMRibKMRDKMRFKMRibFDKMRibFKMRibDKMRFDKMRibFDKMRibFKMRibDKMRFDMAE
" In particular, we consider the rating information, feature information, and demographics information asdescribed in Section "
Table shows the performance of different binations of kernels for the SML dataset
" We have shown not only theMean Absolute Error MAE, but also a number of measures of the ability to classify ﬁlms as either highly rated or poorlyrated refer to Appendix B for details"
" We observe reasonable performance using just rating information, demographicinformation and feature information"
" Interestingly, for this dataset, bining kernels does not give signiﬁcantly betterperformance than using a kernel based on a single source of information"
" A plausible explanation of this observation is thatour error rates are close to the optimum that can be achieved there is a limit on the performance of any system due to theﬁckleness of the users making the ratings or, at least, we are close to the optimum given the way we have represented theproblem"
" On other datasets where, for example, ratings for some users are very sparse, demographic and feature informationcan be much more signiﬁcant"
 The other striking feature of Table is that multiplying kernels together seems to be moresuccessful than adding different kernels
Similar results not shown were observed in the case of FT and ML datasets
" We also attempted to linearly bine thepredictions from different kernels, but again this gave no improvement"
 Combining the user and itembased versionsThe methods of bining the user and itembased versions mentioned in Section 
 did not give any signiﬁcantimprovement over the individual results for the whole dataset
" To check the performance for imbalanced datasets, we randomly selected users and movies from the SML dataset, and users and movies from the FT dataset and randomly withheld x of their ratings"
" We checked the performance for two cases for Case , the value of x was chosenuniformly at random to lie between and That is, x , , whereas for Case , the value of x lies between and That is, x , "
 The latter case creates a relatively imbalanced subset of the dataset as pared to the former one
"Table shows the performance of userbased, itembased, and different methods used to bine the individual versions"
 We use the average of user and itembased versions as a baseline
 We observe that linearly bining the individualremender systems does not give signiﬁcant improvement over the baseline and the same is true for the second methoddiscussed in Appendix 
" However, KMRVarHybrid does signiﬁcantly improve the performance, with pvalue in the case of pairt test pared with the baseline remender found to be less than for both datasets"
 Similar results were observedfor other datasets as well
" What is evident from Table is that user and itembased versions of the algorithm are plementary and can improve the performance, if bined in a systematic way, for the imbalanced dataset"
Table Combining the userbased and itembased versions under imbalanced datasets
 The Case produces a relatively sparse subset of the dataset pared to Case
 The best results are shown in bold font
ApproachKMRibKMRubKMRib KMRubKMRLinearHybridKMRCntKMRVarHybridHybridMAECase FT
 Information Sciences 
" Sparse, skewed, and imbalanced datasetsIn practical applications remender systems often have access to limited and highly skewed information"
 Examples ofthese are
New user coldstart scenario where new users have relatively few ratings
Newitem coldstart scenario where new items have relatively few ratings
Long tail scenario where the majority of items have only a few ratings
Imbalanced sparse datasets where the majority of usersitems have only a few ratings
In the datasets that we have used so far our test set consists of randomly chosen ratings and these are overwhelmingly inthe dense region of the rating matrix
" That is, the users that we tested typically have rated many items and the items havebeen rated by many users"
" Thus, the results we have described so far are not strongly inﬂuenced by problems of limited andskewed information"
" However, these problems are often vital for a remender system to prosper"
" For example, to attractnew users it is highly beneﬁcial to be able to give them good quality remendations before they have made many ratings"
"Similarly, to introduce new items into the system it is useful to make sensible remendations even if the item has onlygained a few ratings"
"Table Comparing MAE observed in different approaches under new user coldstart scenario, for the SML dataset"
" The superﬁx M represents the correspondingversion of the KMR algorithm, where we take into account the max, mean, mode, and median of the output probability distribution"
 The best results are shownin bold font
ApproachKMRibKMRubKMRDKMRFKMRMibKMRMubKMRMFKMRMDKMRMibFBest MAEMAE
Rating WeightMean WeightMode WeightMedian Weightinoitcderp lanif eht ni sroitcderp liaudvdnii eht fo sthgeWi
Number of Ratings SML DataSet 
 Weight learning over the validation set for the new user coldstart problem SML dataset
 ‘‘Number of Ratings’’ represents the number of ratingsgiven by an active user in the training set
 Information Sciences We have tested the four scenarios outlined above by modifying the datasets we have been using to exaggerate the sparseness or skewness of the data
 We found that in all cases the standard predictor that we have been using up to now gives verypoor performance
" However, we could very substantially improve the performance by bining the standard predictorwith predictions using the mean, median and mode of Wuqi as described in Section "
 In the tables shown belowwe denote the modiﬁed predictor with a superscript M
We concentrate on the newuser coldstart scenario as the results are representative of all four scenarios
 The only majordifference is in the newitem coldstart scenario where the featurebased and demographicbased remenders also perform well as they are less inﬂuenced by a lack of ratings
" Results for the new item coldstart, long tail, and sparse data scenarios are given in Appendix C"
" New user coldstart scenarioTo test the performance of the proposed algorithms under the new user coldstart scenario, we selected randomusers, and kept their number of ratings in the training set to , , , , and "
" Keeping the number of ratings less than ensures that a user is new, and it captures well the new user coldstart problem"
" The corresponding MAE, representedby MAE, MAE, MAE, MAE, and MAE is shown in Table "
 Using the standard predictor provides very poor performance
" We can substantially improve the performance by bining the standard predictor with predictions using themean, median and mode of Wuqi as described in Section "
"Recall that we learn the weights for bining the standard predictor with the predictor using the mean, mode and median"
 The value of the weights depend on the dataset
 shows how the weights that have been learned change in the newuser coldstart scenario as we increase the number of ratings in the training set
 The new user coldstart scenario is taken asan example similar results were observed in both the new item coldstart and long tail scenarios
 The xaxis shows thenumber of ratings given by users selected as coldstart users and the yaxis shows the weights associated with differentpredictors
" We observe that the contribution of the mode, mean, and median predictors decreases with the increase inthe number of ratings, and ﬁnally bee zero when the maximum number of ratings are available, whereas, the contribution of the standard ratingsbased predictor increases with the increase in the number of ratings, and bees when themaximum number of ratings are available"
 Conclusion and future workRemender systems is a major research area in machine learning and data mining
" A number of approaches have beenproposed to solve the remender system problem including contentbased ﬁltering, Ontologybased approaches, supervised classiﬁcation techniques, unsupervised clustering techniques, memorybased collaborative ﬁltering, modelbased approaches spanning a number of algorithms including singular value deposition, matrix factorisation techniques, andprincipal ponent analysis"
" All these algorithms suffer from potential problems such as accuracy, scalability, sparsityand imbalanced dataset problems, coldstart problems, and long tail problems in one way or the other"
" Against this background, we propose a new class of kernelbased remendation algorithms that give stateoftheart performance andeliminates the recorded problems with the remender systems making the remendation generation techniquesapplicable to a wider range of practical situations and realworld scenarios"
"The proposed algorithm is petitive with what we believe to be the remender with the best performance proposedby ,"
 Interestingly both the proposed algorithm and the remender proposed in use kernelbased methodsthough in a very different way
" Although kernelbased techniques are known to give excellent performance, remendersystems are challenging because of the size of the datasets"
 By carefully choosing the constraints we have been able to createa kernelbased learning machine that can be trained in linear time in the number of data points
"The algorithm we have developed is very ﬂexible, thus we can easily adapt it so that it is either userbased or itembased"
In addition it can use other information such as textbased features and these features can be easily bined
 The best algorithm on the large datasets switches between the userbased and itembased information depending on the reliability of thepredictions as measured by the spread in the prediction of the algorithms
One interesting feature of the proposed approach is that we map the residues in the ratings onto a density function whichencodes the uncertainty in the residue
 For unseen residues we have interpreted the mapping Wuqi as an approximationto a density function for the residue
" Even though this function is not itself a density function it bees negative in someregions and is not normalised, nevertheless, it is very useful to consider the positive part of the function as a density function from which we can measure the mean, mode, median and variance"
" These measurements help in improving the performance, particularly in the case of sparse data"
One of the current drawbacks of the proposed algorithm is that the training occurs in one step
" Thus, when new data are addedit is costly to retrain the system"
 For practical remender systems this is a signiﬁcant problem as ratings are typically beingadded continuously
 We are currently investigating using a perceptronlike algorithm for updating the Lagrange multipliers
"AcknowledgmentsThe work reported in this paper has formed part of the Instant Knowledge Research Programme of Mobile VCE, the Virtual Centre of Excellence in Mobile & Personal Communications, www"
 The programme is cofunded by theM
 Information Sciences UK Technology Strategy Board’s Collaborative Research and Development programme
 Detailed technical reports on this research are available to all Industrial Members of Mobile VCE
" The third author has received funding from the European Community’s Seventh Framework Programme FP Speciﬁc Programme Cooperation, Theme , Information andCommunication Technologies under grant agreement No"
" Parameters brat, bfeat, and bdemoIn this section, we describe how we tuned the other parameters of the system"
"Parameters brat, bfeat, and bdemo brat bfeat determine the relative weights of rating, feature, and demographic kernelsin the ﬁnal prediction"
 Note that we assume the three bvalues are all positive and sum to one
" Sixtysix parameter sets weregenerated by producing all possible bination of parameters values, ranging from to "
 with differences of 
 Theparameter sets brat and bfeat gave the lowest MAE for all the datasets
Parameters q and q determine the relative weights of userbased and itembased CF in the ﬁnal prediction respectively
 We changed the value of q from to with a difference of 
 and the resulting MAE has been shown in A
" shows that for the SML dataset, the MAE is minimum at q "
", after which it starts increasing again whereas,for the FT dataset, the MAE keeps on decreasing, reaches its minimum at q "
", an then increases again"
 We choose theoptimal value of q to be 
 for SML and the FT dataset respectively
" Similarly, the value of q was trained for otherdatasets"
" It is worth noting that the itembased version got more weight except for the FT dataset in the ﬁnal prediction, forall datasets"
"In the hybrid variant, KMRCntHybrid the parameter hCnt determines the switching point between using the itembased anduserbased algorithms depending on the number of ratings of the item and the user"
 We determine the best value of hCntby varying it between and in steps of 
" shows the parameter hCnt learned for Case , as discussed in Section"
 over the validation set
" We observe that for the SML dataset, the MAE keeps on decreasing with the increase in the valueof hCnt, reaches its minimum at hCnt "
 and then either stays stable or starts increasing again
" For the FT dataset, theMAE decreases initially, when the value of hCnt changes from to "
 and then starts increasing when the value of hCnt increases beyond 
" For this reason, we choose the value hCnt to be "
 for SML and the FT datasets respectively
"Similarly, the value of hCnt was trained for other datasets"
"In the hybrid algorithm, KMRVarHybrid, the parameter hVar controls the switching from the userbased prediction to the itembased prediction depending on the uncertainty in the predictions measured by the variance in the Wuqi"
 To learn thisparameter we changed its value from to in steps of 
 and observed the corresponding MAE
 shows the parameter hVar learned for Case as discussed in Section 
" We observe that for the SML dataset, the MAEkeeps on decreasing with the increase in the value of hVar, reaches its peak at "
", and then starts increasing again"
" For the FTdataset, the decrease in the MAE is not very signiﬁcant, when hVar "
" however, afterwards, a sharp decrease in the MAE isobserved"
" The MAE keeps on decreasing, reaches its minimum at "
", and then either it stays stable or starts increasingagain"
 We choose the optimal value hVar to be 
 for SML and the FT datasets respectively
" Similarly, the valueof hVar was trained for other datasets"
 Parameter hVarAppendix B
 Mean Absolute Error MAEsigned by the user
" It is puted asMAE jpi aijnnXiMAE measures the average absolute deviation between a remender system’s predicted rating and a true rating aswhere pi and ai are the predicted and actual values of a rating, respectively, and n is the total number of rating records in thetest set"
" A rating record is a tuple consisting of a user ID, movie ID, and rating, huid,mid,ri, where r is the rating a reM"
 Information Sciences 
Value of Threshold Parameter ρ SML DataSet
EAM rorrlE etuosbA naeM
EAM rorrlE etuosbA naeM
Hybrid UBIBUBIBHybrid UBIBUBIB
Value of Threshold Parameter ρ FT DataSet
" Learning the optimal value of threshold parameter q, over the validation set, for the imbalanced datasets refer to Section "
mender system has to predict
" It has been used in ,,,,,,,,,"
 The aim of a remender system is tominimise the MAE score
"Normalised Mean Absolute Error NMAE has been used in ,, and is puted by normalising the MAE by a factor"
"The value of the factor depends on the range of the ratings for example, for the MovieLens dataset, it is "
" The motivationbehind this approach is that, the random guessing produces a score of "
" For further information, refer to "
"A closely related measure to the MAE is the Root Mean Squared Error RMSE, which is calculated as followsRMSE vuutnXpi ainiBoth MAE and RMSE are quoted in the literature"
 RMSE will be slightly more sensitive to large outliers
 The RMSE value willalways be greater than or equal to the MAE value
 Receiver Operating Characteristic ROC sensitivityROC measures the extent to which an information ﬁltering system can distinguish between good and bad items
 ROC sensitivity measures the probability with which a system accepts a good item
 The ROC sensitivity ranges from perfect to imperfect with 
" To use this metric for remender systems, we must ﬁrst determine which items are goodsignal and which are bad noise"
 We followed the procedure describe in while using this metric
" It has been used in,,"
 shows the ROCsensitive for the same set of algorithms on the same datasets as shown in Table 
" Precision, Recall, and FPrecision, recall, and F evaluate the effectiveness of a remender system by measuring the frequency with which ithelps users selectingremending a good item"
 Precision gives us the probability that a selected item is relevant
 Information Sciences EAM rorrlE etuosbA naeM
EAM rorrlE etuosbA naeM
Value of Threshold Parameter θ
 SML DataSetCntHybrid UBIBUBIBHybrid UBIBUBIB
Value of Threshold Parameter θ
" Learning the optimal value of threshold parameter, hCnt over the validation set, for the imbalanced datasets refer to Section "
gives us the probability that a relevant item is selected
" Precision and recall should be reported together, as increasing theprecision typically reduces the recall"
" The F Measure bines the precision and recall into a single metric and hasbeen used in many research projects, e"
" F is puted as followsF Precision RecallPrecision RecallThe ﬁrst step in puting the precision and recall is to divide items into two classes relevant and irrelevant, which isthe same as in ROCsensitivity"
" We calculated precision, recall, and F measures for each user, and reported the average results over all users"
 shows the F measure for the same set of algorithms on the same datasets as shown in Table 
" Sparse, skewed, and imbalanced datasetsIn this appendix we present results for the new item coldstart scenario, the long tail scenario and for sparse datasets"
 Performance evaluation under new item coldstart scenarioWe tested the new item coldstart scenario in exactly the same way we did the new user coldstart scenario
" That is, weselected random items, and kept the number of users in the training set who have rated the these item to , , , , and"
" shows again that the standard predictor fails under this scenario, whereas including the mean, mode and median predictor gives very good performance"
 We note that for new items the featurebased and demographicbased remenders work well for the coldstart scenario as these measures are not strongly inﬂuenced by a lack of ratinginformation for an item
 Information Sciences 
EAM rorrlE etuosbA naeM
EAM rorrlE etuosbA naeM
Value of Threshold Parameter θ
 SML DataSetvarHybrid UBIBUBIBHybrid UBIBUBIB
Value of Threshold Parameter θ
" Learning the optimal value of threshold parameter hvar, over the validation set, for the imbalanced datasets refer to Section "
A parison of the proposed algorithm with others in terms of ROCSensitivity metric
 The average with the respective standard deviation of results over fold is shown
 The best results are shown in bold font
AlgorithmBest ROCsensitivitySML
A parison of the proposed algorithm with others in terms of F measured over top remendations metric
 The average with the respective standarddeviation of results over fold is shown
 The best results are shown in bold font
Userbased CFItembased CFHybrid CFSVDKMRibKMRubKMRVarHybridAlgorithmUserbased CFItembased CFHybrid CFSVDKMRibKMRubKMRVarHybridM
 Information Sciences Table A
"Comparing the MAE observed in different approaches under new item coldstart scenario, for the SML dataset"
" The superﬁx M represents the correspondingversion of the KMR algorithm, where we take into account the max, mean, mode, and median of the output probability distribution"
 The average with therespective standard deviation of results over fold is shown
 The best results are shown in bold font
"Comparing MAE observed in different approaches under long tail scenario, for the SML dataset"
" The superﬁx M represents the corresponding version of theKMR algorithm, where we take into account the max, mean, mode, and median of the output probability distribution"
 The average with the respective standarddeviation of results over fold is shown
 The best results are shown in bold font
ApproachBest MAEMAEMAEMAEMAEMAEMAE
Comparing the performance of the algorithms under imbalanced and sparse datasets
" The superﬁx M represents the corresponding version of the KMRalgorithm, where we take into account the max, mean, mode, and median of the output probability distribution"
 The average with the respective standarddeviation of results over fold is shown
 The best results are shown in bold font
ApproachKMRibKMRubKMRFKMRDKMRMibKMRMubKMRMFKMRMDKMRMFibKMRibKMRubKMRFKMRDKMRMibKMRMubKMRMFKMRMDKMRMibFApproachKMRibKMRubKMRDKMRFKMRMibKMRMubKMRMDKMRMFKMRMibFC
 Performance evaluation under long tail scenarioThe long tail scenario is an important scenario for practical remender systems
" In a large Emerce system likeAmazon, there are huge numbers of items that are rated by very few users and hence the remendations generated forthese items would be poor, which could weaken the customers’ trust in the system"
To test the performance of the proposed algorithms under long tail scenario we created the artiﬁcial long tail scenario byrandomly selecting the of items in the tail
 The number of ratings given in the tail part were varied between andM
 Information Sciences —this ensures that the item is new and has very few ratings
" again shows the failure of the standard predictor inthe long tail scenario and the improvement obtained by using the mean, mode and median predictor"
" Performance evaluation under very sparse and imbalanced datasetsTo check the performance of the proposed approaches under very sparse and imbalanced datasets, we created subsets ofthe datasets by withholding x of the ratings from a rating proﬁle of useritem, where x xmin, xmax"
" We show results fortwo scenarios xmin , xmax , xmin , xmax "
" Changing the value of xmin creates different sparse subsets of the dataset, whereas keeping the value of xmax to ensures that the imbalanced dataset is created for eachscenario"
"For the SML and FT datasets, the results are shown in Table A"
 Again this follows the same pattern as the long tail andcoldstart scenarios
" Hyung Jun Ahn, A new similarity measure for collaborative ﬁltering to alleviate the new user coldstarting problem, Information Sciences Katja Astikainen, Liisa Holm, Esa Pitkanen, Sandor Szedmak, Juho Rousu, Towards structured output prediction of enzyme function, BMC Proceedings J"
" Hofmann, Unifying collaborative and contentbased ﬁltering, in Proceedings of the TwentyFirst International Conference on MachineLearning New York, NY, USA, ACM Press, , pp"
" Volinsky, The BellKor solution to the Netﬂix prize, in AT& T LabsResearch Technical report November, "
" Bell, Yehuda Koren, Lessons from the netﬂix prize challenge, SIGKDD Explorations Newsletters "
" Yolanda BlancoFernández, Martı´n LópezNores, Alberto GilSolla, Manuel RamosCabrer, José J"
" PazosArias, Exploring synergies between contentbased ﬁltering and spreading activation techniques in knowledgebased remender systems, Information Sciences "
" Breese, David Heckerman, and Carl Kadie, Empirical analysis of predictive algorithms for collaborative ﬁltering, in Proceedings of theFourteenth Conference on Uncertainty in Artiﬁcial Intelligence San Francisco, CA, USA, UAI’, Morgan Kaufmann Publishers Inc"
" Robin Burke, Integrating knowledgebased and collaborativeﬁltering remender systems, in AAAI Workshop on AI in Electronic Commerce, AAAI,, pp"
" Robin Burke, Hybrid remender systems Survey and experiments, User Modeling and UserAdapted Interaction "
" Mark Claypool, Anuja Gokhale, Tim Mir, Pavel Murnikov, Dmitry Netes, Matthew Sartin, Combining contentbased and collaborative ﬁlters in an onlinenewspaper, in Proceedings of ACM SIGIR Workshop on Remender Systems Berkeley, California, ACM, "
" Dennis DeCoste, Collaborative prediction using ensembles of maximum margin matrix factorizations, in Proceedings of the rd InternationalConference on Machine Learning New York, NY, USA, ICML ’, ACM, , pp"
" Ghazanfar, Adam PrügelBennett, An improved switching hybrid remender system using naive bayes classiﬁer and collaborativeﬁltering, in Lecture Notes in Engineering and Computer Science Proceedings of The International Multi Conference of Engineers and ComputerScientists , IMECS , March, , Hong Kong, , pp"
" Ghazanfar, Adam PrügelBennett, Building switching hybrid remender system using machine learning classiﬁers and collaborativeﬁltering, IAENG International Journal of Computer Science "
" Ghazanfar, Adam PrügelBennett, Novel signiﬁcance weighting schemes for collaborative ﬁltering generating improvedremendations in sparse environments, in DMIN, CSREA Press, , pp"
" Ghazanfar, Adam PrügelBennett, A scalable, accurate hybrid remender system, in Proceedings of the Third InternationalConference on Knowledge Discovery and Data Mining Washington, DC, USA, WKDD ’, IEEE Computer Society, , pp"
" Ghazanfar, Adam PrügelBennett, The advantage of careful imputation sources in sparse dataenvironment of remender systemsgenerating improved svdbased remendations, in IADIS European Conference on Data Mining, July "
" David Goldberg, David Nichols, Brian M"
" Oki, Douglas Terry, Using collaborative ﬁltering to weave an information tapestry, Communications of the Jonathan L"
" Riedl, Evaluating collaborative ﬁltering remender systems, ACM Transactions onACM "
" Thorsten Joachims, Training linear svms in linear time, in Proceedings of the th ACM SIGKDD International Conference on Knowledge Discovery andData Mining New York, NY, USA, KDD ’, ACM, , pp"
" HeungNam Kim, Abdulmotaleb ElSaddik, GeunSik Jo, Collaborative errorreﬂected models for coldstart remender systems, Decision Support Joseph A"
" Miller, David Maltz, Jonathan L"
" Gordon, John Riedl, Grouplens applying collaborative ﬁltering to usenetSystems "
"news, Communications of the ACM "
" Yehuda Koren, Factorization meets the neighborhood a multifaceted collaborative ﬁltering model, in Proceedings of the th ACM SIGKDDInternational Conference on Knowledge Discovery and Data Mining New York, NY, USA, KDD ’, ACM, , pp"
" Yehuda Koren, Factor in the neighbors scalable and accurate collaborative ﬁltering, ACM Transactions on Knowledge Discovery from Data TKDD "
" Csalogány, Methods for large scale SVD with missing values, in Proceedings of KDD Cup and Workshop, Citeseer, "
" Ken Lang, NewsWeeder learning to ﬁlter netnews, in Proceedings of the th International Conference on Machine Learning, Morgan Kaufmannpublishers Inc"
", San Mateo, CA, USA, , pp"
" Lawrence, Raquel Urtasun, Nonlinear matrix factorization with gaussian processes, in Proceedings of the th Annual InternationalConference on Machine Learning New York, NY, USA, ICML ’, ACM, , pp"
" DuenRen Liu, PeiYun Tsai, PoHuan Chiu, Personalized remendation of popular blog articles for mobile applications, Information Sciences Lester Mackey, David Weiss, Michael I"
" Jordan, Mixed membership matrix factorization, in Proceedings of the th International Conference on "
"Machine Learning, June "
" Benjamin Marlin, Collaborative ﬁltering a machine learning perspective, Master’s thesis, University of Toronto, "
" Benjamin Marlin, Modeling user rating proﬁles for collaborative ﬁltering, Advances in Neural Information Processing Systems "
" Rahul Mazumder, Trevor Hastie, Robert Tibshirani, Spectral regularization algorithms for learning large inplete matrices, Journal of MachineLearning Research "
" Prem Melville, Raymod J"
" Mooney, Ramadass Nagarajan, Contentboosted collaborative ﬁltering for improved remendations, in EighteenthNational Conference on Artiﬁcial Intelligence Menlo Park, CA, USA, American Association for Artiﬁcial Intelligence, , pp"
 Information Sciences Stuart E
" Middleton, Capturing knowledge of user preferences with remender systems, Ph"
" thesis, UNIVERSITY OF SOUTHAMPTON, UK, Stuart E"
" Middleton, Harith Alani, David C"
" De Roure, Exploiting synergy between ontologies and remender systems, in The Eleventh InternationalSeptember "
"World Wide Web Conference WWW, "
" Mooney, Loriene Roy, Contentbased book remending using learning for text categorization, in Proceedings of the Fifth ACMConference on Digital Libraries New York, NY, USA, DL ’, ACM, , pp"
" Pennock, Applying collaborative ﬁltering techniques to movie search for better ranking and browsing, in Proceedings of the th ACMSIGKDD International Conference on Knowledge Discovery and Data Mining, ACM, , pp"
" YoonJoo Park, Alexander Tuzhilin, The long tail of remender systems and how to leverage it, in Proceedings of the ACM Conference onRemender Systems New York, NY, USA, RecSys ’, ACM, , pp"
" Pazzani, A framework for collaborative, contentbased and demographic ﬁltering, Artiﬁcial Intelligence Review "
" Pazzani, Daniel Billsus, The Adaptive Web, SpringerVerlag, Berlin, Heidelberg, "
" Pennock, Eric Horvitz, Steve Lawrence, C"
" Lee Giles, Collaborative ﬁltering by personality diagnosis a hybrid memory and modelbasedapproach, in Proceedings of the th Conference on Uncertainty in Artiﬁcial Intelligence San Francisco, CA, USA, UAI ’, Morgan KaufmannPublishers Inc"
" HerreraViedma, A hybrid remender system for the selective dissemination of research resources in atechnology transfer ofﬁce, Information Sciences "
" Rennie Nathan Srebro, Fast maximum margin matrix factorization for collaborative prediction, in Proceedings of the nd InternationalConference on Machine Learning New York, NY, USA, ICML ’, ACM, , pp"
" Paul Resnick, Neophytos Iacovou, Mitesh Suchak, Peter Bergstrom, John Riedl, Grouplens an open architecture for collaborative ﬁltering of netnews,in Proceedings of the ACM Conference on Computer Supported Cooperative Work, CSCW ’, ACM, , pp"
" Mnih, Probabilistic matrix factorization, Advances in Neural Information Processing Systems "
" Badrul Sarwar, George Karypis, Joseph Konstan, John Reidl, Itembased collaborative ﬁltering remendation algorithms, in Proceedings of the thInternational Conference on World Wide Web New York, NY, USA, WWW ’, ACM, , pp"
" Badrul Sarwar, George Karypis, Joseph Konstan, John Riedl, Analysis of remendation algorithms for emerce, in Proceedings of the nd ACMConference on Electronic Commerce New York, NY, USA, EC ’, ACM, , pp"
" Badrul Sarwar, George Karypis, Joseph Konstan, John Riedl, Application of dimensionality reduction in remender systema case study, in ACMWEBKDD Workshop, Citeseer, "
" Badrul Sarwar, George Karypis, Joseph Konstan, John Riedl, Remender systems for largescale emerce scalable neighborhood formation usingclustering, in Proceedings of the Fifth International Conference on Computer and Information Technology, "
" Vincent SchickelZuber, Boi Faltings, Using an ontological apriori score to infer user’s preferences, in Workshop on Remender SystemsECAI, Jesus SerranoGuerrero, Enrique HerreraViedma, Jose A"
" Olivas, Andres Cerezo, Francisco P"
" Romero, A google wavebased fuzzy remender systemto disseminate information in university digital libraries "
", Information Sciences "
" Upendra Shardanand, Pattie Maes, Social information ﬁltering algorithms for automating word of mouth, in Proceedings of the SIGCHI Conference onHuman factors in Computing Systems New York, NY, USA, CHI ’, ACM PressAddisonWesley Publishing Co"
" Nathan Srebro, Jasson D"
" Jaakkola, Maximummargin matrix factorization, Advances in Neural Information Processing Systems , pp"
" Stern, Ralf Herbrich, Thore Graepel, Matchbox large scale online bayesian remendations, in Proceedings of the th InternationalConference on World Wide Web New York, NY, USA, WWW ’, ACM, , pp"
" Sandor Szedmak, Ni Yizhao, R"
" Gunn Steve, Maximum margin learning with inplete data learning networks instead of tables, Journal of MachineLearning Research Proceedings Track "
" Gábor Takács, István Pilászy, Bottyán Németh, Domonkos Tikk, Investigation of various matrix factorization methods for large remender systems,in Proceedings of the nd KDD Workshop on LargeScale Remender Systems and the Netﬂix Prize Competition New York, NY, USA, NETFLIX ’,ACM, , pp"
" Gábor Takács, István Pilászy, Bottyán Németh, Domonkos Tikk, Scalable collaborative ﬁltering approaches for large remender systems, Journal of Loren Terveen, Will Hill, Brian Amento, David McDonald, Josh Creter, Phoaks a system for sharing remendations, Communications of the ACM Machine Learning Research "
" Robin van Meteren, Maarten van Someren, Using contentbased ﬁltering for remendation, in Proceedings of the Machine Learning in the NewInformation Age MLnetECML Workshop, Citeseer, "
" Manolis Vozalis, Konstantinos G"
" Margaritis, Applying SVD on generalized itembased ﬁltering, International Journal of Computer Science andApplications "
" Manolis Vozalis, Konstantinos G"
" Margaritis, Using svd and demographic data for the enhancement of generalized collaborative ﬁltering, Information Mingru Wu, Collaborative ﬁltering via ensembles of matrix factorizations, in Proceedings of KDD Cup and Workshop, Citeseer, "
" GuiRong Xue, Chenxi Lin, Qiang Yang, WenSi Xi, HuaJun Zeng, Yong Yu, Zheng Chen, Scalable collaborative ﬁltering using clusterbased smoothing,in Proceedings of the th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval New York, NY, USA,SIGIR ’, ACM, , pp"
" Daoqiang Zhang, ZhiHua Zhou, Songcan Chen, Nonnegative matrix factorization on kernels, in Proceedings of the th Paciﬁc Rim InternationalConference on Artiﬁcial Intelligence Berlin, Heidelberg, PRICAI’, SpringerVerlag, , pp"
"See discussions, stats, and author profiles for this publication at httpswww"
"netpublicationThe Unreasonable Effectiveness of DataArticle in Intelligent Systems, IEEE · May DOI "
" · Source IEEE XploreCITATIONS authors, includingAlon HalevyGoogle Inc"
"SEE PROFILEREADS,Peter NorvigGoogle Inc"
"SEE PROFILE PUBLICATIONS , CITATIONS PUBLICATIONS , CITATIONS Some of the authors of this publication are also working on these related projectsBiomedical data integration View projectAll content following this page was uploaded by Peter Norvig on December "
The user has requested enhancement of the downloaded file
"E X P E R T O P I N I O NContact Editor Brian Brannon, bbrannonputer"
"orgThe Unreasonable Effectiveness of DataAlon Halevy, Peter Norvig, and Fernando Pereira, GoogleEugene Wigner’s article “The Unreasonable Effectiveness of Mathematics in the Natural Sciences” examines why so much of physics can be neatly explained with simple mathematical formulassuch as f ma or e mc"
" Meanwhile, sciences that involve human beings rather than elementary particles have proven more resistant to elegant mathematics"
 Economists suffer from physics envy over their inability to neatly model human behavior
" An informal, inplete grammar of the English language runs over , pages"
" Perhaps when it es to natural language processing and related fi elds, we’re doomed to plex theories that will never have the elegance of physics equations"
" But if that’s so, we should stop acting as if our goal is to author extremely elegant theories, and instead embrace plexity and make use of the best ally we have the unreasonable effectiveness of data"
"One of us, as an undergraduate at Brown University, remembers the excitement of having access to the Brown Corpus, containing one million English words"
" Since then, our fi eld has seen several notable corpora that are about times larger, and in , Google released a trillionword corpus with frequency counts for all sequences up to fi ve words long"
" In some ways this corpus is a step backwards from the Brown Corpus it’s taken from unfi ltered Web pages and thus contains inplete sentences, spelling errors, grammatical errors, and all sorts of other errors"
 It’s not annotated with carefully handcorrected partofspeech tags
 But the fact that it’s a million times larger than the Brown Corpus outweighs these drawbacks
" A trillionword corpus—along with other Webderived corpora of millions, billions, or trillions of links, videos, images, tables, and user interactions—captures even very rare aspects of human behavior"
" So, this corpus could serve as the basis of a plete model for certain tasks—if only we knew how to extract the model from the data"
Learning from Text at Web ScaleThe biggest successes in naturallanguagerelated machine learning have been statistical speech recognition and statistical machine translation
 The reason for these successes is not that these tasks are easier than other tasks they are in fact much harder than tasks such as document classifi cation that extract just a few bits of information from each document
 The reason is that translation is a natural task routinely done every day for a real human need think of the operations of the European Union or of news agencies
 The same is true of speech transcription think of closedcaption broadcasts
" In other words, a large training set of the inputoutput behavior that we seek to automate is available to us in the wild"
" In contrast, traditional natural language processing problems such as document classifi cation, partofspeech tagging, namedentity recognition, or parsing are not routine tasks, so they have no large corpus available in the wild"
" Instead, a corpus for these tasks requires skilled human annotation"
" Such annotation is not only slow and expensive to acquire but also diffi cult for experts to agree on, being bedeviled by many of the diffi culties we discuss later in relation to the Semantic Web"
 The fi rst lesson of Webscale learning is to use available largescale data rather than hoping for annotated data that isn’t available
" For instance, we fi nd that useful semantic relationships can be automatically learned from the statistics of search queries and the corresponding results or from the accumulated evidence of Webbased text patterns and formatted tables, in both cases without needing any manually annotated data"
 IEEEPublished by the IEEE Computer SocietyiEEE iNTElliGENT SYSTEMSAuthorized licensed use limited to Univ of Calif Berkeley
" Downloaded on February , at from IEEE Xplore"
 Another important lesson from statistical methods in speech recognition and machine translation is that memorization is a good policy if you have a lot of training data
 The statistical language models that are used in both tasks consist primarily of a huge database of probabilities of short sequences of consecutive words ngrams
 These models are built by counting the number of occurrences of each ngram sequence from a corpus of billions or trillions of words
" Researchers have done a lot of work in estimating the probabilities of new ngrams from the frequencies of observed ngrams using, for example, GoodTuring or KneserNey smoothing, leading to elaborate probabilistic models"
" But invariably, simple models and a lot of data trump more elaborate models based on less data"
" Similarly, early work on machine translation relied on elaborate rules for the relationships between syntactic and semantic patterns in the source and target languages"
" Currently, statistical translation models consist mostly of large memorized phrase tables that give candidate mappings between specific source and targetlanguage phrases"
"Instead of assuming that general patterns are more effective than memorizing specific phrases, today’s translation models introduce general rules only when they improve translation over just memorizing particular phrases for instance, in rules for dates and numbers"
 Similar observations have been made in every other application of machine learning to Web data simple ngram models or linear classifiers based on millions of specific features perform better than elaborate models that try to discover general rules
 In many cases there appears to be a threshold of sufficient data
" For example, James Hays and Alexei A"
" Efros addressed the task of scene pletion removing an unwanted, unsightly automobile or exspouse from a photograph and filling in the background with pixels taken from a large corpus of other photos"
" With a corpus of thousands of photos, the results were poor"
" But once they accumulated millions of photos, the same algorithm performed quite well"
" We know that the number of grammatical English sentences is theoretically infinite and the number of possible Mbyte photos is ,,"
" However, in practice we humans care to make only a finite number of distinctions"
" For many tasks, once we have a billion or so examples, we essentially have a closed set that repreFor many tasks, words and word binations provide all the representational machinery we need to learn from text"
"sents or at least approximates what we need, without generative rules"
"For those who were hoping that a small number of general rules could explain language, it is worth noting that language is inherently plex, with hundreds of thousands of vocabulary words and a vast variety of grammatical constructions"
" Every day, new words are coined and old usages are modified"
 This suggests that we can’t reduce what we want to say to the free bination of a few abstract primitives
" For those with experience in smallscale machine learning who are worried about the curse of dimensionality and overfitting of models to data, note that all the experimental evidence from the last decade suggests that throwing away rare events is almost always a bad idea, because much Web data consists of individually rare but collectively frequent events"
" For many tasks, words and word binations provide all the representational machinery we need to learn from text"
 Human language has evolved over millennia to have words for the important concepts let’s use them
 Abstract representations such as clusters from latent analysis that lack linguistic counterparts are hard to learn or validate and tend to lose information
 Relying on overt statistics of words and word cooccurrences has the further advantage that we can estimate models in an amount of time proportional to available data and can often parallelize them easily
" So, learning from the Web bees naturally scalable"
The success of ngram models has unfortunately led to a false dichotomy
" Many people now believe there are only two approaches to natural language processinga deep approach that relies on handcoded grammars and ontologies, represented as plex networks of relations and a statistical approach that relies on learning ngram statistics from large corpora"
"	In reality, three orthogonal problems arise choosing a representation language,encoding a model in that language, and performing inference on the model"
" Each problem can be addressed in several ways, resulting in dozens of approaches"
" The deep approach that was popular in the s used firstorder logic or something similar as the representation language, encoded a model with the labor of a team of graduate students, and did inference with plex inference rules appropriate to the representation language"
" In the s and s, it became fashionable to Marchapril www"
orgintelligent Authorized licensed use limited to Univ of Calif Berkeley
" Downloaded on February , at from IEEE Xplore"
" use finite state machines as the representation language, use counting and smoothing over a large corpus to encode a model, and use simple Bayesian statistics as the inference method"
"But many other binations are possible, and in the s, many are being tried"
" For example, Lise Getoor and Ben Taskar collect work on statistical relational learning—that is, representation languages that are powerful enough to represent relations between objects such as firstorder logic but that have a sound, probabilistic definition that allows models to be built by statistical learning"
 Taskar and his colleagues show how the same kind of maximummargin classifier used in support vector machines can improve traditional parsing
" Stefan Schoenmackers, Oren Etzioni, and Daniel S"
 Weld show how a relational logic and a millionpage corpus can answer questions such as “what vegetables help prevent osteoporosis” by isolating and bining the relational assertions that “kale is high in calcium” and “calcium helps prevent osteoporosis
”Semantic Web versus Semantic InterpretationThe Semantic Web is a convention for formal representation languages that lets software services interact with each other “without needing artificial intelligence
"” A software service that enables us to make a hotel reservation is transformed into a Semantic Web service by agreeing to use one of several standards for representing dates, prices, and locations"
 The service can then interoperate with other services that use either the same standard or a different one with a known translation into the chosen standard
" As Tim BernersLee, James Hendler, and Ora Lassila write, “The Semantic Web will enable machines to prehend semantic documents and data, not human speech and writings"
”The problem of understanding human speech and writing—the semantic interpretation problem—is quite different from the problem of software service interoperability
" Semantic interpretation deals with imprecise, ambiguous natural languages, whereas service interoperability deals with making data precise enough that the programs operating on the data will function effectively"
" Unfortunately, the fact that the word “semantic” appears in both “Semantic Web” and “semantic interpretation” means that the two probBecause of a huge shared cognitive and cultural context, linguistic expression can be highly ambiguous and still often be understood correctly"
"lems have often been conflated, causing needless and endless consternation and confusion"
 The “semantics” in Semantic Web services is embodied in the code that implements those services in accordance with the specifications expressed by the relevant ontologies and attached informal documentation
 The “semantics” in semantic interpretation of natural languages is instead embodied in human cognitive and cultural processes whereby linguistic expression elicits expected responses and expected changes in cognitive state
" Because of a huge shared cognitive and cultural context, linguistic expression can be highly ambiguous and still often be understood correctly"
" Given these challenges, building Semantic Web services is an engineering and sociological challenge"
" So, even though we understand the required technology, we must deal with significant hurdles Ontology writing"
 The important easy cases have been done
" For example, the Dublin Core defines dates, locations, publishers, and other concepts that are sufficient for card catalog entries"
"org defines chromosomes, species, and gene sequences"
 Other organizations provide ontologies for their specific fields
 But there’s a long tail of rarely used concepts that are too expensive to formalize with current technology
" Project Halo did an excellent job of encoding and reasoning with knowledge from a chemistry textbook, but the cost was US, per page"
 Obviously we can’t afford that cost for a trillion Web pages
 PubDifficulty of implementation
 lishing a static Web page written in natural language is easy anyone with a keyboard and Web connection can do it
" Creating a databasebacked Web service is substantially harder, requiring specialized skills"
 Making that service pliant with Semantic Web protocols is harder still
" Major sites with petent technology experts will find the extra effort worthwhile, but the vast majority of small sites and individuals will find it too difficult, at least with current tools"
" In some domains, peting factions each want to promote their own ontology"
" In other domains, the entrenched leaders of the field oppose any ontology because it would level the playing field for their petitors"
" This is a problem in diplomacy, not technology"
" As Tom Gruber says, “Every ontology is a treaty—a social agreement—among people with some mon motive in sharing"
"” When a motive for sharing is lacking, so are mon ontologies"
Inaccuracy and deception
orgintelligent iEEE iNTElliGENT SYSTEMSAuthorized licensed use limited to Univ of Calif Berkeley
" Downloaded on February , at from IEEE Xplore"
 know how to build sound inference mechanisms that take true premises and infer true conclusions
" But we don’t have an established methodology to deal with mistaken premises or with actors who lie, cheat, or otherwise deceive"
" Some work in reputation management and trust exists, but for the time being we can expect Semantic Web technology to work best where an honest, selfcorrecting group of cooperative users exists and not as well where petition and deception exist"
The challenges for achieving accurate semantic interpretation are different
 We’ve already solved the sociological problem of building a network infrastructure that has encouraged hundreds of millions of authors to share a trillion pages of content
 We’ve solved the technological problem of aggregating and indexing all this content
" But we’re left with a scientific problem of interpreting the content, which is mainly that of learning as much as possible about the context of the content to correctly disambiguate it"
 The semantic interpretation problem remains regardless of whether or not we’re using a Semantic Web framework
" The same meaning can be expressed in many different ways, and the same expression can express many different meanings"
" For example, a table of pany information might be expressed in ad hoc HTML with column headers called “Company,” “Location,” and so on"
" Or it could be expressed in a Semantic Web format, with standard identifiers for “Company Name” and “Location,” using the Dublin Core Metadata Initiative pointencoding scheme"
" But even if we have a formal Semantic Web “Company Name” attribute, we can’t expect to have an ontology for every possible value of this attribute"
" For example, we can’t know for sure what pany the string “Joe’s Pizza” refers to because hundreds of businesses have that name and new ones are being added all the time"
 We also can’t always tell which business is meant by the string “HP
” It could refer to Helmerich & Payne Corp
 when the column is populated by stock ticker symbols but probably refers to HewlettPackard when the column is populated by names of large technology panies
 The problem of semantic interpretation remains using a Semantic Web formalism just means that semantic interpretation must be done on shorter strings that fall between angle brackets
belong in the same column of a table
 We’ve never before had such a vast collection of tables and their schemata at our disposal to help us resolve semantic heterogeneity
" Using such a corpus, we hope to be able to acplish tasks such as deing when “Company” and “Company Name” are synonyms, deing when “HP” means Helmerich & Payne or HewlettPackard, and determining that an object with attributes “passengers” and “cruising altitude” is probably an aircraft"
"The same meaning can be expressed in many different ways, and the same expression can express many different meanings"
 What we need are methods to infer relationships between column headers or mentions of entities in the world
" These inferences may be incorrect at times, but if they’re done well enough we can connect disparate data collections and thereby substantially enhance our interaction with Web data"
" Interestingly, here too Webscale data might be an important part of the solution"
 The Web contains hundreds of millions of independently created tables and possibly a similar number of lists that can be transformed into tables
 These tables represent structured data in myriad domains
 They also represent how different people organize data—the choices they make for which columns to include and the names given to the columns
" The tables also provide a rich collection of column values, and values that they deed ExamplesHow can we use such a corpus of tables Suppose we want to find synonyms for attribute names—for example, when “Company Name” could be equivalent to “Company” and “price” could be equivalent to “discount”"
" Such synonyms differ from those in a thesaurus because here, they are highly context dependent both in tables and in natural language"
" Given the corpus, we can extract a set of schemata from the tables’ column labels for example, researchers reliably extracted "
" million distinct schemata from a collection of million tables, not all of which had schema"
 We can now examine the cooccurrences of attribute names in these schemata
" If we see a pair of attributes A and B that rarely occur together but always occur with the same other attribute names, this might mean that A and B are synonyms"
 We can further justify this hypothesis if we see that data elements have a significant overlap or are of the same data type
" Similarly, we can also offer a schema autoplete feature for database designers"
" For example, by analyzing such a large corpus of schemata, we can discover that schemata that have the attributes Make and Model also tend to have the attributes Year, Color, and Mileage"
" Providing such feedback to schemata creators can save them time but can also help them use more mon attribute names, thereby decreasing a possible Marchapril www"
orgintelligent Authorized licensed use limited to Univ of Calif Berkeley
" Downloaded on February , at from IEEE Xplore"
 source of heterogeneity in Webbased data
" Of course, we’ll find immense opportunities to create interesting data sets if we can automatically bine data from multiple tables in this collection"
 This is an area of active research
"Another opportunity is to bine data from multiple tables with data from other sources, such as unstructured Web pages or Web search queries"
" For example, Marius Paşca also considered the task of identifying attributes of classes"
" That is, his system first identifies classes such as “Company,” then finds examples such as “Adobe Systems,” “Macromedia,” “Apple Computer,” “Target,” and so on, and finally identifies class attributes such as “location,” “CEO,” “headquarters,” “stock price,” and “pany profile"
"” Michael Cafarella and his colleagues showed this can be gleaned from tables, but Paşca showed it can also be extracted from plain text on Web pages and from user queries in search logs"
" That is, from the user query “Apple Computer stock price” and from the other information we know about existing classes and attributes, we can confirm that “stock price” is an attribute of the “Company” class"
" Moreover, the technique works not just for a few dozen of the most popular classes but for thousands of classes and tens of thousands of attributes, including classes like “Aircraft Model,” which has attributes “weight,” “length,” “fuel consumption,” “interior photos,” “specifications,” and “seating arrangement"
"” Paşca shows that including query logs can lead to excellent performance, with percent precision over the top attributes per class"
" Choose a representation that can use unsupervised learning on unlabeled data, which is so much more plentiful than labeled data"
" Represent all the data with a nonparametric model rather than trying to summarize it with a parametric model, because with very large data sources, the data holds a lot of detail"
" For natural language applications, trust that human language has already evolved words for the important concepts"
" See how far you can go by tying together the words that are already there, rather than by inventing new concepts with clusters of words"
" Now go out and gather some data, and see what it can do"
"Choose a representation that can use unsupervised learning on unlabeled data, which is so much more plentiful than labeled data"
" Wigner, “The Unreasonable Effectiveness of Mathematics in the Natural Sciences,” Comm"
" Pure and Applied Mathematics, vol"
", A Comprehensive Grammar of the English Language, Longman, "
" Carroll, Computational Analysis of PresentDay American English, Brown Univ"
" Franz, Web T Gram Version , Linguistic Data Consortium, "
" Vasserman, “Translating Queries into Snippets for Improved Query Expansion,” Proc"
" Computational Linguistics Coling , Assoc"
" Computational Linguistics, , pp"
", “Learning to Create DataIntegrating Queries,” Proc"
" Very Large Databases VLDB , Very Large Database Endowment, , pp"
" Efros, “Scene Completion Using Millions of Photographs,” Comm"
" Taskar, Introduction to Statistical Relational Learning, MIT Press, "
", “MaxMargin Parsing,” Proc"
" Empirical Methods in Natural Language Processing EMNLP , Assoc"
" for Computational Linguistics, , pp"
" Weld, “Scaling Textual Inference to the Web,” Proc"
" Empirical Methods in Natural Language Processing EMNLP , Assoc"
" for Computational Linguistics, , pp"
" Lassila, “The Semantic Web,” Scientific Am"
", “Towards a Quantitative, PlatformIndependent Analysis of Knowledge Systems,” Proc"
" Principles of Knowledge Representation, AAAI Press, , pp"
" “Interview of Tom Gruber,” AIS SIGSEMIS Bull"
", “WebTables Exploring the Power of Tables on the Web,” Proc"
" Very Large Data Base Endowment VLDB , ACM Press, , pp"
" Paşca, “Organizing and Searching the World Wide Web of Facts"
" Step Two Harnessing the Wisdom of the Crowds,” Proc"
 th Int’l World Wide Web Conf
Alon Halevy is a research scientist at Google
 Contact him at halevygoogle
Peter Norvig is a research director at Google
 Contact him at pnorviggoogle
Fernando Pereira is a research director at Google
 Contact him at pereiragoogle
orgintelligent iEEE iNTElliGENT SYSTEMSView publication statsView publication statsAuthorized licensed use limited to Univ of Calif Berkeley
" Downloaded on February , at from IEEE Xplore"
